acceptedDate,authors,contributors,createdDate,dataProvider,depositedDate,documentType,doi,downloadUrl,fullText,id,identifiers,title,language,publishedDate,publisher,references,sourceFulltextUrls,updatedDate,yearPublished,links,abstract,tags,fulltextStatus
,"[{'name': 'Harmon, Casey'}]",[],2021-12-09T12:27:28+00:00,"{'name': 'ScholarWorks@UA', 'url': 'https://api.core.ac.uk/v3/data-providers/2150'}",,,,https://core.ac.uk/download/481573328.pdf,"ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 1
Artificial Intelligence and Machine Learning for All Students
A Meta-Synthesis
Casey Harmon
Submitted in partial fulfillment of the requirements of the Master of Education in Special
Education degree at the University of Alaska Southeast
RECOMMENDED: _____________________________________________________
Heather Batchelder, Ph.D.
APPROVED: __________________________________________________________
Jill Burkert, Ph.D., Academic Advisor
___________________________________________________________
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 2
Date
Abstract
This meta-synthesis investigates the increasingly powerful and promising fields of
artificial intelligence and machine learning as these technologies filter into the sphere of
education. Smart technologies have been quickly gaining momentum in our society and have
piqued the interest of many educators and administrators that are considering early adoption and
applications of these new and promising technologies that may be able to offer teachers,
administrators, and students new ways to access information and student learning. However, it is
not without some resistance toward these technologies, that we consider their diverse
applications in classrooms. This research of 43 articles address the applications, caveats, biases,
and possibilities that these new, smart technologies using artificial intelligence and machine
learning can offer to improve the education of students with and without disabilities.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 3
1. Introduction
1.1. Background
Schools have always been and will always be a mirror of our society at large. Our
students are the product of our society, and our societal issues and demands are reflected in our
classroom just as they are seen in our communities.
Near the end of the eighteenth century, the Industrial Revolution began in the United
States and caused societal changes in communities throughout the country. Just as society was
changing, social reformers criticized the current education system as not meeting the needs of the
students coming from families that worked in industrial occupations (Ireh, 2016).
Early educational reformers like John Dewey believed that “in an industrial society the
school should become more industrial.” At that time it was believed that “industrial” education
was more engaging and stimulating to the child. Prevailing wisdom gave that the material was
more approachable as it was immediately relevant to their daily world and students could easily
make connections between curriculum and their experiences (Ireh, 2016).
As the industrial revolution progressed, more and more children entered school, yet many
students had difficulty advancing through school. It was argued that if schools were to meet the
needs of their students, they would need to provide a curriculum that benefited children of the
working class, and if they wanted students to stay in school, they would need to provide students
with the education and skills needed for occupational success. The increase in the number of
students from working class families forced schools to adapt curriculum offerings to the needs of
these children and prepare them for the world of work (Ireh, 2016).
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 4
Today, in classrooms throughout the United States, we are facing similar social
reformations in classrooms by the means of the “Technology Revolution.” As our society, and
our world, is becoming more globally connected through technology, educators and education
reformers push education technology into our classrooms. Just as in our society, technology has
increased significantly in our classrooms, but classroom practices are often stagnant. The added
usage of iPads and computers are not increasing student performance, and are not meeting the
technological skill needs in our work forces (Jenkin, 2015). More often than not, the
technologies used in the classroom have become a replacement for traditional practices and are
not always used to their full potential, nor do they meet the demands of professional skills
required for work forces outside of the classroom.
Being current with societal demands is not new to education. Many educators understand
the importance of current practices to engage students and help them meet their full potential.
Long before technology infused its way into our daily lives, educators used differentiated
instruction to help students learn on their own path of understanding and help each individual
make sense of ideas (Scalise, 2009). With the boom in education technology, many new software
programs entered the marketplace. As the technology developed and changed, so did the
programs offered to districts and educators. Today, technological curriculum replacements and
supplements change the way students and teachers interact with education. Formative
assessments are used to guide instruction during the process of learning. The process of
assessment and deciding what content is offered to each child can be seamless and immediate
with artificial intelligence.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 5
Differentiation and creating a technological classroom based on the theory of universal
design becomes faster and easier with instant feedback to the teacher and learner when delivered
on a computer based platform. Often time machine learning technologies are designed to stretch
a child’s opportunity to learn, and can identify areas of interest and heightened engagement to
give the student a choice among objectives. Many different networks of learning can be made
available through machine learning and artificial intelligence, allowing students to make choices
as they go, personalizing learning under the assumption that no two students are alike and,
therefore, differentiation is required to customize their learning to fit each student’s path of
understanding.
Currently, schools stand in the middle of old and new models of schooling with tensions
between the two rising (Erstad, Eickelmann & Eichhorn, 2015). The demands on the teacher are
consistently increasing while support in the classroom is consistently decreasing, necessitating
creative and innovate ways to meet the needs of differentiating instruction and preparing students
for the world of work. Teacher attrition and class sizes are rising, while teacher wages and
budgets for students are stagnant (Erstad, Eickelmann & Eichhorn, 2015).
While artificial intelligence and machine learning cannot solve the larger issues of
retaining high quality teachers and fixing the budget issues of public schools in the United States,
these technologies may alleviate some of the pain and pressure current teachers are experiencing
in their profession and meet the technological demands of society, offer individualized
instruction for learners, and empower teachers to focus their efforts on helping strengthen
individual student learning and less on whole group instruction and reteaching.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 6
Even given that artificial intelligence is used in arenas such as finance, manufacturing,
medicine and other sectors, artificial intelligence has not made great headway in education, even
though it gives the impression that it would be very beneficial considering educational reliance
on standardized testing (Ramaswami, 2009). Considering how schools are being held
accountable for passing standardized tests, it seems appropriate to tailor formative machine
learning to help students make progress at their individual level to accelerate learning for
performance on standardized tests. But even more progressively, artificial intelligence could
potentially replace standardized tests all together and show student progress at their individual
level from the beginning of the school year to the reporting period. However, with the education
climate being steadfast on accountability from standardization it may be difficult for artificial
intelligence programs to be accepted at federal levels (Ramaswami, 2009). There is nearly no
intelligent software being utilized for large summative testing—but it is easily possible.
There are many artificial intelligence products available in the marketplace worldwide
that are highly individualized, deeply personal to student learning needs, and focus on
intervention for individualization (Ramaswami, 2009). Educational reformers argue that
one-size-fits all testing is a thing of the past and old outdated models of learning and assessment
will, and should be, disrupted by technological innovations that specialize in personalized
instruction. Traditional classroom testing could be completely phased out, but teachers and
policy makers would be rich with student data.
Some educators fear that when technology replaces traditional education they become
less valuable in the classroom, but the opposite may be true. Teachers will always be hugely
valuable in education, as humans will always benefit from interpersonal interaction. Artificial
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 7
intelligence allows teachers to focus on other facets of the job outside of the curriculum and can
direct teachers to instruct with more precision on exactly what students are prepared to learn. As
said by Rama Ramaswami in her article, Is the Future Now for A.I.?, “AI can’t replace human
teachers, but if it is done well, it has a role in the classroom. Educators can use all the help they
can get.”
The current public school classroom teacher faces many challenging obstacles as they
educate a very diverse population with many different home languages, backgrounds, and
abilities. Teachers are challenged with rising classroom size, challenging behaviors with little
support, incredibly diverse learning needs, and requirements of standardized testing. Allowing
artificial intelligence and machine learning to take over some of those responsibilities may
alleviate some of the pressure from the teachers and actually create a more individualized
learning plan for all students.
1.2. Author’s beliefs and experiences
My professional experiences and philosophical beliefs have stemmed from a natural and
authentic journey into the world of education. I had a very typical and average public education
experience growing up in Alaska, completing most of my education in Anchorage public
schools. My empathetic nature and love of caregiving led me into a long journey of becoming a
teacher. No one in my family went to college – in fact, the rhetoric from my family was that
college was a waste of time and the only way to be successful in a career was to have good work
ethic and to show up. Being an independent woman with a passion for working with children,
that would not work for me. I began my own journey despite my family’s beliefs. I started taking
classes and working towards advancement at an optional school. Education would become an
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 8
important part of my life, and I want to imbue my love for learning into the lives of all of my
current and future students. From my own experiences, I believe that education is key to
overcoming adversity. If I had followed in the footsteps of my family, I wouldn’t have been able
to become the teacher that I am today, and I couldn’t positively impact the lives of so many
young children as they become successful adults and lifelong learners.
From my experience working in an optional school environment, I gained a whole new
perspective on education. Optional schools are an alternative option to a traditional public school
education and generally focus on recognizing individual strengths, needs and interests of each
learner. I was excited about being in the school and teaching children in a holistic and genuine
manner. I found the type of teacher I wanted to be; I found the type of experience I thought was
best for children; I found the motivation I needed to complete my degree and start my career as
an educator. I did my teacher preparation programs in predominantly optional school settings. I
also took a intercultural exchange residency in St. Paul, Alaska and volunteer taught in Malawi
and Swaziland for a summer. I always had the freedom to teach children in a setting that
supported my philosophy of education and gave me the freedom to make choices that were best
for my students. My personal beliefs aligned with the pedagogy of the schools I had taught
in—the belief that: all children are curious and want to learn; when children are respected they
will be respectful; children will have enthusiasm and motivation for learning when it is
meaningful and relevant; children learn best when we nurture the whole child socially,
physically, emotionally, creatively, and educationally; education should be differentiated to
individual interests, experiences, aptitude, skills, and knowledge; content areas should be
integrated into what the child is interested in; children are held responsible in their educational
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 9
journey; education is guided through play and exploration; children are encouraged to solve
problems; assessment is used to guide learning; and environments should be child-centered. I
was excited to get a job doing something that I excelled at and that I enjoyed. Until I didn’t get
offered a job one of those optional schools. I took the first job that I was presented with. I have
been working at a Title I public school in Anchorage, Alaska for the past five years. When I was
hired, I was told that I would be working in a direct instruction school and I would be evaluated
on the effectiveness of my direct instruction strategies. I was trained in whole group direct
instruction, whole group classroom management strategies, and how to keep my instruction
fast-paced to keep students engaged. As a teacher, I have done very well in my role at this
school. I believe I bring balance to my classroom and do the best that I can within the given
parameters of my job expectations—like so many of my colleagues do, as well. This school is
not unlike many of the public schools I have been to, nor is it unlike my own personal experience
as a child in Anchorage public schools. It does not, however, align quite as well with my own
personal philosophy of education and the pedagogy that I believe is best for the future of
learning.
The main difference of my teacher preparation programs and my current placement at a more
traditional school is the accountability of what to teach and when to teach it. In my previous
experiences I could choose the curriculum, adapt instruction to the child’s interests, and integrate
curriculum into projects and themes students want to study. Now, I am responsible to teach the
current curriculum in the order directed and to teach certain lessons and concepts within a certain
timeframe. Like all good teachers, we differentiate instruction to students, we respond to the
instruction and provide interventions, but in the end it still feels like I’m “spoon-feeding”
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 10
concepts and ideas that students either aren’t academically ready for, or they aren’t interested in
learning because they cannot personally see how it is relevant to their interests or goals, despite
our best efforts.
As I reflect on my experiences, I question if the traditional public school or “factory model”
of education is still relevant to our society, and if it is the most efficient use of teachers’ or
students’ time. How do we meet all the demands of a public school classroom and still cater
instruction to meet the needs of every individual learner? Our industrial revolution is over and
we are bursting with amazing feats in technological advancement in all aspects of human life. I
believe it is time our public schools not only catch up with the technology revolution to meet the
current needs of teachers and students, but lead the way in making knowledge and education
accessible to all students through new and exciting technological endeavors. The technology that
is available today can change the face of education all across the world.
Having a technology rich classroom is not something I ever thought I would be interested in,
but given my perception of the current state of public schools, I believe it is one of the best ways
to free up valuable teacher time to focus on more important aspects of the job. I believe it can
intellectually stimulate students in ways that direct instruction cannot. I believe students see
technology as being more relevant to their future and, therefore, they become more engaged with
what they are learning. As technology erupts in our society, many schools are pushing back on
technology use in education under the premise that students already have enough exposure to
technology outside of school. That is true, however, I believe that most students in low-income
Title I public schools are not literate in appropriate technology use, nor are they exposed to the
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 11
technology use that is going to benefit them in future careers which are becoming more and more
reliant on technology.
As a student myself, technology use in my schooling was minimal, even though I grew up
during the initial years of the technology revolution. Schools seem to be consistently behind in
catching up with technology in relation to the private sector. How can we prepare students for
work outside of the classroom when they are often taught in an environment not suited to teach
them the skills demanded in the workplace? Today, most schools have plentiful technology, and
this technology is often used in meaningful ways. However, there is still much opportunity for
technology to have a truly transformative impact for students, teachers, and administrators alike.
We use programs like the Google Classroom suite to replace worksheets, pencil and paper,
and give students a new way to present their projects. We have programs like IXL that give
students a series of questions for independent practice on grade level subjects. What is rarely
used in schools, which has the most potential to make student learning more tailored, with more
accurate data, and more accessible to all students is the cutting edge field of artificial intelligence
and machine learning.
Artificial intelligence is a broad term to describe a suite of technologies wherein complex
algorithms “learn” from vast amounts of historical data to produce valuable insights or tailor
experiences for individuals (Press, 2017). Some commonly known examples of artificial
intelligence and machine learning include: email spam filtering, Netflix movie recommendations,
Google search results, self-driving cars, disease diagnosis, and a host of other applications.
Within the educational arena, artificial intelligence and machine learning have been able to
differentiate instruction based on student responses and personalize education for all students
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 12
based on algorithms that target students at their zone of proximal development to accelerate
learning. These types of programs are not age or grade based and provide a much more detailed
and granular learner profile than any report card or teacher ever could (The Economist, 2017).
Artificial intelligence and machine learning can be impactful outside of classroom
instruction. For instance, machine learning algorithms are being used within a number of
educational applications, ranging from high school to college, to identify students at a high risk
for dropout. This provides staff and administrators with an opportunity to provide early
intervention and improve graduation rates and student outcomes (Lakkaraju, et al., 2015).
For teachers, artificial intelligence and machine learning can save valuable teacher time on
lesson planning and grading, as those tasks can be accomplished within the application, leaving
teachers with more time to focus on other important facets of providing a more whole-child
approach to education. Teachers can work on creating opportunities for hands-on activities,
social engagement, peer relationships, and guided instruction based on feedback from the
applications (The Economist, 2017).
This type of technology has the potential to transform the role of the teacher in the
classroom. During portions of the day where students are using machine learning applications,
teachers can act as a tutor using the feedback they receive to guide student understanding. They
can pull groups of students to do mini lessons based on the detailed data that comes from the
application or do other small group focus studies (The Economist, 2017). In an era of ever
increasing classroom sizes, teachers could be empowered to more efficiently use their time to
interact with students in more meaningful ways.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 13
Artificial intelligence and machine learning also have the potential to not only reach more
students figuratively, but literally as well. Students in rural areas, where quality education can be
scarce, can be provided with a tailored education experience – without the typical costs or
infrastructure required to operate an entire school (The Economist, 2017).
I also believe artificial intelligence and machine learning can eliminate the need for isolated
progress monitoring assessments, as student progress would be constantly recorded and
interventions could be provided instantly and seamlessly.
Students with learning disabilities can receive accommodations within the applications and
the tailored learning experiences can be made to meet their needs. Artificial intelligence and
machine learning can improve the lives of students with other impairments as well. For instance,
Microsoft has created an application for people with vision impairments that can describe in
detail whatever a wearable camera “sees.” This system utilizes machine learning to perpetually
increase its accuracy by “learning” from new examples. Similarly, Google uses machine learning
to automatically caption YouTube videos for people who are hearing impaired. IBM’s Watson
project has developed software which simplifies and clarifies idioms and figures of speech for
people with autism or dementia to clarify written content. I can also see this as being useful for
students whose first language is not English.
As a professional teacher working in a arguably regressive instructional regime, I have
formulated the following research questions:
1. What are the barriers to using artificial intelligence and machine learning programs in
public school settings?
2. How can artificial intelligence impact student engagement and learning?
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 14
3. How will artificial intelligence benefit students with and without learning disabilities?
4. How can artificial intelligence specifically aide students with physical impairments?
Like one of the great scientists of my generation, Bill Nye the Science Guy, said, “no
problem is too big when humans and technology collaborate” (Goldschein, et al., 2017). My
hope is that through open mindedness toward technological advancements in the fields of
artificial intelligence and machine learning, we can transform the face of traditional public
education in a way that alleviates stress, pain, and the feeling of an equivocal educational
experience.
1.3. The purpose of this meta-synthesis
This meta-synthesis, which focuses on artificial intelligence and machine learning for all
students in public education, has several purposes. One purpose is to review journal articles
related to the use of artificial intelligence and machine learning technologies in classrooms for all
students—specifically what barriers are there to using these programs. Another purpose is to
review journal articles on how artificial intelligence impacts student engagement and learning, I
am specifically interested in how artificial intelligence and machine learning can reach students
in rural settings where education is not easily accessible, and in large class sizes where teachers
aren’t easily able to make one-on-one connections on a regular basis with each learner. In
addition to this purpose, I am analyzing journal articles for the benefits and drawbacks artificial
intelligence and machine learning may have for students with and without learning disabilities.
Further, I will be analyzing technologies that use artificial intelligence to aide students with
physical impairments. My final purpose in conducting this meta-synthesis is to identify thematic
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 15
significances in the articles and connect them to my own teaching experiences in public
education classrooms with meeting students’ individual needs in education.
2. Methods
2.1. Selection criteria
The 43 journal articles and documents included in this meta-synthesis met one or more of
the following selection criteria.
1. The articles explored the use of artificial intelligence and machine learning programs in
educational settings.
2. The articles explored issues related to the use of artificial intelligence and machine
learning for students with and without disabilities.
3. The articles explored issues related to the impact of artificial intelligence on student
engagement and learning.
4. The articles were published in peer reviewed journals related to the field of education.
5. The articles were published between 2005-2017.
2.2. Search procedures
Database searches and ancestral searches were conducted to locate items for this
meta-synthesis.
2.2.1. Database searches
I conducted Boolean searches within four databases that index articles related to the field
of artificial intelligence and machine learning in regards to educational and classroom
applications. The four databases included the: (a) Education Resource Information Center
(ERIC, Ebscohost); (b) Education Journals (Proquest); (c) Education and Technology Journals
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 16
(Google Scholar); (d) Education and Technology Journals (IEEEXplore). Searches were
conducted using the following specific search terms:
1. (“Artificial Intelligence”) AND (“Classroom”)
2. (“Artificial Intelligence”) AND (“Classroom”) AND (“Children”)
3. (“Artificial Intelligence”) AND (“Education”)
4. (“Artificial Intelligence”) AND (“Education”) AND (“Machine Learning”)
5. (“Artificial Intelligence”) AND (“Teaching”) AND (“Technology”)
6. (“Demands on Teachers”) AND (“Technology”)
7. (“Differentiation”) AND (“Technology”)
8. (“Machine Learning”) AND (“Education”)
9. (“Machine Learning”) AND (“Primary Education”)
10. (“Technology”) AND (“Education”) AND (“History”)
The various database searches yielded a total of 27 articles that met my
selection criteria (Aberšek & Aberšek, 2012; Balakrishnan & David, 2010; Chin et al., 2010;
Chrysfiadi & Virvou, 2015; Clark & Whetstone, 2014; Du Boulay, 2016; Doom, 2016; Erstad,
Eickelmann, & Eichhorn, 2015; Gadanidis, 2017; Gonçalves, Fdez-Riverola, Rodrigues,
Carneiro, & Novais, 2015; Heffernan et al., 2016; Ireh, 2016; Jenkin, 2015; Kotsiantis, 2012;
Lakkaraju et al., 2015; Lin, Wang, Chao, & Chien, 2012; McArthur, Lewis, & Bishary, 2005;
Nabiyev et al., 2013; Pareto, 2014; Parkavi, Ramar, & Ramesh, 2013; Porter, 2017; Ramaswami,
2009; Roll & Wylie, 2016; Scalise, 2009; Timms, 2016; Wallace, McCartney & Russell, 2010;
Woolf, Lane, Chaudhri, & Kolodner, 2013).
2.2.2. Artificial Intelligence Journals
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 17
Due to the modest amount of current and applicable articles regarding artificial
intelligence and machine learning as it relates to education in a classroom setting in the above
mentioned databases, I also conducted searches within the International Journal of Artificial
Intelligence in Education.
These searches yielded a total of six articles that met my selection criteria (Chou & Chan,
2016; Kinshuk, Chen, Chang, & Chew, 2016; Koedinger & Aleven, 2016; Passonneau,
McNamara, Muresan, & Perin, 2017; Rosé & Ferscheke, 2016; Walker & Ogan, 2016)
2.2.3. Technology and Education Magazines and Online Resources
To augment the articles about artificial intelligence and machine learning as it relates to
education in a classroom setting in the above mentioned databases, I also conducted searches
within popular technology and education magazines on online resources
These searches yielded a total of seven articles that met my selection criteria (Ark, 2015;
Biedelman, 2018; Davison, 2016; Gaskell, 2016; Rizzotto, 2017; Petrilli, 2018; The Economist,
2017)
2.2.4. Ancestral searches
An ancestral search involves reviewing the reference lists of previously published works
to locate literature relevant to one’s topic of interest (Welch, Brownell, & Sheridan, 1999). I
conducted ancestral searches using the reference lists of the previously retrieved articles. These
ancestral searches yielded three additional articles that met my selection criteria (Arroyo, Woolf,
Cooper, Burleson, & Muldner, 2011; Stone et al., 2016; Zubrzycki, 2016).
2.3. Coding procedures
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 18
I utilized a coding form to categorize the information in each of the 43 articles. This
coding form was based on: (a) publication type; (b) research design; (c) participants; (d) data
sources; (e) findings of the studies.
2.3.1. Publication types
For this meta-synthesis, articles were evaluated and classified by publication type (e.g.
research study, theoretical work, descriptive work, opinion piece/position paper, guide, annotated
bibliography, review of the literature). Research studies use a formal and systematic method to
gather and/or analyze quantitative and/or qualitative data. Theoretical works discuss existing
literature to analyze, expand, or further define a specific philosophical and/or theoretical
assumption. Descriptive works describe phenomena and experiences but do not disclose
systematic methods of attaining data. Opinion pieces/position papers explain, rationalize, or
advocate a particular course of action based on the author’s opinions and/or beliefs. Guides give
instructions or advice explaining how practitioners might implement a new program and/or
policy. An annotated bibliography is a list of cited works on a particular topic, followed by a
descriptive paragraph explaining, evaluating, or critiquing the source. Reviews of the literature
critically analyze the published literature on a topic through summary, classification, and
comparison to identify essential themes of previously published work (Table 1).
2.3.2. Research design
Each empirical study was classified by research design (e.g. quantitative research,
qualitative research, mixed methods research). Quantitative research is the collection and
analysis of numerical data to relay information. Qualitative research utilized language to
describe issues, experiences, and phenomena. Mixed methods research is the combination of both
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 19
quantitative (e.g. numerical) and qualitative (e.g. non-numerical) research methods conducted
within a single study (Table 2).
2.3.3 Participants, data sources, and findings
I identified the participants in each of the studies (e.g. middle and high school students,
elementary and middle school students, and college level students ). I also identified the data
sources that were analyzed for each study (e.g. data sets from artificial intelligence and machine
learning programs, questionnaires and surveys, and assessments). Finally, I summarized the
findings of each study (Table 2).
2.4. Data analysis
I used a modified version of the Stevick-Colaizzi-Keen method previously employed by
Duke (2011) and Duke and Ward (2009) to analyze the 43 articles included in this
meta-synthesis. I first identified statements found to be significant in each article. For the
purpose of this meta-synthesis, I considered statements to be significant when they addressed
issues related to: (a) automation of basic activities; (b) teacher feedback; (c) individualized
learning, modified learning opportunities, and differentiation; (d) teacher’s role; (e) safe
educational risk taking; (f) accessibility; (g) balancing time spent with technology and traditional
instruction; (h) remote instruction; and (i) potential caveats and biases in artificial intelligence. I
then created a list of non-repetitive, verbatim significant statements with paraphrased formulated
meanings. The formulated meanings represented my understanding and interpretation of each
significant statement. Lastly, I grouped the formulated meanings from all 43 items into emergent
themes that represented the crux and content of the entire body of literature used for this
meta-synthesis (Table 3).
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 20
3. Results
3.1. Publication type
I located 43 articles that met my selection criteria. The publication type of each article is
located in Table 1. Fifteen of the 43 articles (34.8%) included in this meta-synthesis were
research studies (Aberšek & Aberšek, 2012; Arroyo, Woolf, Cooper, Burleson, & Muldner, 2011;
Balakrishnan & David, 2010; Chin et al., 2010; Clark & Whetstone, 2014; Doom, 2016;
Gonçalves, Fdez-Riverola, Rodrigues, Carneiro, & Novais, 2015; Kotsiantis, 2012; Lakkaraju et
al., 2015; Lin, Wang, Chao, & Chien, 2012; Nabiyev et al., 2013; Pareto, 2014; Parkavi, Ramar,
& Ramesh, 2013; Scalise, 2009; Wallace, McCartney, & Russell, 2010). Ten of the 43 articles
(23.2%) were theoretical works (Chou & Chan, 2016; Du Boulay, 2016; Gadanidis, 2017; Ireh,
2016; Koedinger & Aleven, 2016; McArthur, Lewis, & Bishary, 2005; Porter, 2017; Rosé &
Ferscheke, 2016; Stone et al., 2016; Timms, 2016). Ten of the 43 articles (23.2%) were opinion
pieces/position papers (Ark, 2015; Beidelman, 2018; Davison, 2016; Gaskell, 2016; Jenken,
2015; Petrilli, 2018; Ramaswami, 2009; Rizzotto, 2017;The Economist, 2017; Zubrzycki, 2016).
Five of the 43 articles (11.6%) were descriptive works (Erstad, Eickelmann, & Eichhorn, 2015;
Heffernan et al., 2016; Kinshuk, Chen, Chang, & Chew, 2016; Passonneau, McNamara,
Muresan, & Perin, 2017; Walker & Ogan, 2016). Three of the 43 articles (6.9%) were a review
of literature (Chrysfiadi & Virvou, 2015; Roll & Wylie, 2016; Woolf, Lane, Chaudhri, &
Kolodner, 2013).
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 21
Table 1
Author(s) & Year of Publication Publication Type
Aberšek & Aberšek, 2012 Research
Ark, 2015
Opinion Piece/Position
Paper
Arroyo, Woolf, Cooper, Burleson, & Muldner, 2011 Research Study
Balakrishnan & David, 2010 Research Study
Beidelman, 2018
Opinion Piece/Position
Paper
Chin, Dohmen, Cheng, Oppezzo, Chase, & Schwartz, 2010 Research Study
Chou & Chan, 2016 Theoretical Work
Chrysfiadi & Virvou, 2015 Review of Literature
Clark & Whetstone, 2014 Research Study
Davison, 2016
Opinion Piece/Position
Paper
Du Boulay, 2016 Theoretical Work
Doom, 2016 Research Study
Erstad, Eickelmann, & Eichhorn, 2015 Descriptive Work
Gadanidis, 2017 Theoretical Work
Gaskell, 2016
Opinion Piece/Position
Paper
Gonçalves, Fdez-Riverola, Rodrigues, Carneiro, & Novais, 2015 Research Study
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 22
Heffernan, Ostrow, Kelly, Selent, Inwegen, Xiong, & Williams,
2016
Descriptive Work
Ireh, 2016 Theoretical Work
Jenkin, 2015
Opinion Piece/Position
Paper
Kinshuk, Chen, Chang, & Chew, 2016 Descriptive Work
Koedinger & Aleven, 2016 Theoretical Work
Kotsiantis, 2012 Research Study
Lakkaraju, Aguiah, Shan, Miller, Bhanpuri, Ghani, & Addison, 2015 Research Study
Lin, Wang, Chao, & Chien, 2012 Research Study
McArthur, Lewis, & Bishary, 2005 Theoretical Work
Nabiyev, Vasif, Karal, Hasan, Arslan, Selahattin, Erumit, Kursut, &
Cebi, 2013
Research Study
Pareto, 2014 Research Study
Parkavi, Ramar, & Ramesh, 2013 Research Study
Passonneau, McNamara, Muresan, & Perin, 2017 Descriptive Work
Petrilli, 2018
Opinion Piece/Position
Paper
Porter, 2017 Theoretical Work
Ramaswami, 2009
Opinion Piece/Position
Paper
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 23
Rizzotto, 2017
Opinion Piece/Position
Paper
Roll & Wylie, 2016 Review of Literature
Rosé & Ferscheke, 2016 Theoretical Work
Scalise, 2009 Research Study
Stone, Brooks, Brynjolfsson, Calo, Etzioni, Hager, Hirschberg,
Kalyanakrishnan, Kamar, Kraus, Leyton-Brown, Parkes, Press,
Saxenian, Shah, Tambe, & Teller, 2016
Theoretical Work
The Economist, 2017
Opinion Piece/Position
Paper
Timms, 2016 Theoretical Work
Walker & Ogan, 2016 Descriptive Work
Wallace, McCartney, & Russell, 2010 Research Study
Woolf, Lane, Chaudhri, & Kolodner, 2013 Review of Literature
Zubrzycki, 2016
Opinion Piece/Position
Paper
3.2. Research design, participants, data sources, and findings of the studies
As previously stated, 15 research studies were located that met my selection criteria
(Aberšek & Aberšek, 2012; Arroyo, Woolf, Cooper, Burleson, & Muldner, 2011; Balakrishnan &
David, 2010; Chin et al., 2010; Clark & Whetstone, 2014; Doom, 2016; Gonçalves,
Fdez-Riverola, Rodrigues, Carneiro, & Novais, 2015; Kotsiantis, 2012; Lakkaraju et al., 2015;
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 24
Lin, Wang, Chao, & Chien, 2012; Nabiyev et al., 2013; Pareto, 2014; Parkavi, Ramar, &
Ramesh, 2013; Scalise, 2009; Wallace, McCartney, & Russell, 2010). The research design,
participants, data sources, and findings of each of these studies are identified in Table 2.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 25
Table 2
Authors
Research
Design
Participants Data Sources Findings
Aberšek &
Aberšek,
2012
Quantitative 34 college
students
E-learning
tools;
comparisons
Teaching technology without
pedagogy is empty and
useless, but artificially
intelligent learning tools,
when implemented well, can
increase student performance.
Arroyo,
Woolf,
Cooper,
Burleson, &
Muldner,
2011
Mixed
Methods
108 9th and
10th grade
students from
two high
schools (one
low and the
other high
achieving)
Pretest; survey;
questionnaire;
post-test
In the tutoring models (a type
of artificial intelligence and
machine learning program
that users interact with), all
students displayed advantages
to having a learning
companion in their machine
learning tutoring.
Interestingly, female students
showed more positive
attitudes toward content when
their learning companion was
female as opposed to male
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 26
students who showed no
preference. This study relates
to artificial intelligence and
machine learning for students
because it helps program
developers and educators to
identify what components of
tutoring programs students
will be more engaged in and
can make better choices in
meeting the needs of learners
as they interact with artificial
intelligence and machine
learning.
Balakrishnan
& David,
2010
Quantitative Data sets
from 513
school-aged
children
identified as
having a type
of learning
disability
Rough sets and
decision trees;
comparisons
Rough set machine learning
algorithms can be highly
effective in predicting
students with learning
disabilities and can identify
the signs and symptoms of
the specific learning
disability. This means that
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 27
well-designed artificial
intelligence and machine
learning programs used in
schools can help educators
properly identify students
with learning disabilities, so
that teachers can provide
appropriate accommodations
for those learners in all areas
of the classroom.
Chin,
Dohmen,
Cheng,
Oppezzo,
Chase, &
Schwartz,
2010
Quantitative 6 teachers
and 1,034 5th
grade
students
Pretest;
teachable
agents;
summative
assessments
There are many valid
concerns for the future of
technology in the classroom,
but artificial intelligence in
the form of teachable agents
(learning companions that
students demonstrate their
learning to, learn from, and
interact with) can add value
to student learning without
taking away from the content
taught in the traditional
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 28
curriculum despite
“lost-time” in traditional
learning settings. They can
also prepare students to learn
new content from regular
classroom instruction even
when not directly using the
software. When students use
these artificial intelligence
programs they are better able
to understand future concepts
not yet introduced in regular
classroom instruction as a
way to build background
knowledge.
Clark &
Whetstone,
2014
Mixed
Methods
35 teachers
from 15
participating
elementary
school and
2,542
students in
Survey; data
sets from
artificial
intelligence
software
platform
The usage of the machine
learning software was
strongly related to
improvement in student
ability and the combination of
usage with regular instruction
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 29
grades K-5th
grade
resulted in significant gains
for low-performing students.
Doom, 2016 Mixed
Methods
16 teachers
from 7
districts and
112 students
in K-6th
grade
Surveys; pre-
and post;
student
comprehension
scores
Results indicate that there is
no significant impact on
student achievement in
relation to teacher perception
of blending tradition learning
and artificially intelligent
learning programs. All
students made gains, but what
this research study
specifically proved was that
when teachers were
uncomfortable with the
technology or had negative
assumptions of technology,
students still made growth in
their learning in the same way
as students with classroom
teachers  having positive
biases toward technology. So,
even when instructors have
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 30
their own personal beliefs and
assumptions, if teachers use
the technology, learning will
not be negatively impacted.
Gonçalves,
Fdez-Riverol
a, Rodrigues,
Carneiro, &
Novais,
2015
Quantitative 34 high
school
students
Socio-economic
characterization;
computer
monitoring
Including new technologies in
the classroom can come with
its own set of problems and
drawbacks including student
engagement, staying on track,
and committing to learning
with so many easily
accessible temptations on the
computer. Some artificial
intelligence and machine
learning programs are starting
to include functions allowing
the monitoring of student
interaction with software
programs using performance
sensors  during technology
usage in the classroom. This
can show the teacher if the
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 31
student is in the program and
working at an appropriate
rate. These inclusions can
provide high level
information to the teacher on
individual student
management and intervention
in regards to student fatigue,
stress and attention. The
research concludes that
monitoring student progress
in the program can signal to
the teacher when the learner
may need intervention to
interacting with the
technology (teacher
motivation, help, breaks,
etc.).
Kotsiantis,
2012
Quantitative 354 college
student
records
Classification
and regression
algorithms;
sequential
Integrated machine learning
systems can cater to the
individual needs of an
institution or student.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 32
pattern analysis;
clustering; web
mining
Lakkaraju,
Aguiah,
Shan, Miller,
Bhanpuri,
Ghani, &
Addison,
2015
Quantitative Two school
districts with
combined
data sets
from 24,787
students in
grades
6th-12th
Historical data;
lists of student
attributes
Machine learning approaches
can predict students at risk of
not graduating on time due to
adverse attributes allowing
schools to be proactive in
improving educational
outcomes for identified
students.
Lin, Wang,
Chao, &
Chien, 2012
Mixed
Methods
40 people Questionnaire;
facial-recognitio
n software
Facial-emotional recognition
to respond to learners
emotions in tutoring systems
increases user motivation and
learning.
Nabiyev,
Vasif, Karal,
Hasan,
Arslan,
Selahattin,
Erumit,
Mixed
Methods
Four teachers
and 59
students in
10th grade
Course grades;
interviews;
questionnaire
Artificial intelligence based
distance education models
can be responsive to the
needs of students, but
preferences of not using
screens for learning was
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 33
Kursut, &
Cebi, 2013
shown in the population of
students who are more
familiarized with traditional
learning on paper with pencil.
Pareto, 2014 Mixed
Methods
443 students
with and
without
disabilities
ranging from
grade 2nd-8th
Tests; pre- and
post;
questionnaire;
game-logging
database;
survey;
Artificial intelligence tutoring
programs that are game-based
can engage primary students
both with and without special
needs, and students can learn
from these games.
Parkavi,
Ramar, &
Ramesh,
2013
Mixed
Methods
900
secondary
students from
nine schools
in a single
district
Questionnaire;
school records
Student grades could be
predicted based on students
attributes not related to
education, such as parental
occupation, allowing
institutions to identify at-risk
students and provide
early-interventions.
Scalise,
2009
Quantitative 521 high
school and
university
students
Assessment Artificial learning
differentiation approaches
were not often used by
students when they were
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 34
given the choice of
modification. This means that
when accommodations were
built into artificial teaching
programs many students with
learning disabilities elected
not to use them even when
made available.
Wallace,
McCartney,
& Russell,
2010
Mixed
Methods
Junior and
senior
university
students
enrolled in
computer
programming
courses
Survey;
comparisons
between
game-based vs.
non-game-based
projects;
observations
Artificially intelligent
game-based learning is
effective in student
engagement based on student
feedback and instructor
observation.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 35
3.2.1. Research design
Eight of the 15 studies (53.3%) utilized a mixed methods research design (Arroyo, Woolf,
Cooper, Burleson, & Muldner, 2011; Clark & Whetstone, 2014; Doom, 2016; Lin, Wang, Chao,
& Chien, 2012; Nabiyev et al., 2013; Pareto, 2014; Parkavi, Ramar, & Ramesh, 2013; Wallace,
McCartney, & Russell, 2010). Seven of the 15 studies (46.7%) used a quantitative research
design (Aberšek & Aberšek, 2012; Balakrishnan & David, 2010; Chin et al., 2010; Gonçalves,
Fdez-Riverola, Rodrigues, Carneiro, & Novais, 2015; Kotsiantis, 2012; Lakkaraju et al., 2015;
Scalise, 2009).
3.2.2. Participants and data sources
The majority of the 15 research studies included in this meta-synthesis analyzed data
from students ranging from elementary school to college, Some of the research studies included
collected data from teachers and some of the research studies included data collected from
students with disabilities. Six of the studies (40%) analyzed data collected from middle and high
school students (Arroyo, Woolf, Cooper, Burleson, & Muldner, 2011; Gonçalves, Fdez-Riverola,
Rodrigues, Carneiro, & Novais, 2015; Lakkaraju et al., 2015; Nabiyev et al., 2013; Parkavi,
Ramar, & Ramesh, 2013; Scalise, 2009). Four of the studies (26.7%) analyzed data from
elementary and middle school aged children (Chin et al., 2010; Clark & Whetstone, 2014; Doom,
2016; Pareto, 2014). Three of the studies (20%) analyzed data from college level students
(Aberšek & Aberšek, 2012; Kotsiantis, 2012; Wallace, McCartney, & Russell, 2010). Two of the
studies (13.3%) analyzed data from a selection of various aged people (Balakrishnan & David,
2010; Lin, Wang, Chao, & Chien, 2012).
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 36
Data sets from artificial intelligence and machine learning computer programs and the
utilization of surveys and questionnaires provided the main data sources used in the research
studies. Ten of the 15 studies (66.7%) used data sets from artificial intelligence and machine
learning computer programs (Aberšek & Aberšek, 2012; Balakrishnan & David, 2010; Chin et
al., 2010; Clark & Whetstone, 2014; Gonçalves, Fdez-Riverola, Rodrigues, Carneiro, & Novais,
2015; Kotsiantis, 2012; Lakkaraju et al., 2015; Lin, Wang, Chao, & Chien, 2012; Pareto, 2014;
Wallace, McCartney, & Russell, 2010). Four of the 15 studies (26.7%) used surveys and
questionnaires to collect data (Arroyo, Woolf, Cooper, Burleson, & Muldner, 2011; Doom, 2016;
Nabiyev et al., 2013; Parkavi, Ramar, & Ramesh, 2013). Other data sources were also used in the
research studies including, pre and post-test assessments, summative assessments, student
grades, student demographic and records analysis, and observations.
3.2.3 Findings of the studies
The findings of the 15 research studies included in this meta-synthesis can be
summarized as follows.
1. When it comes to creating and choosing artificial intelligence and machine learning
programs for teaching students new concepts or building on to their current knowledge, most
students respond well to having a teachable agent/learning companion that they can relate to.
Gender and cultural identity need to be considered to encourage students to feel included and be
responsive to the learning environment.
2. Well-designed artificial intelligence and machine learning programs can work with
teachers to identify important risk factors for students including those with learning disabilities
and/or students who are at risk of adverse learning outcomes.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 37
3. Artificial intelligence and machine learning programs can work collaboratively with
traditional instruction to create a well-rounded blended learning environment and provide
interventions for students at their appropriate level, and they can even build on student
understanding and pre-teach concepts related to what the class is working on.
4. Game based learning in artificial intelligence and machine learning programs serve as
a motivational and engaging tool that teachers struggle with in a traditional setting – keeping up
with the fast paced expectations of students.
5. Some students, specifically those with learning disabilities, may choose to not use
provided accommodations even when they are offered.
6. Computers could track student work and provide teachers with opportunities to
intervene when students are struggling.
7. The more comprehensive and integrated artificial intelligence and machine learning
programs are, the more feedback and data teachers can use to help students and, subsequently,
the more valuable the programs can be.
3.3. Emergent themes
Seven themes emerged from my analysis of the 43 articles and documents included in
this meta-synthesis. These emergent themes, or theme clusters, include: (a) redefining the
teacher’s role within the classroom; (b) teacher feedback; (c) individualized learning, modified
learning opportunities, and differentiation; (d) safe educational risk taking; (e) accessibility and
remote instruction; (g) potential caveats and biases in artificial intelligence and machine
learning; and (h) ideas for development. These seven theme clusters and their formulated
meanings are represented in Table 3.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 38
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 39
Table 3
Theme
Clusters
Formulated Meanings
Redefining the
Teacher’s Role
within the
Classroom
● Schools that use a learner-centered pedagogy are more effective at
integrating new artificially intelligent technologies that are tailored to
students’ learning needs in the classroom.
● Artificially intelligent tutors will likely take over traditional instruction
as the teacher of new information, but cannot replace a teachers’ role as
mediators and human beings, facets highly important for the whole
learning process.
● When teachers emphasize casual integration of artificial intelligence,
students perform well on summative tests.
● Incorporating new technologies in the field of artificial intelligence and
machine learning added value to teacher instruction and did not hinder
student achievement considering “lost” instructional time given to the
technology.
● Learning companions within artificially intelligent environments can
serve as motivation for students by increasing engagement,
responsibility, and completion of tasks.
● Artificial intelligence tutoring programs can effectively be used with
traditional instruction as a small group rotation for intervention or
enrichment depending on student need.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 40
● Results from artificial intelligence tutoring software were greatest when
students were actively engaged, and engagement increases when teachers
monitor student activities.
● Combining artificial intelligence education with traditional instruction
can free up teachers to work with students in smaller groups.
● With large, and often increasing class sizes, blending artificial
intelligence education is more successful than traditional instruction, but
not as successful as one-on-one human tutoring.
● Integrating artificial intelligence education into classrooms as
one-on-one computer tutors, with teachers working with small groups, is
beneficial to student learning.
● Despite the teacher’s perception or belief about new technologies being
incorporated into their classroom instruction, student growth or
regression was not impacted.
● With artificially intelligent classroom models, teachers still serve as the
important role as facilitators of the learning process, and the use of these
technologies gives teachers more flexibility in connecting subjects,
student grouping, and collaborating with other schools, companies, or
other learning organizations.
● When artificially intelligent education introduces students to themes or
instructs students, the teacher is able to focus on deeper learning with his
or her students.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 41
● Schools that adopt new technologies as educational resources, give
teachers a new role as directors for student learning.
● Research supports the importance of teachers in the education of
students using artificial intelligence for instruction as students require
adjustments and guidance in their learning environments, resources, and
different needs.
● New technologies create a different type of demand on teachers by
facilitating new ways of working with students, accessing information
and communication, and applying new technologies in the classroom.
Some new technologies can initiate opportunities for the teacher to help
students problem solve and more deeply understand what they are
learning across content and domains.
● Using artificial intelligence software to supplement classroom instruction
will help students understand concepts better and help them improve
their skills faster.
● Meeting students’ needs requires teachers to personalize, inspire interest
and creativity, demonstrate value, and differentiate instruction and
motivation. Incorporating artificial intelligence can help teachers meet
these demands for the many learners in their class.
● Teachers can serve as tutors to help with academic work and work with
students on building positive character, curiosity in learning, and
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 42
self-awareness in small groups while artificially intelligent machines
teach students core academics.
● Teachers save time by not having to spend time on grading or lesson
plans for academic instruction and instead use that time to plan
interventions based on student data.
● Relationships students build with artificial intelligence can complement
human relationships by creating deeply engaging effective learning
experiences.
● No matter how much a teacher achieves in the classroom, society will
continue to expect more from education and since we are unable to meet
the demands of society, nor many of the educational needs of students,
artificial intelligence can meet some of the 21st century skills digital
natives are comfortable with in an environment that is personalized to
their traits and affect.
● Many students are failed by the current educational practices and
artificial intelligence may be the answer to mitigating that problem.
● Artificial intelligence can transcend traditional learning institutions and
have a greater impact on lifelong learning, continuing education, and
professional development.
● Teachers are overwhelmed with their job responsibilities and one
solution for that problem can be from the use of smart technologies.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 43
Teacher
Feedback
● When teachers are included in the conversation of data analysis, not just
data collection, they are better at integrating the data to provide
customized learning solutions tailored to student needs, content area, and
student demographics. Artificial intelligence data collection can provide
instant feedback on student aptitude directly to teachers.
● Allowing teachers to be closely connected to student data collected from
artificial intelligence software, in areas of academics, social, behavioral,
and emotional development, can help teachers understand the whole
child better.
● Students who used teachable agents in artificially intelligent software
programs prior to teacher instruction demonstrated a deeper casual
understanding of the content and built longer chains of inference with
related content, which teachers could then respond to in their instruction.
● Artificially intelligent machine learning systems can help teachers and
school administration identify student who are at–risk of unfavorable
school outcomes, such as dropping out, and what might be causing them
to struggle in school. The programs can provide schools with insights
and interventions to assist the student in overcoming their learning
challenges.
● Machine learning algorithms in education can help schools make
predictions of student outcomes, scheduling students and classes, and
automating grading and organizational tasks.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 44
● Data-mining through artificial intelligence and machine learning
provides teachers with new methods to track student learning and
development.
● Data from tailored and customizable learning experiences allows
teachers to adjust their instruction and help at-risk students by
intervening when students are not understanding concepts at the
individual or whole class level.
● Teachers, administration, and students can use integrated machine
learning systems to organize their learning needs, make predictions about
their learning, and meet their diverse educational needs.
● Machine learning can predict for teachers and students how a student
will perform in their class and what grade they will likely receive based
on student data.
● Machine learning algorithms can predict students who will likely not
graduate on time, allowing teachers to provide intervention.
● Data mining using machine learning algorithms have helped teachers and
administration provide extra support and better tutoring for at-risk
students based on predictions of grades, dropping out, and other adverse
educational outcomes from socio-economic data.
● Advanced technologies can provide teachers with more insights and data
analysis to help them make more informed decisions for their students.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 45
● New technologies can assess current instruction and guide teachers to
respond to their own instruction in real time by guiding teachers to ask
higher-order questions to engage students and encourage higher
achievement by students.
● Machines as teacher observers use algorithms to detect specific findings
in teacher instruction and can help teachers monitor student engagement
and guide teachers in teaching at a student’s zone of proximal
development for optimal growth.
● Students’ assignments and tests can be analyzed by machine learning
algorithms for data collection and response to instruction.
● Data mining and machine learning will improve classroom instruction
regardless of how technologically enriched the instruction may be.
● Artificial intelligence has the power to make standardized tests obsolete
as artificial intelligence products provide more meaningful data that is
less disruptive to the student, and it can tutor students in fundamental
concepts and then assess the students’ knowledge.
● Augmented realities and artificial intelligence can very accurately read
psychological responses to stimuli constantly in real time and even
predict student actions; if the technologies can understand a learner’s
relationship to a subject unconsciously then standardized testing may be
unnecessary.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 46
● Artificial intelligence powers teachers’ ability to deliver information to
students.
Individualized
Learning,
Modified
Learning and
Differentiation
● Machine learning algorithms have been faster, more accurate and less
labor intensive in the detection of students with learning disabilities.
● Artificial intelligence and machine learning can easily diagnose a child’s
disability at an early age to help families, teachers, and schools know to
implement early intervention services for that child.
● Females and minority students, specifically those from poverty, felt more
confident and had greater learning gains when their artificially intelligent
learning companions were of their same gender or racial
identity—especially when the learning companion spoke in a similar
vernacular as the student.
● Quality education will always require actively engaged human teachers,
but artificial intelligence can enhance learning at all levels by
personalizing instruction.
● Students using artificially intelligent tutoring programs were better able
to transfer their understanding from one specific content area to other
seemingly unrelated domains.
● Artificially intelligent programs provide students with the capability and
centralization to see and apply cause and effect relationships to their
learning.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 47
● Learning through artificial intelligence is able to incorporate many
elements of pedagogical models for deep and complex learning.
● Artificially intelligent tutoring systems are able to collect background
information on student understanding and what they haven’t yet learned
to cater instruction to the individual child’s learning needs.
● Artificial intelligence can be used to dynamically modify and adapt the
learning process to the child’s knowledge level and understanding.
● Blending traditional instruction with artificial intelligence education
provides all students with initial instruction, but then has the ability to
embed scaffold supports, enrichment, and hints to guide student
instruction.
● Personalization of artificial intelligence in education provides students
with a holistic, learner-centered educational experience while also
meeting the demands of systematic changes in education.
● Because individuals learn at different speeds, start at different points, and
come from different backgrounds, artificial intelligence education
reflects a more complex reality than traditional classroom practices.
● Artificial intelligence uses adaptability to create more tailored and
customizable learning for students.
● Teachers can have access to data that monitors the effects of fatigue,
stress, and attention in real time at the individual level and are then able
to intervene and manage students appropriately.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 48
● Artificial intelligence can read user emotional expression and can choose
how to respond to that learner and can deepen the application’s level of
human interaction.
● Students with and without special needs were engaged in learning
mathematical concepts from teachable agents in artificially intelligent
learning programs.
● Because of the variety of gaming and instructional models at varying
difficulty levels that can be made available in artificially intelligent
operating systems, students with a wide-range of ages and ability are
able to have appropriate and meaningful learning opportunities at their
skill level.
● Artificial intelligence in education is very personalized and can focus on
intervention and the individual’s education and can accelerate higher
order thinking or remediation when needed.
● Artificial intelligence can help students learn to mastery, experiment in
learning, and have a personalized learning experience that could have a
lasting effect on education.
● Students have greater academic achievement when they receive a
personalized education.
● Artificial intelligence in education has the possibility to truly understand
each learner as an individual by adjusting educational experiences in
response to students’ emotional and intellectual engagement. The
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 49
software can customize all experiences to maximize learning and
interest, creating truly unique interactive experiences for personalized
mastery-based education.
● Artificial intelligence allows students to choose the form or shape of
their artificially intelligent instructors for a lesson.
● To truly provide learners with an individualized education, the only way
to achieve it is through artificial intelligence. Their digital abilities and
limitless access to information allows them to understand content and
students better than teachers can, given the constraints of teacher time
and classroom size.
● Instruction can be differentiated with different types of media,
interactivity, and student responses that can collect data and deliver
custom content seamlessly.
● Machine learning systems will learn  as the students learn, providing
rapid advances in our understanding of how students learn and provide
rich data to accelerate development and personalize education.
● Learning tutors in artificial intelligence can accelerate learning in
children.
● Artificial intelligence in education can adaptively create social
relationships with their learners to model learning and provide adaptive
cognitive support for students with disabilities.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 50
● Artificial intelligence has the ability to make project based learning the
pinnacle of educating students to make all learning relevant to the
individual.
Safe
Educational
Risk Taking
● In artificially intelligent and machine learning models, students can serve
as the tutee, but can also serve as the tutor to their artificial intelligence
learning companion to help students solidify their learning in a safe,
nonjudgmental environment.
● Students using artificial intelligence for instruction developed positive
affect and attitudes toward the content area.
● Students can control their own learning process from choosing their
teachers and undertaking assessments at their own time.
● The learning models can look the same or different for all students,
wherein students, regardless of ability, aren’t singled out as having a
different learning path as their same age peers.
● Disenfranchised countries and people living in poverty can access
education at a more affordable cost than attending tuition-based
educational institutions.
● Scaffolding and differentiation within learning models can be designed
imperceptibly during student learning.
Accessibility
and Remote
Instruction
● Low achieving students have had the greatest benefit and educational
gains from artificially intelligent software programs.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 51
● Low performing students made significant gains in their understanding
when teachers combined their regular instruction with artificial
intelligence tutoring.
● Artificially intelligent learning environments could be able to
contextualize the learning process for students by considering their
location, environment, proximity, and living situation to generate better
learning experiences based on these factors.
● Traditional education leaves struggling students with years of cumulative
knowledge gaps; artificial intelligence can make foundational skills
accessible to students with significant learning gaps.
● Immersive technologies, such as augmented reality, with artificial
intelligence reshapes student relationships with information.
● Artificial intelligence can create educational experiences for
collaboration from vastly different locations in the globe and can pair
students heterogeneously, homogeneously, or by interests.
● Classrooms can take any form as an abstract place where human
connection, collaboration, and learning can exist.
● Artificial learning environments can be much more cost effective as they
reduce costs in real estate, human labor, and materials.
● Artificial intelligence has the ability to translate material to different
languages rapidly and with fair accuracy, which will provide access to
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 52
educational materials to students in different countries who don’t have
access to textbooks and materials in their spoken language.
● Students with the greatest poverty had greater improvements in ability
using artificially intelligent learning programs.
● Artificially intelligent learning technologies have a similar social
influence on students as relationships with peers, teachers, and
communities.
● Artificial intelligence based learning systems can impact remote
instruction in many ways by connecting learners together, providing
access to materials and learning tools, and engaging students in
meaningful ways.
● Pressure to contain costs and serve a large number of students can be
alleviated with the blending of formal classroom educational experiences
and self-paced, individualized education through artificial intelligence.
Potential
Caveats and
Biases in AI
and ML
● Student attitudes and unproductive behaviors can be impacted by the
learning environment within the artificially intelligent software.
● Technological innovation is constantly changing, while research is
lagging as researchers have difficulty keeping up with the pace of
innovation.
● Increased complexity in the classroom with addition artificially
intelligent educational programs can be a challenge for teachers as they
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 53
often serve as the bridge to student learning and applying knowledge to
real life applications.
● Some research supports the notion that education systems that have
invested heavily in computers haven’t seen the improvement they were
expecting.
● New technologies can be disruptive to student learning.
● Removing the distraction of screens and electronics from the classroom
can encourage engagement between the teacher and the student.
● Because technology changes and evolves so rapidly, some people believe
students should only be learning core skills instead of how to use and
manipulate technology.
● Teaching is about human interaction and children shouldn’t learn
through machines at a young age.
● Students who are in the habit of using traditional pencil and paper
methods of learning were resistant to giving up those methods even
when they found artificially intelligent learning systems to be successful
in terms of learning and problem solving.
● Student engagement is directly correlated to results in productive
learning in artificial intelligence learning environments.
● Encountering political hurdles in the advancement of artificial
intelligence in the classroom may have an effect on how well and how
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 54
far the new technologies can be used, considering privacy issues of data
mining with teachers and parents.
● Learning how to integrate face-to-face human interaction with artificially
intelligent technologies remains a challenge for research.
● Ethics in artificial intelligence provide challenges to development as
software can be developed to lie, manipulate, or encourage deep
relational interactions with learners.
● If we are unable to adopt new strategies afforded by artificial intelligence
in student learning, our educational systems will likely fail to meet the
challenges students will have facing them in the future.
Ideas for
Development
● Gender differences will need to be considered in the development and
implementation of artificial intelligence software considering the
extensive research on brain-based differences between how the different
genders learn, get help, problem solve, and respond to motivational
techniques.
● Teachers will be the most successful at implementing new artificial
intelligence and machine learning software into their classrooms when
professional development strategies are truly integrative by providing
on-going support, on-the-job training on data collection and analysis, and
adjusting teaching to respond to student data.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 55
● Intelligent computer based learning can be a very useful tool for
education, but will require careful design and extensive testing to rule
out any unfavorable outcomes.
● Teaching and learning happens in the classroom; when considering new
research and changing practices, students will learn and achieve more
when our society invests more time and resources into the classrooms as
well.
● Design must engage students by matching challenges and difficulties
with incentives and motivational activities.
● Game-based learning in artificially intelligent systems increase student
engagement.
● Effective questioning from learning agents in artificially intelligent
programs helped students transfer their learning to real life applications.
● Learning agents in artificially intelligent programs need to be designed
to meet the cognitive and social development of young children for them
to be appropriate. These programs may not be appropriate for children
under the age of 8.
● Artificial intelligence education has the potential to be incorporated in
the classroom in ways outside of the computer or iPad and can be
embedded in other devices such as robots, sensors and smart classrooms
making the educational experience more engaging to help students learn
and to help teachers teach more effectively.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 56
● Data mining from smart classrooms could lead to new understandings of
how people and learners behave in life.
● New technologies cannot impact education in isolation, but rather as a
resource to a complex system that must always consider pedagogy,
environment, and instruction.
● Combined efforts of engineers, psychologists, and educators are needed
to create the most appropriate educational model that encompasses
artificial intelligence and is easily accessible and usable in education.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 57
4. Discussion
In this section, I have summarized the emergent themes from my analysis of the 43
articles included in this meta-synthesis. I have connected these emergent themes to my present
teaching practices and to my personal and professional experiences as a consumer and educator
of smart technologies. I will also provide personal and professional experiences and discuss how
these theme clusters will continue to impact me as an educator.
4.1. Redefining the teacher’s role within the classroom
The need for revaluating the teacher’s role in the classroom stems from the increasing
and overwhelming workload, stress, and pressure teachers are under in today’s society in
combination with the development of new technologies that change the way people interact with
and access new information in the world around them. Considering the many advancements in
smart technologies in today’s society and the general lack of changes in content delivery in
educational institutions, many students will be unprepared to work in environments rich with
these technologies and the 21st century skills that these jobs require. Private schools and public
schools in wealthy communities do not face the same challenges that most public schools are
burdened by, and are often afforded more opportunity to deviate from the traditional teacher’s
role as lecturer to tutor, mentor, and facilitator of rich educational experiences. Moving from
traditional instruction to individualized learning with support in high-quality critical thinking
skills is of paramount importance to students who will need to compete in a shifting job market.
Societal and individual student demands that are placed on teachers are increasingly
difficult to meet. With automation of tasks such as grading, assessment, and initiating new
content, teachers are able to utilize rich data to increase instruction, compliment information
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 58
students are learning, build background, and encourage inference making across domains. With
increasing expectations and class sizes, and decreases in planning time and professional
development, artificial intelligence learning systems can mitigate many of the pressures in the
classroom and allow educators to focus on the most important parts of teaching.
These statements demonstrate that the teacher’s role within the classroom is highly
important in creating an educationally rich environment and can arguably make the teacher able
to do their job better, reach more students, and enrich the learning experiences of their students.
Until our society is able to provide teachers with small class sizes, time for professional
development and rich meaningful lesson planning, and appropriate supports for adversity in the
classroom, artificial intelligence may be the best way to give all students and teachers equality in
their education and instruction. Artificial intelligence will enrich the instruction that students
receive by freeing up teachers’ time to focus on deeper learning opportunities, while providing
teachers with superior feedback on student learning to make their instruction or tutoring the most
appropriate and attainable for learners. Artificial intelligence and smart technologies in the
classroom can seem intimidating to teachers, but I believe that including these advancements in
learning into our teaching practices can make our jobs as educators less task-oriented, less
stressful, and more impactful to student learning and acquisition. Students will in-turn be more
prepared for new job skills and real world knowledge that will be required of them in
competitive job markets.
Artificial intelligence technologies will never replace the importance of teachers in
quality instruction, but can make teachers even more meaningful, valuable, and essential to
building a broad foundation of skills and knowledge in students.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 59
4.2. Teacher feedback
Data-driven decision making has been an important aspect of teacher feedback in
education throughout formal learning institutions. Data collection and analysis has been highly
mandated by policy makers and administration as a means to make data based decisions in
districts and schools, as well as to criticize poor performing schools. Educators are often left out
of the analysis part of data-driven decision making and serve mostly as data collectors, especially
in regards to high-stakes standardized testing, which is often criticized for harming students’
attitudes toward learning, showing poor correlation to students’ ability, and introducing added
pressure for teachers to teach to the test, not to ability. Artificial intelligence and machine
learning uses data-mining techniques to collect incredibly precise data on students’ emotions,
affect, social behaviors, background, academics, and much more. The data is much more useful
and provides teachers, administrators, and policy makers with more data points which help them
to understand the whole child. In fact, a large amount of research supports the idea that artificial
intelligence and machine learning can make standardized testing obsolete. Data that machine
learning and artificial intelligence can provide is much more accurate in collecting data and
understanding what students know and have not yet learned, and there technologies are able to
collect the data in a much more harmonious and less intrusive way—creating less problems with
disrupting student learning and causing adverse reactions from students and teachers. Artificial
intelligence can be so subtle and precise that it can accurately pick up on nearly unidentifiable
physiological reactions to certain stimuli that can provide data for teachers on students’
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 60
familiarity with certain content based solely on how their body responds to the material. If we
have accurate data on how a student unconsciously reacts to information they know or do not
know, then the whole idea of testing student ability in those instances becomes unnecessary—we
wouldn’t need to test students to collect duplicative test data that supports them knowing or not
knowing that specific content.
Even more valuable, teachers can strengthen their instruction based on the rich student
data they can receive from artificial intelligence and machine learning programs. The data can
guide interventions, enrichment, and instruction for teachers and provide them with more
information on student learning and their learning process at large. Artificial intelligence
education has been able to accurately identify students with learning disabilities and provide
teachers with interventions for those students. Data-mining algorithms have also been able to
identify at-risk students for adverse educational outcomes, such as failing courses or dropping
out of school, to guide teachers and administration to provide supports for those students.
If a teacher is not using technology in their instruction of students, they can still benefit
from artificial intelligence and machine learning in their classroom. Artificial intelligence and
machine learning is able to assess student engagement, understanding of content, and the quality
of teacher questioning to provide feedback on teacher instruction and change teaching practices
to make instruction more valuable for learners, even if students themselves don’t interact with
these systems.
Even if schools shy away from using artificial intelligence and machine learning
programs in their classrooms on computers or screens, the feedback teachers and schools can
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 61
collect and analyze from artificial intelligence can be highly beneficial for teaching, learning,
and helping students achieve to their highest potential.
4.3. Individualized learning, modified learning and differentiation
Personalization in education is possibly the greatest way that artificial intelligence can
change educational experiences for learners. The “one size fits all” approach to teaching is a
great disservice to our children and to our society. All people have different interests, aptitudes,
and funds of knowledge—why shouldn’t that be reflected in educating different people?
Instruction using artificial intelligence and machine learning can start at different places,
take different paths and end completely different depending on the user and the data the program
receives from that user. Artificial intelligence can make learning completely customizable and
individualized. Research suggest that students, specifically those that are female or other
marginalized groups, learn best when they work with peers, tutors, or instructors that look and
speak like them. Artificial intelligence allows the teacher or the student to choose who or what
their learning companion will be—the options could be endless.
Individualization, modified learning and differentiation is embedded in artificial
intelligence. It can consider all facets of a student and create a learning program that is the most
engaging and meaningful for that individual. Students have the power to make choices based on
their interests and passions and make learning revolve around their ability and thirst for
knowledge. Passion projects and project based learning could be the center of which all other
content and instruction revolve.
4.4. Safe educational risk taking
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 62
Many students are personally bridled by their fear of embarrassment, looking or feeling
“stupid,” or negative peer reactions in their learning and acquisition of new knowledge. There
are many ways that teachers can create environments where students feel safe and valued, so that
they will take those risks to expand their depth of learning. Still some students may not respond
as easily to teacher initiatives for students to make those leaps especially if they have had
negative experiences in previous settings in school or at home and in the community.
Artificial intelligence and machine learning education can provide students with the
opportunity to take safe educational risks in an environment where they can be their true and
authentic self without any fear of negative reaction. Students who might be afraid of teaching a
peer a new concept in the classroom might be more willing to take that risk with an artificial
learning companion in their smart program.
When students feel like they struggle in an area they often build negative attitudes and
behaviors toward that subject. A student who has experience performing poorly in math will say
they hate math or even act up in math class to try to get out of having to display their lack of
understanding in front of their teacher or classmates. Artificial intelligence education can provide
supports in developing positive attitudes as well as academic supports for learners that do not
single them out from their classmates. All programs can look similar and different at the same
time, making students unaware that some classmates may be at a different ability level than
themselves.
4.5. Accessibility and remote instruction
Artificial intelligence education opens the door for learners in remote education settings
or in places where education is of low quality or unaffordable. Artificial intelligence requires less
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 63
resources and less additional expenses that many societies have difficulty providing. Many
students who have accessibility issues to information or schooling can more easily access vast
amounts of materials and learning experiences through artificial intelligence. These programs
can also respond and react to student information to provide materials in their language,
customize learning to their life experiences, and make learning more personalized for their
unique needs.
Students with learning and physical disabilities can equally access learning through
efforts of artificial intelligence. Many artificial intelligence initiatives had been underway due to
the unique learning needs for people with disabilities. These programs are able to expose them to
knowledge and learning in ways they haven’t been able to experience before.
4.6. Potential caveats and biases in artificial intelligence and machine learning
Making any radical changes to the status quo of any field as huge as education comes
with a whole host of caveats that need to be considered. We have to protect our profession and be
certain that the decisions we make impact our students positively and do no harm. The biggest
concern for the implementation of artificial intelligence and machine learning education in the
classroom is that the programs need to be developed with learning needs of students with all
ability levels in mind. Because artificial intelligence and machine learning grows and improves
based on data collected from large amounts of student input, it has the possibility to assemblage
student learning needs based on the greatest amounts of data it has received; this can create
biases toward the mean and ignore outliers, which could easily be the fewer amount of students
with learning disabilities.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 64
When considering new, smart technologies and the capabilities they have in not only
influencing learning, but also influencing the social and emotional relationships children form
with technologies, we have to be skeptical of the ethical dilemmas that may present with these
technologies. These dilemmas can include the relationships students may form with their
learning companions, ensuring that programs are designed to not manipulate learners, and
making sure students are rich with face-to-face human interactions.
Some philosophies of education and followers of those philosophies disapprove of the
use of technology in the educational setting on the premises that too much screen time can be
damaging to nurturing the whole child, children may already experience over exposure to
technology, and education should be about human interaction. And, with the global connection
artificial intelligence can have on education, many stakeholders are concerned for the privacy
issues that can arise from the data-mining these programs can use on student performance,
background, and socio-economic information.
4.7. Ideas for development
The most important consideration for developing new artificially intelligent learning
software in classrooms is the inclusion of student individualization and diverse backgrounds.
Female, minority, impoverished, and students with disabilities need to have personalized learning
experiences that are developed with their individual, unique learning needs in consideration.
Many technological advances in education have gone to the wayside either as they
become archaic or because teachers did not receive proper and on-going professional
development for the technology to become successfully integrated. With artificial intelligence
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 65
and machine learning, teachers need to receive proper professional development and receive
continued support in the classroom.
Age appropriate practices need to be considered when integrating technology in the
classroom and may be inappropriate for young children. Teachers and psychologists have to be
involved in the development and research process of artificial intelligence and machine learning
to ensure that development supports best practice.
Developers of smart technologies have been quite successful at incorporating engagement
strategies and are able to monitor student engagement for teacher feedback, but artificial
intelligence is only successful when learners are actively engaged and thus makes engagement
hugely important to the development of these programs.
5. Conclusion
The findings of this meta-synthesis have opened my eyes to the magnificent and
monumental applications for artificial intelligence and machine learning in the sphere of
education. These advances in technology, while still largely in the research and development
phase, can be incredibly valuable in the inclusive classroom. While a host of caveats and barriers
have emerged from the research on the development of artificial intelligence and machine
learning in the classroom, I believe that these applications can prove invaluable to teachers and
students. In some educational environments, artificial intelligence and technology in general may
not be necessary, but in the general public school classroom, artificial intelligence and machine
learning can accelerate student knowledge and help students to compete in the work force of the
future.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 66
With the many challenges that teachers face in the classroom, artificial intelligence and
machine learning can alleviate the pressures on teachers while building a highly engaging and
personalized learning experience for students. I believe that with careful design, backed by
adequate research, artificial intelligence and machine learning can impact student engagement
and learning in ways that the educational field has never before experienced.
Advancements in artificial intelligence and machine learning have proven to be
advantageous for people with impairments and disabilities. It will have to encompass thoughtful,
careful design, but can open the doors for students with disabilities to interact with information
and learning in new and exciting ways.
As a teacher, lifelong learner, and consumer of new technologies, I am not simply excited
for the ways that artificial intelligence and machine learning can simplify many tasks in my life,
but also for the many ways these technologies can expand my ability to learn and interact with
information throughout the world. I look forward to sharing those emotions with my students so
that they may experience things in their learning that they otherwise wouldn’t have the
opportunity to experience. The findings of this meta-synthesis indicate that there is much to
consider and develop to make these applications the most appropriate for student use, but the
benefits can be astronomical to the quality of life and education to students with and without
disabilities.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 67
References
Aberšek, B., & Aberšek, M. K. (2012). Role of teacher and/or technology in the education
process. Technology of Education/ Professional Journal on Education.
Ark, T. V. (2015, November 26). 8 ways machine learning will improve education. Retrieved
from http://www.gettingsmart.com/2015/11/8-ways-machine-learning-will-improve-
education/
Arroyo, I., Woolf, B. P., Cooper, D. G., Burleson, W., & Muldner, K. (2011). The impact of
animated pedagogical agents on girls' and boys' emotions, attitudes, behaviors and
learning. 11th IEEE International Conference on Advanced Learning
Technologies,506-510.
Balakrishnan, K., & David, J. M. (2010). Machine learning approach for prediction of learning
disabilities in school-age children. International Journal of Computer Applications,9(11),
7-14.
Beidelman, J. (2018). Big data: Trends in the education sector. Retrieved from
https://trueinteraction.com/big-data-trends-in-the-education-sector/
Chin, D. B., Dohmen, I. M., Cheng, B. H., Oppezzo, M. A., Chase, C. C., & Schwartz, D. L.
(2010). Preparing students for future learning with teachable agents. Educational
Technology Research & Development,649-669.
Chou, C., & Chan, T. (2016). Reciprocal tutoring: Design with cognitive load
sharing. International Artificial Intelligence in Education Society,26, 512-535.
Chrysfiadi, K., & Virvou, M. (2015). Advances in personalized web-based education(Vol. 78).
Switzerland: Springer International Publishing.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 68
Clark, A. K., & Whetstone, P. (2014). The impact of an online tutoring program on mathematics
achievement. The Journal of Educational Research,107(6), 462-466.
Davison, M. (2018). AI and the classroom: Machine learning in education. Retrieved from
https://trueinteraction.com/ai-and-the-classroom-machine-learning-in-education/
Du Boulay, B. (2016, November). Artificial Intelligence as an effective classroom assistant. AI
and Education, 1541-1672(16), 76-81.
Doom, C. A. (2016, August). Teacher +  technology = blended learning: How important is the
teacher in this equation? (Doctoral dissertation, University of Nebraska, 2016). ProQuest
Dissertations and Theses. (UMI No. 10139062)
Erstad, O., Eickelmann, B., & Eichhorn, K. (2015). Preparing teachers for schooling in the
digital age: A meta-perspective on existing strategies and future challenges. Educational
Informational Technologies,20, 641-654.
Gadanidis, G. (2016). International Conference on Information, Communication Technologies in
Education 2016 Proceedings. Artificial intelligence, computational thinking, and
mathematics education(pp. 83-90).
Gaskell, A. (2016, November 4). Machine learning and the future of education. Forbes.
Goldschein, G., Pizzi, J., Naidus, M., Nye, B., Spingarn-Koff, J., Wood, T., ... Spurlock, M.
(Producers), Pleasants, C., Venkataramanujam, P., Wall, T., Drucker, M., Lichtman, F.,
Naidus, M., ... Totten, S. (Writers), & Murray, N. (Director). (2017). Bill Nye Saves the
World [Film]. United States of America: Netflix.
Gonçalves, S., Rodrigues, M., Carneiro, D., Fdez-Riverola, F., & Novais, P. (2015). Boosting
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 69
learning: Non-intrusive monitoring of student's efficiency. Advances in Intelligent
Systems and Computing,374, 73-80.
Heffernan, N. T., Ostrow, K. S., Kelly, K., Selent, D., Van Inwegen, E. G., Xiong, X., &
Williams, J. J. (2016). The future of adaptive learning: Does the crowd hold the
key? International Journal of Artificial Intelligence in Education,26, 615-644.
Ireh, M. (2016). Influence of social reform ideologies on industrial/technology education.
Jenkin, M. (2015, December 2). Tablets out, imagination in: The schools that shun
technology. The Guardian (London, England).
Kinshuk, Chen, N., Cheng, I., & Chew, S. W. (2016). Evolution is not enough: Revolutionizing
current learning environments to smart learning environments. International Journal of
Artificial Intelligence in Education,26, 561-581.
Koedinger, K.R. & Aleven V. (2015, December 9).  An interview reflection on ""intelligent
tutoring goes to school in the big city"". International Journal of Artificial Intelligence in
Education, 26, 13-24.
Kotsiantis, S. B. (2011). Use of machine learning techniques for educational proposes: A
decision support system for forecasting students' grades. Artificial Intelligence
Review,37(4), 331-334.
Lakkaraju, H., Aguiar, E., Shan, C., Miller, D., Bhanpuri, N., Ghani, R., & Addison, K. L.
(2015). A machine learning framework to identify students at risk of adverse academic
outcomes. Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining,1909-1918.
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 70
McArthur, D., Lewis, M., & Bishay, M. (2005). The roles of artificial intelligence in education:
Current progress and future prospects. I-manager's Journal of Education
Technology,1(4), 42-80.
Nabiyev, V., Karal, H., Arslan, S., Erumit, A. K., & Cebi, A. (2013). An artificial intelligence-
based distance education system: Artimat. Turkish Online Journal of Distance
Education,14(2), 4th ser., 81-98.
Pareto, L. (2014). A teachable agent game engaging primary school children to learn arithmetic
concepts and reasoning. International Journal of Artificial Intelligence in Education,24,
251-283.
Parkavi, P., Ramesh, V., & Ramar, K. (2013). Predicting student performance: A statistical and
data mining approach. International Journal of Computer Applications,63(8), 35-39.
Passonneau, R. J., McNamara, D., Muresan, S., & Perin, D. (2017). Preface: Special issue on
multidisciplinary approaches to AI and education for reading and writing. International
Journal of Artificial Intelligence in Education,27, 665-670.
Petrilli, M. J. (n.d.). Big data transforms education research. Education Next,18(1). Retrieved
from http://educationnext.org/big-data-transforms-education-research-can-machine-
learning-unlock-keys-to-great-teaching/
Porter, J. E. (2017). Professional communication as phatic: From classical eunoia to personal
artificial intelligence. Business and Professional Communication Quartersly,80(2),
174-193.
Press, G. (2017, August 27). Artificial intelligence (AI) defined. Retrieved from
https://www.forbes.com/sites/gilpress/2017/08/27/artificial-intelligence-ai-
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 71
defined/#69d253887661
Ramaswami, R. (2009). Is the future now for A.I.? Technological Horizons in Education,36(2),
42-47.
Rizzotto, L. (2017, June 23). The future of education: How A.I. and immersive tech will reshape
learning forever. Retrieved from
https://medium.com/futurepi/a-vision-for-education-and-its-immersive-a-i-driven-future-
b5a9d34ce26d
Roll, I., & Wylie, R. (2016). Evolution and revolution in artificial intelligence in
education. International Journal of Artificial Intelligence in Education,26, 582-599.
Rosé, C. P., & Ferschke, O. (2016). Technology support for discussion based learning: From
computer supported collaborative learning to the future of massive open online
courses. International Journal of Artificial Intelligence in Education,26, 660-678.
Scalise, K. (2009). New electronic technologies for facilitating differentiated instruction. I-
manager's Journal of Education Technology,4(4), 39-45.
Stone, P., Brooks, R., Brynjolfsson, E., Calo, R., Etzioni, O., Hager, G., . . . Teller, A. (2016,
September). Artificial intelligence and life in 2030. One Hundred Year Study Panel.
Stanford, CA: Stanford University
The Economist. (2017, July 22). Technology is transforming what is happening when a child
goes to school. Retrieved from https://www.economist.com/news/briefing/21725285-
reformers-are-using-new-software-personalise-learning-technology-transforming-what-
happens
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 72
Timms, M. J. (2016). Letting artificial intelligence in education out of the box: Educational
cobots and smart classrooms. International Journal of Artificial Intelligence in
Education,26, 701-712.
Walker, W., & Ogan, A. (2016). We're in this together: Intentional design of social relationships
with AIED Systems. International Journal of Artificial Intelligence in Education,26(2),
713-729.
Wallace, S. A., McCartney, R., & Russell, I. (2010). Games and machine learning: A powerful
combination in an artificial intelligence course. Computer Science Education,20(1), 17-
36.
Woolf, B. P., Lane, C., Chaudhri, V. K., & Kolodner, J. L. (2013, Fall). AI grand challenges for
education. AI Magazine, (Special Issue), 1-22.
Zubrzycki, J. (2016, July 22). As project-based learning gains in popularity, experts offer
caution. Retrieved from http://blogs.edweek.org/edweek/curriculum/2016/07/as_project-
based_learning_gain.html
ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING FOR ALL STUDENTS 73
",481573328,"{'doi': None, 'oai': 'oai:scholarworks.alaska.edu:11122/12336'}",Artificial Intelligence and Machine Learning for All Students: A Meta-Synthesis,,2018-01-01T00:00:00+00:00,University of Alaska Southeast,[],['https://scholarworks.alaska.edu/bitstream/11122/12336/1/MEd_2018_Thesis_HarmonCasey.pdf'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/481573328.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/481573328'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/481573328/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/481573328/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/481573328'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/481573328?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=4&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","This meta-synthesis investigates the increasingly powerful and promising fields of artificial intelligence and machine learning as these technologies filter into the sphere of education. Smart technologies have been quickly gaining momentum in our society and have piqued the interest of many educators and administrators that are considering early adoption and applications of these new and promising technologies that may be able to offer teachers, administrators, and students new ways to access information and student learning. However, it is not without some resistance toward these technologies, that we consider their diverse applications in classrooms. This research of 43 articles address the applications, caveats, biases, and possibilities that these new, smart technologies using artificial intelligence and machine learning can offer to improve the education of students with and without disabilities","['Thesis', 'Meta-Synthesis', 'Literature Review', 'Artificial Intelligence', 'Smart Technologies', 'Accessing Information']",disabled
,"[{'name': 'NC DOCKS at The University of North Carolina at Greensboro'}, {'name': 'Suthaharan, Shanmugatha ""Shan""'}]",[],2020-11-24T18:13:00+00:00,"{'name': 'The University of North Carolina at Greensboro', 'url': 'https://api.core.ac.uk/v3/data-providers/3065'}",,,,https://core.ac.uk/download/345087530.pdf,"Big Data Analytics: Machine Learning and Bayesian Learning Perspectives -- What is 
done? What is not? 
 
By: Shan Suthaharan 
 
This is the peer reviewed version of the following article: 
 
Suthaharan S. Big data analytics: Machine learning and Bayesian learning perspectives—What is 
done? What is not? WIREs: Data Mining and Knowledge Discovery. 2019;9:e1283. 
https://doi.org/10.1002/widm.1283 
 
which has been published in final form at https://doi.org/10.1002/widm.1283. This article 
may be used for non-commercial purposes in accordance with Wiley Terms and Conditions 
for Use of Self-Archived Versions. 
 
Abstract: 
 
Big data analytics provides an interdisciplinary framework that is essential to support the current 
trend for solving real‐world problems collaboratively. The progression of big data analytics 
framework must be clearly understood so that novel approaches can be developed to advance this 
state‐of‐the‐art discipline. An ignorance of observing the progression of this fast‐growing 
discipline may lead to duplications in research and waste of efforts. Its main companion field, 
machine learning, helps solve many big data analytics problems; therefore, it is also important to 
understand the progression of machine learning in the big data analytics framework. One of the 
current research efforts in big data analytics is the integration of deep learning and Bayesian 
optimization, which can help the automatic initialization and optimization of hyperparameters of 
deep learning and enhance the implementation of iterative algorithms in software. The 
hyperparameters include the weights used in deep learning, and the number of clusters in 
Bayesian mixture models that characterize data heterogeneity. The big data analytics research 
also requires computer systems and software that are capable of storing, retrieving, processing, 
and analyzing big data that are generally large, complex, heterogeneous, unstructured, 
unpredictable, and exposed to scalability problems. Therefore, it is appropriate to introduce a 
new research topic—transformative knowledge discovery—that provides a research ground to 
study and develop smart machine learning models and algorithms that are automatic, adaptive, 
and cognitive to address big data analytics problems and challenges. The new research domain 
will also create research opportunities to work on this interdisciplinary research space and 
develop solutions to support research in other disciplines that may not have expertise in the 
research area of big data analytics. For example, the research, such as detection and 
characterization of retinal diseases in medical sciences and the classification of highly interacting 
species in environmental sciences can benefit from the knowledge and expertise in big data 
analytics. 
 
Keywords: big data analytics | machine learning | Bayesian optimization | knowledge discovery | 
deep learning 
 
Article: 
1 INTRODUCTION 
 
The scientific term “big data analytics” may describe an analytical framework that provides 
approaches to extract knowledge from a big data environment and characterize the data source 
that produced the big data environment which is large, complex, heterogeneous, unstructured, 
unpredictable, and exposed to scalability problems (Suthaharan, 2015). This analytical 
framework not only provides theory and methods, but also facilitates the selection of big data 
systems and software. In general, the big data analytics adopt machine learning as one of the 
supporting tools to formulate this analytical framework; hence, it is dependent upon the 
successful advancements of its companion field of machine learning and other alternatives, such 
as data mining. When necessary, the machine learning models are parametrized with two types 
of parameters: hyperparameters and learned‐parameters (Thornton, Hutter, Hoos, & Leyton‐
Brown, 2013). The suitable values for the hyperparameters are determined before training; 
hence, it requires proficiency in machine learning, whereas the learned‐parameters are derived by 
the machine learning algorithms from the training data sets. When interdisciplinary applications 
are merged in big data analytics framework, a big question comes to everyone's mind is that, 
with the huge spectrum of users of big data, how the big data analytics framework will serve 
them better by grouping them into individuals with different levels of proficiency, ranging from 
nonexpert users to data analysts to data scientists, for both big data analytics research and 
applications. The aforementioned parametrization approaches in machine learning models, 
adopted by big data analytics, create problems and challenges to several disciplines when, in 
general, the expertise in machine learning is lacking. Similarly, the adaptive selection of machine 
learning algorithms for different applications is also a challenge to nonexpert users of big data 
analytics and machine learning. Therefore, the expectations of the individuals from different 
disciplines are the availability of a flexible big data analytics framework that is more intelligent 
and can minimize the need for user expertise for tuning model parameters and selecting suitable 
machine learning models and algorithms for their applications. 
 
One of the recent advances in machine learning is the automated machine learning (AutoML) 
technique which adopts Bayesian optimization (Shahriari, Swersky, Wang, Adams, & De 
Freitas, 2016) and enables simultaneous selection and optimization of hyperparameters of 
machine learning models, which include logistic regression, support vector machine, decision 
tree, and random forest (Thornton et al., 2013). It is a very useful technique that enables the use 
of machine learning models in an interdisciplinary setting; however, it still suffers from 
performance degradation issues due to large, complex, unstructured, unpredictable, scalable, and 
heterogeneous data characteristics in big data analytics. The Bayesian optimization is not fully 
understood in a big data analytics settings; hence, it is still questionable whether the AutoML 
approach will provide desired results with big data analytics framework under interdisciplinary 
settings. What is required now is a newly defined research domain that allows exploration of big 
data analytics with the advancement of AutoML toward developing smart machine learning that 
is fully automated, adaptive, and cognitive in nature. This new research domain can make big 
data analytics more interdisciplinary by changing big data analytics framework much smarter 
through the adoption of FullAutoML techniques. 
 
The purpose of this review article is to report the recent progress in big data analytics, machine 
learning, and Bayesian learning, and the way these three areas work together as companions 
toward meeting the expectation of interdisciplinary research and applications. In particular, this 
article reviews and reports the research progress in big data analytics and in the efforts made to 
make machine learning much smarter so that the interdisciplinary research can benefit from big 
data analytics techniques and technologies. 
 
2 BIG DATA ANALYTICS 
 
The current progress in big data analytics can be appreciated by observing its widespread 
applications in many scientific and nonscientific disciplines. The recent major conference in the 
discipline—the 2017 I.E. International Conference on Big Data, held in Boston, MA, USA, on 
December 11–14, 2017—incorporated several topics that highlight the trend of big data analytics 
and the interests of research community in the field. Using the compiled conference program and 
the proceedings published in this research forum, we can divide the current progress in big data 
analytics into the following focused‐driven research domains: 
 
• Descriptive big data analytics: It addresses the theoretical and design aspects of modeling 
and algorithms of big data analytics and associated big data characteristics; 
• Predictive big data analytics: It focuses on the topics of machine learning that helps the 
study of predictive and classification models and algorithms for big data analytics; 
• Visual big data analytics: It defines the preprocessing and visualization techniques that 
help us understand the big data characteristics through exploration analysis; 
• Streaming big data analytics: It describes the theory and methods required to study 
spatiotemporal characteristics of big data and machine learning models and algorithm; 
• Graphical big data analytics: It helps to study big data environment using graphical 
models and network analysis under machine learning and big data analytics paradigm; 
• Big data systems and software: It addresses the designing and building of big data 
systems and software that focus on efficient resource utilization and big data processing. 
 
Although the progress of big data analytics can be grouped into above categories, a closer and 
careful understanding of the current research activities in each of these categories suggest that 
the companion field of machine learning contributes significantly to the successful progress of 
big data analytics which include brain network analysis (Khazaee, Ebrahimzadeh, & Babajani‐
Feremi, 2016), social network analysis (Cybenko, 2017), and ecological network analysis 
(Stephens, Sánchez‐Cordero, & González Salazar, 2017). Hence, it cannot be separated from big 
data analytics. In addition, Bayesian inference is also emerging with big data analytics, because 
of the parametrization and optimization requirements of the machine learning models and 
algorithms (Klein, Bartels, Falkner, Hennig, & Hutter, 2015; Shahriari et al., 2016). The research 
in systems and software also focuses on the optimization of the performance of a computing 
environment; hence, the parametrization and parallelization of the computing processes and 
resources are explored (Dean & Ghemawat, 2010; White, 2012). The systems and software are 
essentials to house the big data analytics framework; hence, their progress is first discussed, and 
then the research progress in machine learning and Bayesian optimization, focusing on big data 
analytics, is discussed. 
 
3 SYSTEMS AND SOFTWARE 
 
The current big data systems and software have been developed as a parametric platform using a 
divide‐and‐conquer algorithm so that the distributed resources (e.g., cores, memory, executors, 
and software modules) can be parallelized to achieve optimal performance for big data analytics. 
It also focuses on providing both local and global (i.e., cloud) computing resources as an 
affordable mechanism for different levels of users and applications—an essential feature that is 
required for an interdisciplinary setting. Figure 1 illustrates these two types of big data analytics 
framework (local systems and software, and cloud computing resources) that are observable 
clearly in the current progress. It mainly shows the currently available cloud computing 
resources such as the Microsoft Azure (Azure: https://azure.microsoft.com/en-us/), Amazon Web 
Services (AWS: https://aws.amazon.com/), and Google Cloud Platform 
(GCP: https://cloud.google.com/), and the way they are utilized with Apache Hadoop 
(http://hadoop.apache.org/) and Spark (http://spark.apache.org/) systems, and the machine 
learning framework TensorFlow (https://www.tensorflow.org/) along with the programming 
languages such as R (https://www.r-project.org/), and Python (https://www.python.org/). 
 
 
Figure 1. A possible set of systems and software for big data analytics framework 
 
3.1 Parametrization 
 
Presently, the big data systems and software adopt the concept called (key, value)‐pair to 
parametrize big data computing environments. The use of this concept can be seen in the 
currently available computing platforms, such as the Hadoop distributed file systems, 
MapReduce framework, and Spark in‐memory computing system that adopt the programming 
languages, including R, Python, Java, and Scala with machine learning software libraries 
(Pääkkönen & Pakkala, 2015). These big data processing computing platforms were initially 
developed to perform big data analytics in local computing environment; however, as the 
research in big data analytics progresses, a lack of resources has been realized and the ideas were 
extended to cloud architecture for addressing scalability issues with other big data 
characteristics. Figure 1 also illustrates some of the latest tools developed for big data systems 
and software (Bhatt, Patwa, & Sandhu, 2017; Komarek, Pavlik, & Sobeslav, 2017). 
 
3.2 Parallelization 
 
The aforementioned parametrization techniques enable the parallelization of computing 
resources with automated resource allocation and utilization. A simple example of systems and 
software parametrization and parallelization can be found in chapter 5 of book by the author 
(Suthaharan, 2015). In the current big data system, this parallelization is not transparent and it 
performs optimal resource allocation and utilization automatically. However, in some situations, 
it is important for the users to set the system parameters such as the number of cores, executors 
and memory size. Therefore, the progress in this domain has also shifted to allow user 
intervention and suggest some mechanism to select a correct combination of system parameters 
(Sundaravarathan, Martin, Rope, McRoberts, & Statchuk, 2016). 
 
4 MACHINE LEARNING 
 
The initial progress in machine learning has defined the standard methodological processes that 
include the development of parametric models and development of algorithms that help optimize 
the model parameters using training, validation, and testing processes, and given (labeled) data. 
These methodological processes may be grouped into two categories: interpretable knowledge 
discovery (Suthaharan, 2015) and actionable knowledge discovery (Cao, 2015). Additionally, the 
machine learning models are defined using two types of parameters, namely, hyperparameters 
and learned‐parameters. The hyperparameter values must be selected at the beginning of the 
training process—requires some expertise in the research domain—and the learned‐parameters 
are optimized by machine learning algorithms. 
 
4.1 Parametrization 
 
To date, the machine learning models are parametrized in three groups: mathematical, 
hierarchical, and layered models. The mathematical models include the models developed using 
the concept of support vectors and statistical regression—examples are support vector machine, 
logistic regression, and lasso regression. The hierarchical models include the models developed 
based on the concept of decision tree—examples are random forest models. The layered models 
include the models proposed based on the concept of neural networks—examples include deep 
learning models. The first two types of approaches have been proposed to address batch learning. 
Later, a need for on‐line learning (i.e., learning from a single point) was realized for big data 
analytics; hence, the layered models have been proposed. 
 
4.2 Optimization 
 
The optimization has been performed in a conventional manner, targeting the extraction of exact 
and complete knowledge from data; hence, strong mathematical techniques have been deployed 
to optimize model parameters. These approaches are based on the concepts of gradient descent or 
stochastic gradient descent (Luketina, Berglund, Greff, & Raiko, 2016). However, the latest 
trend is in the use of Bayesian optimization—a probabilistic approach—to address the problems 
and challenges evolve from the big data characteristics, such as data heterogeneity, 
unpredictability, and scalability (Shahriari et al., 2016; Wainer & Cawley, 2017), when the 
optimization of model parameters is the main objective. 
 
5 SMART MACHINE LEARNING 
 
As deep learning technique progresses—of course! as a promising solution to solve big data 
analytics problems—researchers have been trying to make machine learning techniques more 
intelligent as possible to meet the requirements of interdisciplinary research and applications. 
Hence, the expected features of machine learning techniques are currently divided into three 
groups: automatic, adaptive, and cognitive features. As a result, the current trend in big data 
analytics focuses on three types of machine learning—AutoML, adaptive machine learning, and 
cognitive machine learning. In essence, Bayesian learning, Bayesian optimization, and related 
approaches can help us develop smart machine learning. 
 
5.1 Bayesian learning 
 
The Bayesian learning, which includes Bayesian optimization and Bayesian mixture models, is 
especially adopted to optimize hyperparameters because of its ability to defend the difficulties 
that come from the big data characteristics using probabilistic approaches. Some of the latest 
progress in the integration of Bayesian optimization in big data analytics and machine learning 
are the techniques proposed in (Polson & Sokolov, 2017), (Shahriari et al., 2016), and (Snoek, 
Larochelle, & Adams, 2012). These approaches replace mathematical optimization with 
probabilistic optimization—a preferable approach for big data settings. 
 
5.2 Bayesian mixture model and machine learning 
 
The Bayesian mixture models assume data heterogeneity and represent a data domain in action 
as a composition of multiple sub domains (or clusters) with finite or infinite number of mixture 
distributions (Tafaj, Kasneci, Rosenstiel, & Bogdan, 2012). An example of its implementation in 
software systems for a big data environment is that the master node in a data sharing network can 
distribute these sub domains to worker nodes by associating them with general prior distributions 
and then the worker nodes process them and generate corresponding posterior distributions to 
specify hyperparameters for machine learning algorithms, such as the kernel and regularization 
parameters in Support Vector Machine (SVM) (Klein et al., 2015), and weights and learning‐rate 
parameters in deep learning (Suthaharan, 2015). 
 
5.3 Automated machine learning 
 
A machine learning technique can be claimed to be truly automatic, only if every step of the 
machine learning technique is automated such that it can minimize the problems of 
interdisciplinary applications. The research in AutoML for big data analytics is still in progress 
and it includes the research work reported in (Feurer et al., 2015) and (Luo, 2016). The current 
approaches mainly focus on the optimization of hyperparameters and the selection of models and 
algorithms to create suitable AutoML approaches that can be adopted by any types of users of 
interdisciplinary applications. 
 
One aspect of an AutoML is the optimization of hyperparameters, such as the regularization 
parameter in lasso regression (Suthaharan, 2015), kernel and regularization parameters in SVM 
(Nguyen, Gupta, Rana, & Venkatesh, 2017; Wainer & Cawley, 2017), and learning rate in deep 
learning. This objective requires efficient learning models and algorithms. The current research 
progress in this area shows a significant interest and an appropriate use of computational 
techniques based on Bayesian (Klein et al., 2015) and radial basis function (Ilievski, Akhtar, 
Feng, & Shoemaker, 2017). 
 
Another aspect of an AutoML is the automatic selection of models and algorithms for 
optimization and learning. As we know, the hyperparameter selection requires the automatic 
selection of suitable models and algorithms from a large pool of models and algorithms for 
optimization with respect to a given data set. However, in big data analytics environment, this 
selection process is problematic and very challenging because of the data characteristics that 
include complexity, scalability, and heterogeneity. A significant research has been performed—
including (Thornton et al., 2013) and (Sparks et al., 2015)—in this problem space; however, the 
research is still progressing because of the emerging problems and challenges that evolve from 
the interdisciplinary nature of big data analytics. 
 
5.4 Adaptive machine learning 
 
The adaptive machine learning is not new and it also includes AutoML concept. The concept of 
adaptive machine learning can be dated back to 1990s (Blum, 1998; Littlestone & 
Warmuth, 1994), as stated in the paper (Torabi, Sayad, & Balke, 2005). Today, it can be 
observed that the big data characteristics and the current widespread interdisciplinary 
applications have enforced new constraints and requirements that triggered the exploration of 
novel approaches for adaptive machine learning. 
 
One aspect of adaptive machine learning is the selection and optimization of the learned model 
parameters with respect to changing data characteristics between interdisciplinary domains. The 
examples include the techniques published in (Anagnostopoulos, Anagnostopoulos, & 
Hadjiefthymiades, 2011) and (Azodi, Gawron, Sapegin, Cheng, & Meinel, 2015). In other words, 
we can say that the progress in adaptive machine learning focuses on learning techniques that are 
adaptive to unseen data. Another aspect of adaptive machine learning is the revision of machine 
learning models and algorithms, with a minimum effort, to work with changing data 
characteristics between interdisciplinary domains. For example, in a recent paper, the authors 
have proposed a learning‐based approach that generates an adaptive best‐fit algorithm from a set 
of supervised learning algorithms to detect silent errors that occur in a high‐performance 
computing environment (Subasi et al., 2017). They described the silent errors as silent data 
corruption in high performance computing systems, and these errors corrupt the execution results 
with no warning and undetectable by hardware or software. 
 
5.5 Cognitive machine learning 
 
Cognitive machine learning is an emerging field and it includes both the adaptive and AutoML 
concepts. In big data analytics, the use of exact or complete knowledge for making decisions is 
impractical because of the big data characteristics that include data heterogeneity, 
unpredictability, and scalability. Hence, the machine learning research community has realized 
the need for developing techniques and technologies that mimic the cognitive processes that the 
humans use to solve complex environmental problems and make decisions through 
approximation, hypothesis, and reasoning. Hence, computational intelligence and cognitive 
computing have been studied in recent years by focusing on machine learning and big data 
analytics (Suthaharan, 2016; Wang et al., 2018). 
 
5.6 Intelligent computing (Computational Intelligence) 
 
The intelligent computing (or computational intelligence) is another concept that has been 
integrated in big data analytics to help computers learn from data in the similar way that the 
human brain learns from data (Modha et al., 2011). This concept includes the techniques such as 
the fuzzy sets, genetic algorithms, and neural networks–The latest book chapter by Samanpour, 
Ruegenberg, and Ahlers (2018) discusses clearly about the integration of these evolutionary 
algorithms with machine learning, especially in the interdisciplinary domain. 
 
5.7 Cognitive computing 
 
This research focuses on resource‐efficient, cost‐effective, and cognition‐enabled computing 
platforms for big data analytics (Suthaharan, 2016). In this computing platform, cognition‐enable 
means the availability of methodologies and techniques that mimic human cognitive process that 
utilize object‐recognition, speech‐recognition, and functional brain networks (machine learning, 
reasoning, and analysis) to make cognitive decisions. Similarly, the cost‐effective feature 
includes computational cost as well as affordable systems and software that support 
interdisciplinary settings. Finally, resource‐efficient feature includes the automated resource 
allocation and resource utilization with no transparency to users—It is especially useful to 
nonexpert users of the learning models (Hurwitz, Kaufman, & Bowles, 2015). 
 
5.8 Smart deep learning 
 
A literature review suggests that machine learning for big data analytics is converging to deep 
learning techniques. For example, based on the proceedings of the 2017 International Conference 
on Machine Learning held in Sydney, Australia, on August 6–11, 2017, we can clearly see that 
the research interest of the machine learning community has shifted toward deep learning 
significantly. The deep learning, compared to other machine learning techniques, focuses on 
single‐data‐point analysis (i.e., it enables both on‐line learning and batch learning), which is a 
highly preferred option for a big data analytics framework. Hence, it is expected in the current 
research of big data analytics that the deep learning will be studied, focusing on automated, 
adaptive, and cognitive approaches to create smart deep learning. 
 
6 CONCLUSIONS 
 
The progress of research in big data analytics and machine learning was reviewed and 
understood in three phases: past, present, and future. In Phase 1, a significant amount of research 
has been done on data analytics and machine learning by defining data‐driven approaches. In 
other words, the focus of research in Phase 1 was mainly on interpretable knowledge 
discovery theme (i.e., the development of models and algorithms that enhance the discovery of 
interpretable knowledge from a given set of data). In essence, the data‐driven approaches 
focused on the discovery of patterns that help understand the source that produced the data. 
 
In Phase 2, the focus of data analytics research shifted to big data analytics based on the 
redefinition of data that describe big data characteristics. Hence, the research has started to shift 
toward domain‐driven approaches (which of course included data‐driven approaches). In other 
words, the focus of the research in Phase 2 was on actionable knowledge discovery theme. The 
actionable knowledge discovery defines the practical significance of the knowledge discovered 
from data through domain‐driven data mining. Hence, it defines the flow of knowledge from 
data‐driven approaches to domain‐driven interdisciplinary applications. 
 
It is now well understood that big data analytics research spans across multiple disciplines where 
AutoML models and algorithms are required to solve problems without the help of data science 
experts. Hence, as we have seen, the selection and optimization of hyperparameters became a 
very strong research component among machine learning (or artificial intelligence) and data 
science research community. A significant research still has to be done on hyperparameter 
selection and optimization using Bayesian optimization to develop AutoML approaches that are 
useful for interdisciplinary big data analytics. 
 
In Phase 3, over the next two decades, a significant research is expected on FullAutoML 
approaches, toward achieving Smart Machine Learning techniques that are fully automated, 
adaptive, and cognitive under big data characteristics. However, to advance this research, it is 
important to study approaches that help discover patterns that are sensitive to changes between 
interdisciplinary domains. Hence, the progress reported in this paper suggests an establishment 
of a new research theme—transformative knowledge discovery. It means that, when a learning 
model is developed for a big data environment (big data domain or a discipline), it is important 
to discover the knowledge that can cause a noticeable change to the model, in addition to 
discovering the knowledge that is interpretable and actionable. This knowledge will help the 
model to become adaptive to a new environment. Hence, when a learning model is built for a 
data domain, it is important to study all the potential alternatives, which include both the real and 
hypothetical alternatives. 
 
The discovery of transformative knowledge from big data can significantly benefit from 
Bayesian learning, including Bayesian optimization and Bayesian mixture models. In other 
words, we need to define a new research discipline called Transformative Data Science and 
Analytics, or Transformative Data Science and Big data, or Transformative Big Data 
Analytics that study Bayesian optimization approaches and Bayesian mixture models 
extensively. 
 
References 
 
Anagnostopoulos, T., Anagnostopoulos, C., & Hadjiefthymiades, S. (2011). An adaptive 
machine learning algorithm for location prediction. International Journal of Wireless Information 
Networks, 18(2), 88–99. 
 
Azodi, A., Gawron, M., Sapegin, A., Cheng, F., & Meinel, C. (2015). Leveraging event structure 
for adaptive machine learning on big data landscapes. In S. Boumerdassi, S. Bouzefrane, & É. 
Renault (Eds.), Proceedings of the international conference on mobile, secure and programmable 
networking (pp. 28–40). Switzerland: Springer. 
 
Bhatt, S., Patwa, F., & Sandhu, R. (2017). Access control model for AWS internet of things. In 
Z. Yan, R. Molva, W. Mazurczyk, & R. Kantola (Eds.), Proceedings of the international 
conference on network and system security (pp. 721–736). Cham: Springer. 
 
Blum, A. (1998). On‐line algorithms in machine learning. In A. Fiat & G. J. Woeginger (Eds.), 
Online algorithms. Lecture notes in computer science (Vol. 1442, pp. 306–325). Berlin, 
Heidelberg: Springer‐Verlag. 
 
Cao, L. (2015). Actionable knowledge discovery and delivery. In L. Cao (Ed.), Metasynthetic 
computing and engineering of complex systems (pp. 287–312). London: Springer. 
 
Cybenko, G. (2017). Parallel computing for machine learning in social network analysis. In 
Proceedings of the international parallel and distributed processing symposium work‐shops (pp. 
1464–1471). Lake Buena Vista, FL: IEEE. 
 
Dean, J., & Ghemawat, S. (2010). MapReduce: A flexible data processing tool. Communications 
of the ACM, 53(1), 72–77. 
 
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). 
Efficient and robust automated machine learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. 
Sugiyama, & R. Garnett (Eds.), Proceedings of the advances in neural information processing 
systems (pp. 2962–2970). Curran Associates, Inc. 
 
Hurwitz, J., Kaufman, M., & Bowles, A. (2015). Cognitive computing and big data analytics. 
Indianapolis, USA: John Wiley & Sons, Inc. 
 
Ilievski, I., Akhtar, T., Feng, J., & Shoemaker, C. A. (2017). Efficient hyperparameter 
optimization for deep learning algorithms using deterministic RBF surrogates. In Proceedings of 
the AAAI (pp. 822–829). Retrieved from 
https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14312 
 
Khazaee, A., Ebrahimzadeh, A., & Babajani‐Feremi, A. (2016). Application of advanced 
machine learning methods on resting‐state fMRI network for identification of mild cognitive 
impairment and Alzheimer's disease. Brain Imaging and Behavior, 10(3), 799–817. 
 
Klein, A., Bartels, S., Falkner, S., Hennig, P., & Hutter, F. (2015). Towards efficient Bayesian 
optimization for big data. In Proceedings of the NIPS 2015 workshop on Bayesian optimization. 
 
Komarek, A., Pavlik, J., & Sobeslav, V. (2017). Performance analysis of cloud computing 
infrastructure. In M. Younas, I. Awan, & I. Holubova (Eds.), Proceedings of the international 
conference on mobile web and information systems (pp. 303–313). Cham: Springer. 
 
Littlestone, N., & Warmuth, M. K. (1994). The weighted majority algorithm. Information and 
Computation, 108(2), 212–261. 
 
Luketina, J., Berglund, M., Greff, K., & Raiko, T. (2016). Scalable gradient‐based tuning of 
continuous regularization hyperparameters. In Proceedings of the international conference on 
machine learning (pp. 2952–2960). Retrieved from JMLR.org 
 
Luo, G. (2016). A review of automatic selection methods for machine learning algorithms and 
hyper‐parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics, 
5(1), 18. 
 
Modha, D. S., Ananthanarayanan, R., Esser, S. K., Ndirango, A., Sherbondy, A. J., & Singh, R. 
(2011). Cognitive computing. Communications of the ACM, 54(8), 62–71. 
 
Nguyen, D. T., Gupta, S., Rana, S., & Venkatesh, S. (2017). Stable Bayesian optimization. In J. 
Kim, K. Shim, L. Cao, J. G. Lee, X. Lin, & Y. S. Moon (Eds.), Proceedings of the Pacific‐Asia 
conference on knowledge discovery and data mining (pp. 578–591). Cham: Springer. 
 
Pääkkönen, P., & Pakkala, D. (2015). Reference architecture and classification of technologies, 
products and services for big data systems. Big Data Research, 2(4), 166–186. 
 
Polson, N. G., & Sokolov, V. (2017). Deep learning: A Bayesian perspective. Bayesian Analysis, 
12(4), 1275–1304. 
 
Samanpour, A. R., Ruegenberg, A., & Ahlers, R. (2018). The future of machine learning and 
predictive analytics. In C. Linnhoff‐Popien, R. Schneider, & M. Zaddach (Eds.), Digital 
marketplaces unleashed (pp. 297–309). Berlin, Heidelberg: Springer. 
 
Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2016). Taking the human 
out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1), 148–175. 
 
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine 
learning algorithms. In F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger (Eds.), 
Proceedings of the 25th international conference on neural information processing systems (pp. 
2951–2959). USA: Curran Associates Inc. 
 
Sparks, E. R., Talwalkar, A., Haas, D., Franklin, M. J., Jordan, M. I., & Kraska, T. (2015). 
Automating model search for large scale machine learning. In Proceedings of the 6th ACM 
symposium on cloud computing (pp. 368–380). New York, NY, USA: ACM. 
 
Stephens, C. R., Sánchez‐Cordero, V., & González Salazar, C. (2017). Bayesian inference of 
ecological interactions from spatial data. Entropy, 19(12), 547. 
 
Subasi, O., Di, S., Balaprakash, P., Unsal, O., Labarta, J., Cristal, A., … Cappello, F. (2017). 
MACORD: Online adaptive machine learning framework for silent error detection. In 
Proceedings of the international conference on cluster computing (pp. 717–724). Honolulu, HI: 
IEEE. 
 
Sundaravarathan, K., Martin, P., Rope, D., McRoberts, M., & Statchuk, C. (2016). MEWSE: 
Multi‐engine workflow submission and execution on apache yarn. In B. Jones (Ed.), Proceedings 
of the 26th annual international conference on computer science and software engineering (pp. 
194–200). Riverton, NJ, USA: IBM Corp. 
 
Suthaharan, S. (2015). Machine learning models and algorithms for big data classification: 
Thinking with examples for effective learning. New York, USA: Springer. 
 
Suthaharan, S. (2016). A cognitive random forest: An intra‐and intercognitive computing for big 
data classification under cune condition. In V. N. Gudivada, V. V. Raghavan, V. Govindaraju, & 
C. R. Rao (Eds.), Handbook of statistics (Vol. 35, pp. 207–227). Elsevier. 
 
Tafaj, E., Kasneci, G., Rosenstiel, W., & Bogdan, M. (2012). Bayesian online clustering of eye 
movement data. In S. N. Spencer (Ed.), Proceedings of the symposium on eye tracking research 
and applications (pp. 285–288). New York, NY, USA: ACM. 
 
Thornton, C., Hutter, F., Hoos, H. H., & Leyton‐Brown, K. (2013). Auto‐WEKA: Combined 
selection and hyperparameter optimization of classification algorithms. In R. Ghani, T. E. 
Senator, P. Bradley, R. Parekh, & J. He (Eds.), Proceedings of the 19th ACM SIGKDD 
international conference on knowledge discovery and data mining (pp. 847–855). New York, 
NY, USA: ACM. 
 
Torabi, K., Sayad, S., & Balke, S. T. (2005). On‐line adaptive Bayesian classification for inline 
particle image monitoring in polymer film manufacturing. Computers & Chemical Engineering, 
30(1), 18–27. 
 
Wainer, J., & Cawley, G. (2017). Empirical evaluation of resampling procedures for optimising 
SVM hyperparameters. Journal of Machine Learning Research, 18(15), 1–35. 
 
Wang, Y., Howard, N., Kacprzyk, J., Frieder, O., Sheu, P., Fiorini, R. A., … Widrow, B. (2018). 
Cognitive informatics: Towards cognitive machine learning and autonomous knowledge 
manipulation. International Journal of Cognitive Informatics and Natural Intelligence, 12(1), 1–
13. 
 
White, T. (2012). Hadoop: The definitive guide. California, USA: O'Reilly Media, Inc. 
",345087530,"{'doi': None, 'oai': 'oai:libres.uncg.edu/24677'}",Big data analytics: Machine learning and Bayesian learning perspectives—What is done? What is not?,"{'code': 'en', 'name': 'English'}",2018-01-01T00:00:00+00:00,,[],['http://libres.uncg.edu/ir/uncg/f/S_Suthaharan_Big_2018.pdf'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/345087530.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/345087530'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/345087530/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/345087530/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/345087530'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/345087530?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=5&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Big data analytics provides an interdisciplinary framework that is essential to support the current trend for solving real-world problems collaboratively. The progression of big data analytics framework must be clearly understood so that novel approaches can be developed to advance this state-of-the-art discipline. An ignorance of observing the progression of this fast-growing discipline may lead to duplications in research and waste of efforts. Its main companion field, machine learning, helps solve many big data analytics problems; therefore, it is also important to understand the progression of machine learning in the big data analytics framework. One of the current research efforts in big data analytics is the integration of deep learning and Bayesian optimization, which can help the automatic initialization and optimization of hyperparameters of deep learning and enhance the implementation of iterative algorithms in software. The hyperparameters include the weights used in deep learning, and the number of clusters in Bayesian mixture models that characterize data heterogeneity. The big data analytics research also requires computer systems and software that are capable of storing, retrieving, processing, and analyzing big data that are generally large, complex, heterogeneous, unstructured, unpredictable, and exposed to scalability problems. Therefore, it is appropriate to introduce a new research topic—transformative knowledge discovery—that provides a research ground to study and develop smart machine learning models and algorithms that are automatic, adaptive, and cognitive to address big data analytics problems and challenges. The new research domain will also create research opportunities to work on this interdisciplinary research space and develop solutions to support research in other disciplines that may not have expertise in the research area of big data analytics. For example, the research, such as detection and characterization of retinal diseases in medical sciences and the classification of highly interacting species in environmental sciences can benefit from the knowledge and expertise in big data analytics",[],disabled
,"[{'name': 'Smith, Liezl'}]","['Boshoff, W. H.', 'Lamprecht, C.', 'Stellenbosch University. Faculty of Economic and Management Sciences. School of Accountancy.']",2019-03-14T06:10:01+00:00,"{'name': 'Stellenbosch University SUNScholar Repository', 'url': 'https://api.core.ac.uk/v3/data-providers/926'}",,,,https://core.ac.uk/download/188221025.pdf,"USER CONSIDERATIONS WHEN APPLYING MACHINE 
LEARNING TECHNOLOGY TO ACCOUNTING TASKS 
by 
Liezl Smith 
Thesis presented in partial fulfilment of the requirements of the degree of 
Master of Commerce (Computer Auditing) in the Faculty of 
 Economic and Management Sciences at Stellenbosch University 
Supervisor: Prof W Boshoff 
Co-supervisor: Dr C Lamprecht
December 2018
i 
Declaration 
By submitting this dissertation electronically, I declare that the entirety of the work contained 
therein is my own original work, that I am the sole author thereof (save to the extent explicitly 
otherwise stated), that reproduction and publication thereof by Stellenbosch University will 
not infringe any third-party rights and that I have not previously in its entirety or in part 
submitted it for obtaining any qualification. 
Liezl Smith 
Date: December 2018 
Copyright © 2018 Stellenbosch University 
All rights reserved 
Stellenbosch University  https://scholar.sun.ac.za
ii 
Machine learning is a strategic technology that can have an important effect on business, as 
it is able to perform tasks efficiently that were previously only performed by humans. When 
implementing this technology in the relevant business processes and utilising it effectively, 
users have to understand both it as well as other aspects have to be considered. It was found 
that one area that is well suited to the adoption of machine learning, is accounting. In addition, 
prior research has shown a need for accounting users to be educated in machine learning as 
part of their professional training. Therefore, the aim of this study was to enhance users’ 
understanding of machine learning technology specifically in the performance of accounting 
processes. 
A grounded theory methodology was employed to identifying the accounting tasks machine 
learning could perform, to describe how machine learning functions and to identify the risks, 
benefits and limitations associated with the technology. Finally, steps and considerations 
when implementing machine learning technology in the accounting process were provided. 
The findings of this research are that the user has a key role to play when using machine 
learning technology in the accounting processes and thus has to understand the technology, 
the risks and limitations, as well as the benefits of the technology. The risks discussed relate 
not only to machine learning technology but also to all the components that enable the 
functioning of the technology to ensure alignment with the accounting process goals. 
Based on these findings, this research presents the user considerations and steps to take 
when implementing machine learning in selected accounting processes. These can be used 
to identify areas that may require attention when a business is adopting machine learning. 
One important consideration is the implementation of adequate data governance. This is 
because most of the risks identified for machine learning technology are data risks. Further 
research could therefore be directed at developing a data governance framework for machine 
learning technologies. 
ABSTRACT 
Stellenbosch University  https://scholar.sun.ac.za
iii 
OPSOMMING 
Masjienleer is 'n strategiese tegnologie wat 'n belangrike uitwerking kan hê op besigheid, 
aangesien dit take doeltreffend kan uitvoer wat voorheen net deur mense uitgevoer is. 
Wanneer hierdie tegnologie in die toepaslike besigheids prosesse geïmplementeer en 
doeltreffend benut word, moet gebruikers dit verstaan en verskeie ander aspekte oorweeg. 
Daar is bevind dat Rekeningkunde een area is wat goed geskik is vir die aanneming van 
masjienleer. Daarbenewens, het vorige navorsing bevind dat rekeningkundige gebruikers 
opgelei moet word in masjienleer as deel van hul professionele opleiding. Die doel van hierdie 
studie was dus om gebruikers se begrip van masjienleertegnologie te verbeter, spesifiek in 
die uitvoering van rekeningkundige prosesse. 
'n Gefundeerde teorie navorsingsmetodologie is gebruik om die rekeningkundige take wat 
masjienleer kan uitvoer te identifiseer, te beskryf hoe masjienleer funksioneer en om die 
risiko's, voordele en beperkings wat met die tegnologie verband hou, te identifiseer. Ten slotte 
is stappe en oorwegings tydens die implementering van masjienleertegnologie in die 
rekeningkundige proses verskaf. 
Die bevindinge van hierdie navorsing is dat die gebruiker 'n sleutelrol speel wanneer 
masjienleertegnologie in die rekeningkundige prosesse gebruik word en dus moet die 
gebruiker die tegnologie, die risiko's en beperkings, sowel as die voordele van die tegnologie 
verstaan. Die risiko's wat bespreek word, hou nie net verband met masjienleertegnologie nie, 
maar ook met al die komponente wat die funksionering van die tegnologie moontlik maak om 
belyning met die doelwitte van die rekeningkundige proses te verseker. 
Op grond van hierdie bevindinge, bied hierdie navorsing die gebruikersoorwegings en die 
stappe om te neem wanneer masjienleer in geselekteerde rekeningkundige prosesse 
geïmplementeer word. Hierdie oorwegings en stappe kan gebruik word om areas te 
identifiseer wat aandag benodig wanneer 'n besigheid masjienleer implementeer. Een 
belangrike oorweging is die implementering van voldoende databeheer, aangesien die 
meeste van die risiko's wat vir masjienleertegnologie geïdentifiseer is, data-risiko's is. Verdere 
navorsing kan dus gerig word op die ontwikkeling van 'n data-beheerraamwerk vir 
masjienleertegnologieë. 
Stellenbosch University  https://scholar.sun.ac.za
 iv 
 
ACKNOWLEDGEMENTS 
 
I am forever grateful to all those persons who assisted and supported me in the completion 
of this work. I firstly want to thank my heavenly Father, this work was enabled by you alone. 
All glory be yours. In addition, I would like to express a special word of thanks to the following 
people: 
• My husband, William, for every hour of selfless time you gave up to stand by and 
support me. This thesis would not exist without you. 
• My grandmother, who has since passed away, for supporting me to carry on. 
• My parents, who love and support me and brought me thus far. 
• My supervisor, Stiaan Lamprecht. I have such respect for your wisdom and knowledge; 
thank you for your guidance and willingness to share your expertise with me. 
 
  
Stellenbosch University  https://scholar.sun.ac.za
 v 
 
TABLE OF CONTENTS 
 
ABSTRACT ............................................................................................................................ i 
OPSOMMING ....................................................................................................................... iii 
Chapter 1: Introduction ....................................................................................................... 1 
1.1 Background ............................................................................................................... 1 
1.2 Research focus ......................................................................................................... 2 
1.3 Research design and methodology .......................................................................... 4 
1.4 Research motivation ................................................................................................. 5 
1.5 Research scope ........................................................................................................ 6 
1.6 Limitation of research................................................................................................ 6 
1.7 Organisation of the research ..................................................................................... 7 
Chapter 2: Accounting processes and accounting tasks ................................................ 8 
2.1 Introduction ............................................................................................................... 8 
2.2 Translation of manual and electronic documents into accounting information .......... 9 
2.2.1 Breaking down the translation of documents into tasks .................................... 10 
2.2.2 Technology used in document translation ........................................................ 13 
2.2.3 Data available in document translation ............................................................. 14 
2.3 Reconciliation of financial information ..................................................................... 15 
2.3.1 Breaking down the account reconciliation process into tasks ........................... 16 
2.3.2 Technology used in the account reconciliation process ................................... 17 
2.3.3 Data available in the account reconciliation process ........................................ 18 
2.4 Preparation of management accounts .................................................................... 19 
2.4.1 Breaking down the management accounts reporting process into tasks .......... 20 
2.4.2 Technology used in the management accounts reporting process................... 22 
2.4.3 Data available in the management reporting process ...................................... 23 
2.5 Conclusion .............................................................................................................. 24 
Chapter 3: Overview of machine learning ....................................................................... 25 
3.1 Introduction ............................................................................................................. 25 
3.2 Context and framework of machine learning ........................................................... 26 
3.3 Types of machine learning algorithm ...................................................................... 27 
3.4 Machine learning architecture ................................................................................. 28 
3.5 Description of the supervised learning techniques .................................................. 30 
3.5.1 Classification algorithms ................................................................................... 30 
3.5.2 Prediction algorithms ........................................................................................ 39 
3.5.3 Dual-use algorithms: classification and prediction ............................................ 40 
3.6 Description of unsupervised learning techniques .................................................... 45 
3.6.1 Pattern detection .............................................................................................. 45 
3.6.2 Clustering ......................................................................................................... 46 
Stellenbosch University  https://scholar.sun.ac.za
 vi 
 
3.7 Description of the semi-supervised learning techniques ......................................... 50 
3.7.1 Semi-supervised clustering .............................................................................. 50 
3.8 Tasks that can be addressed by machine learning technology ............................... 51 
3.9 Conclusion .............................................................................................................. 55 
Chapter 4: Risks, benefits and limitations when implementing machine learning ...... 56 
4.1 Introduction ............................................................................................................. 56 
4.2 Machine learning technology risks pertaining to the accounting objectives ............ 56 
4.2.1 Qualitative characteristics for financial reporting .............................................. 57 
4.2.2 Machine learning risks and benefits per accounting objective .......................... 58 
4.3 Technology governance of the machine learning life cycle ..................................... 62 
4.4 Machine learning architecture risks ........................................................................ 64 
4.5 Business infrastructure risks when building machine learning models ................... 67 
4.5.1 Configuration of machine learning architecture ................................................ 68 
4.5.2 Interoperability of analysis tools ....................................................................... 68 
4.5.3 Serving architecture ......................................................................................... 68 
4.5.4 Monitoring ........................................................................................................ 69 
4.6 Acquiring machine learning from a service provider ............................................... 69 
4.7 The benefits and limitations of various machine learning techniques ..................... 70 
4.8 User-related risks .................................................................................................... 78 
4.9 Maintenance risks ................................................................................................... 78 
4.10 Security risks .......................................................................................................... 79 
4.11 Conclusion .............................................................................................................. 81 
Chapter 5: Guidelines for implementing machine learning in an accounting context 86 
5.1 Introduction ............................................................................................................. 86 
5.2 Step 1: Assigning responsibility for implementing machine learning technology .... 86 
5.3 Step 2: Consider the accounting objectives ............................................................ 88 
5.4 Step 3: Consider the machine learning model and architectural components ........ 89 
5.4.1 Machine learning model considerations ........................................................... 89 
5.4.2 Data considerations .......................................................................................... 91 
5.4.3 Feature selection and training and testing set considerations .......................... 92 
5.4.4 Algorithm considerations .................................................................................. 93 
5.4.5 Testing considerations ..................................................................................... 93 
5.5 Step 4: Consider infrastructure needs..................................................................... 94 
5.5.1 Service provider and purchased machine learning considerations .................. 96 
5.6 Step 5: Consider user requirements ....................................................................... 97 
5.7 Step 6: Consider the security requirements ............................................................ 97 
5.8 Conclusion .............................................................................................................. 99 
Chapter 6: Conclusion ..................................................................................................... 100 
List of references ............................................................................................................. 103 
Stellenbosch University  https://scholar.sun.ac.za
 vii 
 
 
LIST OF FIGURES 
 
Figure 1: The traditional record-to report process .................................................................. 9 
Figure 2: Flow chart of tasks using a bill recognition system as an example ....................... 10 
Figure 3: Tasks in the reconciliation accounting process ..................................................... 15 
Figure 4 Tasks in the management reporting process ......................................................... 19 
Figure 5: The components of management accounts .......................................................... 20 
Figure 6: The Artificial Intelligence Tree: The Many Branches of Artificial Intelligence 
Application ........................................................................................................................... 27 
Figure 7: Machine Learning Architecture ............................................................................. 29 
Figure 8: Types of classification machine learning techniques ............................................ 31 
Figure 9: An example of a decision tree ............................................................................... 33 
Figure 10: A Bayesian Belief Network showing causal relationships between events ......... 37 
Figure 11: Accurate classification of P using the k-nearest neighbour algorithm ................. 38 
Figure 12: A two-dimensional example of a support vector machine ................................... 41 
Figure 13: A single neuron ................................................................................................... 41 
Figure 14: A multilayer neural network with a hidden layer .................................................. 42 
Figure 15: An artificial network classifying elements of an invoice ....................................... 43 
Figure 16: A high level diagram of the convolutional neural network used for image 
classification......................................................................................................................... 45 
Figure 17: The Self-Organising Map Network ...................................................................... 47 
Figure 18: An example of K-means follow diagram .............................................................. 49 
Figure 19: Machine Learning Architecture (Copy of Figure 7) ............................................. 64 
Figure 20: Machine learning support infrastructure .............................................................. 68 
 
  
Stellenbosch University  https://scholar.sun.ac.za
 viii 
 
LIST OF TABLES 
 
Table 1: Research questions and corresponding research objectives ................................... 3 
Table 2: Types of reconciliations .......................................................................................... 16 
Table 3: An example of a decision tree training set ............................................................. 32 
Table 4: Naïve Bayes training data ...................................................................................... 36 
Table 5: Accounting problem types and recommended machine learning techniques......... 52 
Table 6: Technology life cycle user involvement .................................................................. 63 
Table 7: Machine learning risks mapped to accounting objectives ...................................... 58 
Table 8: Machine learning benefits per accounting objective ............................................... 61 
Table 9: Machine learning architecture risks ........................................................................ 65 
Table 10: Benefits and limitations of respective machine learning techniques .................... 70 
Table 11: Benefits and limitations of machine learning techniques mapped to objectives ... 74 
Table 12: Identified risks and relevant user considerations ................................................. 82 
Table 13: Sections applicable to accounting user tasks in data science life cycle ............... 87 
 
  
Stellenbosch University  https://scholar.sun.ac.za
 1 
 
Chapter 1: Introduction 
 
1.1 Background 
In order to maintain a competitive advantage, Gartner Inc. advises that companies should 
examine the business impacts of strategic technologies, which may indicate a need to adjust 
business models and operations. Failure to examine these technologies may mean the loss 
of a competitive advantage (Cearly, Walker & Burke, 2016). One such strategic technology 
is machine learning, a branch of artificial intelligence. Machine learning has enabled many 
tasks that would previously have been performed by humans to be efficiently and accurately 
completed by a computer. 
 
Cearly et al. (2016) state that machine learning technologies have been developed to assist 
systems to appear to understand, learn, predict and adapt. They have the potential to 
operate with little or no human guidance, which surpasses that of traditional rule-based 
algorithms. Accordingly, by implementing machine learning technologies, business may 
benefit from an increase in productivity and accuracy as well as substantial cost savings. 
 
The data science needed to create machine learning systems is complex and most 
businesses will therefore choose to acquire packaged machine learning applications rather 
than developing their own (Cearly et al., 2016). However, despite the complexity, 
management must understand the unique characteristics of machine learning technology to 
ensure that it is correctly implemented and aligned with the desired business outcomes 
(Gillion, 2017:3). 
 
The financial division of a business is one of the areas that could benefit from the application 
of machine learning, specifically the area of accounting. In considering the training given to 
professional accountants, PwC (2015:16) recognises the need for undergraduate 
accounting programmes to include advanced topics on machine learning as part of the 
curriculum. Of the various artificial intelligence skills that PwC (2015:16) recommends, 
Sutton, Holt and Arnold (2016:68) argue that machine learning is a key stream of artificial 
intelligence for application in accounting.  
 
This study addresses the need to assist users to understand machine learning technologies, 
specifically in the area of accounting, as explained in the next section. 
Stellenbosch University  https://scholar.sun.ac.za
 2 
 
1.2 Research focus 
In this section, the research problem, research aim, research questions and research 
objectives will be addressed. 
 
Machine learning techniques can assist accounting users in their decision-making. Although 
there are many types of machine learning techniques, they can be divided into two 
categories, namely, predictive techniques and explanatory techniques. The type of 
technique used will depend on the decision being made. For example, predictive techniques 
are able to predict outcomes that are based on patterns learnt by the machine learning 
model from the data although these patterns are often not explained to, or seen by, the user. 
Predictive machine learning techniques are more complex than explanatory machine 
learning techniques, which identify factors that are causally related to an outcome (Sainani, 
2014:841; Sutton et al., 2016:69). 
 
A lack of ability on the part of the accounting decision maker to discriminate between 
explanatory and predictive machine learning technologies when deciding which alternative 
to use and rely on indicates a need for machine learning research in accounting (Sutton et 
al., 2016:69). Assisting accounting decision makers to understand machine learning 
technologies and the issues that require consideration when implementing them would help 
to promote the application of these technologies in accounting and stimulate the need for 
research on these technologies. 
 
Bräuning, Hüllermeier, Keller, & Glaum (2016:296) emphasised the importance of 
understandability and model simplicity in machine learning. Understanding the technology 
and the associated benefits and risks will assist users to select one that is appropriate to 
their needs. It will also make them aware of the issues to consider and the steps to take 
when implementing and using these technologies. Accordingly, the aim of this research is 
to enhance users’ understanding of machine learning technology specifically in carrying out 
accounting processes. 
 
To achieve this aim, the research focused on three accounting processes, identifying the 
tasks involved in each process. These three accounting processes were selected as they 
cover some of the main processes in the traditional record-to-report process which is 
illustrated in Figure 1. These tasks were cast as problems in the accounting processes that 
Stellenbosch University  https://scholar.sun.ac.za
 3 
 
machine learning technology could solve. Subsequently, machine learning technologies 
were identified that could be applied to the identified tasks.  
 
In order to achieve the research aim, three research questions were formulated for this 
study. Research objectives were then in turn set for each research question. The research 
questions and the corresponding research objectives, as well as the sections in which the 
respective findings are discussed, are indicated in Table 1. 
 
Table 1: Research questions and corresponding research objectives 
Research questions Research objectives Findings 
1. Which machine learning 
technologies can be applied 
to existing accounting 
processes? 
To outline the components of the 
accounting processes. 
Section 2.2; 
Section 2.3; 
Section 2.4 
To identify the tasks that machine 
learning technology can perform. 
Section 3.8 
To identify the machine learning 
technologies that can be applied to 
the accounting process tasks. 
Section 3.8 
2. How does the machine 
learning technology function 
and what are the risks and 
benefits associated with the 
technology? 
To explain how the machine learning 
technology functions. 
Section 3.3; 
Section 3.4;  
Section 3.5 
To identify risks, benefits and 
limitations associated with the 
machine learning techniques. 
Chapter 4 
3. What are the considerations 
and steps to take when 
implementing and using 
machine learning 
technologies? 
To explain the role of the user when 
using the machine learning 
technology to address identified 
risks. 
Section 5.2  
To identify the steps to take when 
implementing machine learning 
technology to ensure alignment with 
accounting process goals. 
Chapter 5 
 
  
Stellenbosch University  https://scholar.sun.ac.za
 4 
 
1.3 Research design and methodology 
As the research questions focus on gaining theoretical insights in order to develop a method 
for understanding the technology in an accounting context, the research design will be 
exploratory. An exploratory research design allows for the development of a grounded 
picture of the phenomena as well as the development of tentative theories or hypotheses 
(USC Libraries, n.d.). 
 
A grounded theory methodology was employed to address the research problem. This is a 
methodology that is used to develop a theory, in this case theory about emergent 
technologies where there is not, as yet, an established theory (Sutton, Reinking & Arnold, 
2011:46). Bryant (2002:35) argues in this regard that the grounded theory method is 
particularly suited to information systems technology research and highlights the mandate 
of research to develop both an understanding of discovered facts and adequate models for 
specified purposes. Furthermore, this method aligns with the aim of this research, namely, 
to enhance accounting users’ understanding of machine learning technology. 
 
The research was both qualitative in nature (Creswell, 2009:4) and non-empirical, as the 
existing literature was synthesised (Torraco, 2005:357) to achieve the objective of improving 
the understanding of this technology with respect to accounting tasks. An integrative 
literature review was therefore performed with the aim of enhancing user understanding of 
machine learning, specifically with regard to its application in accounting. 
 
The literature review relied on Scopus, EBSCOhost, IEEE and AAA digital library databases 
(Sutton et al., 2016:64), as well as available publications in the form of books, accredited 
journals and academic work on machine learning, including publications specifically in the 
field of finance. 
 
As a starting point, the three accounting processes selected were investigated to determine 
which components presented problems that could be solved by machine learning. These 
problems are presented as tasks. 
 
Having provided an overview of the selected accounting processes and tasks, the research 
identified various machine learning techniques that could be applied to perform the tasks 
described. This was done by identifying the learning problem, being the task that the 
machine learning needs to be able to perform (Someren & Urbancic, 2006:363), for each 
Stellenbosch University  https://scholar.sun.ac.za
 5 
 
accounting task and then identifying the different types of machine learning technique to 
address each learning problem and explaining their capabilities. 
 
The research then considered how machine learning techniques were already being used 
in accounting practice to address the identified tasks. Where no machine learning had yet 
been applied in practice, suitable machine learning techniques were identified in the 
research. When evaluating the available machine learning technologies, it was evaluated in 
the context of the different technologies that encompass artificial intelligence. A link was 
now presented between accounting processes and machine learning techniques. 
 
Having selected the machine learning techniques to apply to the tasks identified in the 
accounting processes, the functioning of the applicable machine learning technology was 
explained. This was followed by a discussion of the risks, benefits and limitations associated 
with these technology that was grounded in the principals of King IV. Finally, in response to 
these identified risks, benefits and limitations, the issues to be considered when 
implementing the technology and the steps to take when using the technology were 
identified. 
 
1.4 Research motivation 
Bailey and Pearson (1983:532) identify a number of factors that influence user satisfaction 
when using new information technology products. These factors include understanding the 
system, the perceived usefulness of the system, and the congruence between what the user 
wanted and what the product provided.  
 
More specifically, to assist users in deciding which machine learning techniques to apply, 
an understanding of the strengths and weaknesses of these techniques in the context of 
business is useful (Bose & Mahapatra, 2001:211). Therefore, users hoping to exploit the 
advantages of machine learning in their accounting processes would benefit from an 
understanding of the technology, its benefits and its applications, as well as the issues to 
consider and the way that the user can responds to the identified risks.  
 
This research will address the need for research into strategic technologies, namely 
machine learning, and the need to assists users in understanding the technology. Assisting 
users to understand the technology will hopefully encourage them to consider implementing 
machine learning when a problem arises that machine learning could address. 
Stellenbosch University  https://scholar.sun.ac.za
 6 
 
1.5 Research scope 
The focus of this research will be on the following specific processes in accounting: 
1. Translation of manual and electronic documents into accounting software information 
2. Reconciliation of financial information 
3. Preparation of management accounts 
 
1.6 Limitation of research 
The research does not intend to address all the areas of accounting in which machine 
learning intervention is possible; it will only address the three accounting processes that 
have been identified. Only those tasks for which a suitable machine learning technology 
could be found at the time of this research were addressed. Furthermore, the research will 
only consider machine learning technologies appropriate for addressing the identified 
problem types in the accounting process and therefore does not intend to present an 
exhaustive list of machine learning technologies.  
 
Considerable research has been performed in the areas of machine learning applied to 
auditing and the detection of fraud using such technologies, therefore these areas were not 
considered for this research. 
 
In order to achieve the objective of understanding the machine learning technologies, the 
design and functioning of each technology will be explained. In doing so, the research is 
limited to explaining the design and functionality of the technology for the purposes of 
understanding it and identifying the associated risks, benefits and limitations. Therefore, this 
is not explained at the technical level required to develop the technology. 
 
The risks and benefits identified for the machine learning technology will be those that are 
unique to the technology and not those that pertain to the environment in which machine 
learning operates such as database or accounting software risks or risks pertaining to 
supporting technologies such as cloud platforms. Hence, these risks and benefits are not 
addressed in this research. 
  
Stellenbosch University  https://scholar.sun.ac.za
 7 
 
1.7 Organisation of the research 
The research consists of six chapters.  
 
Chapter 2 identifies the various accounting processes that will be addressed and outlines 
the tasks involved in these processes. This assists in outlining the areas that can be 
addressed by machine learning technology.  
 
Chapter 3 provides an overview of the machine learning techniques available to perform the 
tasks identified in the accounting processes. The chapter also provides an understanding of 
the way machine learning technology is structured and how machine learning techniques 
work. This chapter further discusses which techniques are currently applied to accounting 
process tasks and which tasks could be applied to the identified accounting processes tasks 
as based on the existing research. 
 
Chapter 4 explains the risks, benefits and limitations of machine learning technologies.  
 
Chapter 5 formulates the guidelines for implementing machine learning technologies in the 
accounting context. 
 
Chapter 6 concludes with a summary of the main findings of the study for consideration by 
users when implementing machine learning technology in an accounting context. 
  
Stellenbosch University  https://scholar.sun.ac.za
 8 
 
Chapter 2: Accounting processes and accounting tasks 
 
2.1 Introduction 
The aim of this study was to enhance users’ understanding of machine learning technologies 
specifically in the performance of accounting processes. This chapter provides a description 
of the three accounting processes that were selected, and highlights the tasks involved in 
each of these processes. The purpose of the chapter, then, is to investigate the tasks 
performed in each accounting process and the technology that is currently being employed 
in the process and, finally, to describe the tasks. The tasks which could be performed by 
machine learning are presented in chapter 3 section 3.8.  
 
To gain an understanding of the accounting processes, numerous services offered by 
financial process software providers were considered. There is a paucity of published 
research on applications that employ machine learning techniques. While the reason for this 
is unclear, it is speculated that it may be due to a lack of reporting on such applications, as 
a result of the unwillingness to reveal these applications for competitive reasons (Amani & 
Fadlalla, 2017:39). 
 
The three accounting processes identified for this research were broken down into a number 
of tasks, each representing a certain sphere of the administrative activity. After identifying 
the different tasks, specific consideration was given to identifying which technology was 
used in each accounting process. Krutova and Yanchev (2014:13) indicate that the 
technology applied in an accounting process entails a sequence of measures that assist in 
the introduction of resources at the input phase, the processing of the resources and the 
production of different levels of information sets at the output.  
 
Once the technology was identified, it was important to identify the data and the structure of 
the data available for the task, as the data needed to be appropriate for machine learning 
techniques (Fedyk, 2016). The choice of machine learning technique was based partly on 
the differences in data characteristics or data type (Someren & Urbancic, 2006:371). 
  
Stellenbosch University  https://scholar.sun.ac.za
 9 
 
Accordingly, the product of the current chapter is the layout of the tasks performed in the 
three accounting processes selected, together with an understanding of the supporting 
technologies identified as useful for enabling machine learning techniques to be applied and 
the structure of the data available in each accounting process. These will be used to identify 
the relevant machine learning techniques in chapter 3.  
 
The research is set out in accordance with the traditional record-to-report process, as 
illustrated in Figure 1. In line with this, firstly, the manual and electronic documents are 
translated into accounting information, secondly, financial information is reconciled, and 
finally, management accounts are prepared. 
 
 
Figure 1: The traditional record-to-report process (Deming, n.d.:7) (Adapted)  
 
Figure 1 above shows that the record-to-report process commences with the external 
information sources, which are covered in section 2.2, followed by account reconciliations, 
which are addressed in section 2.3, then journal entries and month-end closure, and finally 
analysis and reporting are performed, which are addressed in section 2.4. This study does 
not address the performance of compliance and control procedures. 
 
2.2 Translation of manual and electronic documents into accounting information 
The first process to be addressed is the translation of manual and electronic documents into 
accounting information. The purpose of converting documents into a digital form is to enable 
businesses to analyse the documented data efficiently and more affordably. In pursuance 
of this purpose, the process of detecting, extracting and processing data from documents 
needs to be efficient and accurate (Ming, Liu & Tian, 2003:489; Rhodes & Wheat, 2015:1). 
The efficiency and accuracy of the process may be increased by incorporating machine 
learning techniques. 
 
 
External
Information
Sources
Account 
Recon-
ciliations
Journal 
Entry
Month-End 
Close Analysis Reporting
Compliance
& Control
(Section 2.2) (Section 2.3) (-----------------------Section 2.4-----------------------------) (-----N/A-----) 
Stellenbosch University  https://scholar.sun.ac.za
 10 
 
2.2.1 Breaking down the translation of documents into tasks 
This section describes each of the tasks in the process of translating documents into 
accounting information. Various documents need to be translated into electronic information 
for accounting purposes. An example of this process is illustrated in Figure 2 (Ming et al., 
2003:490) for a supplier invoice. 
 
 
Figure 2: Flow chart of tasks using a bill recognition system as an example (adapted 
from Ming et al., 2003:490) 
 
In Figure 2 the tasks in the documentation translation process are set out in the order in 
which the software or user performs them. The tasks are discussed in more detail as follows: 
 
Task 1. Input: Documents such as invoices are uploaded into the translation software in the 
form of a paper document which is either scanned or loaded electronically, for example in 
the form of an email attachment (Kohlmaier, Hess & Klehr, 2006:1). The document format 
will affect the structure of the available data as described in section 2.2.3. 
 
Task 2. Pre-processing and standardisation: Documents are pre-processed prior to data 
extraction. Various algorithms, which are described in section 2.2.2, are applied to correct 
the images and then standardising techniques are applied to these images, which include 
Input 
↓ 
Pre-processing and standardisation 
↓ 
Document features extraction 
↓ 
                                                                Known type? → No → Update form feature library 
      ↓ Yes 
          Form data field extraction and data recognition 
↓ 
       Validation of document data 
↓ 
Export: Processing and storage of data 
Task 1: 
Task 2: 
Task 3: 
Task 4: 
Task 5: 
Task 6: 
Task 7: 
Stellenbosch University  https://scholar.sun.ac.za
 11 
 
standardising the page size, page orientation, font, text size, colour, element positioning, 
page numbering, margins, column size, watermarks, line numbering and page breaks 
(Rhodes & Wheat, 2015:0029).  
 
 
Task 3. Document features extraction: The components of the documents are translated 
by identifying the document template and the document schema. The schema identifies the 
different elements of information which are to be extracted from the document while the 
template sets out how the information is physically arranged in the document. Based on the 
identified template, the system will then have a set of rules for locating the information for 
each schema in the document (Sorio, 2013:25). 
 
The data is extracted from the documents and converted to an electronic format using 
Optical Character Recognition (OCR) (Kohlmaier et al., 2006:1; Rhodes & Wheat, 
2015:0031). Accordingly, the document features may trigger algorithms which label 
identifiable data elements. For example, the presence of the words “Invoice number” may 
trigger the string of numbers following that header to be labelled as invoice number 
(Kohlmaier et al., 2006:1). The technology used to perform the data and features extraction 
is described in section 2.2.2. 
 
Additionally, the document translation software may extract other types of data from the 
document, for example the metadata, which identifies the document creator, time created, 
document file size, creation date and any modification dates, type of text in the file, version, 
font, name, and other data. These elements can be used as a database navigation key 
(Rhodes & Wheat, 2015:5). 
 
Task 4. Form type feature library which includes document type recognition and 
classification: A library, knowledge repository or set of template-specific extraction rules 
may be kept of known document types, for which particular content, structure, form or other 
attributes have been established. Using the library, the document translation software can 
automatically determine where to find relevant data on a given document based on the 
document data type recognised (Ming et al., 2003:493; Sorio, 2013:27; Rhodes & Wheat, 
2015:0036).  
 
Stellenbosch University  https://scholar.sun.ac.za
 12 
 
The document type can be determined by the document translation software based on the 
extracted identifiable data elements such as the schema and the template by considering 
known document types in the form type feature library as described.  
 
If, however, the document is an unknown type, the extracted results are displayed and the 
results need to be confirmed manually item by item. In this way, the user can train the system 
to identify and extract the key data. These features are subsequently stored in the form-type 
feature library. These stored rules are then able to interpret future documents from the same 
source (Ming et al., 2003:490; Kohlmaier et al., 2006:2; Sorio, 2013:23).  
 
Task 5. Form data field extraction and data recognition: Once the document type is 
established, the document translation software determines where to locate important data 
on the given document (Ming et al., 2003:493; Rhodes & Wheat, 2015:0036). Thus the 
software is able to present structured data from the extraction, because it is able to 
determine which data is important and how that data should be recognised. For example, 
the identifying elements in an invoice, such as an invoice number, enable the software to 
identify the document as an invoice and process the data as invoice data (Kohlmaier et al., 
2006:2). 
 
Task 6. Validation of document data: The software tests the validity, accuracy and 
completeness of the data by means of validation tests. These are, for example, able to 
ascertain whether there are inconsistencies between paired documents such as purchase 
orders and supplier invoices, or whether the invoice is a duplicate invoice. Error detection 
may then prompt the user to process the invoice manually (Kohlmaier et al., 2006:1,2).  
 
Task 7. Processing and storage of data: After the data extraction process, the user is 
shown the structured dataset generated. This interface may be interactive to allow entries 
in the generated dataset to be edited manually if necessary (Rhodes & Wheat, 2015:9). The 
user then confirms the data, and the interpreted data is converted into a file which is saved 
in a database in a specific document format such as Extensible Mark-up Language (XML), 
although there are various other documents formats that could be used (Rhodes & Wheat, 
2015:0068). XML is a standard data exchange format with many variations, which makes 
the data structure complex (Lee, Tsatsoulis & Perry, 2009:1). 
 
Stellenbosch University  https://scholar.sun.ac.za
 13 
 
2.2.2 Technology used in document translation 
Certain technologies can be greatly improved by incorporating machine learning techniques. 
The technologies identified in the document translation process are set out in accordance 
with the various tasks identified in section 2.2.1.  
 
Task 1. Input: The technology employed to upload electronic documents include scanners, 
image capturing devices such as digital cameras and a variety of software, producing 
electronic documents in diverse formats.  
 
Task 2. Pre-processing and standardisation: Images may be corrected by means of a 
trembling process to reduce noise in the image, and image angle testing may be conducted 
to correct for any slant in handwritten words or in entire images. This technology improves 
the data extraction task (Ming et al., 2003:490-492). 
 
Task 3. Document features extraction: Once documents are standardised, recognisable 
elements such as text, numbers and special characters can be automatically detected using 
optical character recognition (OCR) (Rhodes & Wheat, 2015:2).  
 
OCR is a technique used to convert scanned documents into computer readable text 
(Larsson & Segerås, 2016:5). The three basic principles applied to OCR for recognising 
objects include integrity, purposefulness and adaptability (Emmanuel & Nithyanandam, 
2014:439; ABBYY Technologies, n.d.). The last-mentioned, adaptability, may be assisted 
by the program being able to learn by itself, as enabled by machine learning. 
 
Task 4. Form-type feature library which includes document type recognition and 
classification: The document type may be recognised based on its form using image 
classification, which sorts documents by appearance or pattern. This may incorporate 
machine learning techniques. Text classification can be used to classify the document type 
based on content, and both statistical and semantic text analysis are employed to classify 
text content (ABBYY, 2017). 
 
The system can be trained to process flexible or irregular document layouts by incorporating 
machine learning techniques together with natural language processing (NLP) (ABBYY, 
2017). NLP converts human language into a format that computers are able to recognise 
and use (Collobert & Weston, 2008:160). 
Stellenbosch University  https://scholar.sun.ac.za
 14 
 
 
Task 5. Form data field extraction and data recognition: Template-specific extraction 
rules are algorithms which enable the system to extract information from documents. These 
rules predict which field a specific data item refers to, for example a particular item may be 
the invoice number field and another the date field (Sorio, 2013:17). 
 
Case-based reasoning, based on previous cases, can be used to decide which techniques 
to use to extract the data fields (Hamza, Belaïd & Belaïd, 2007:327; Larsson & Segerås, 
2016:8). Watson (1999:307) describes case-based reasoning as a method which attempts 
to solve cases by using solutions observed in similar previous cases. 
 
Task 6. Validation of document data: As with any data input, the accuracy of the data will 
need to be verified. Arithmetic validation rules can be applied to ensure the accuracy of the 
data (OCREX, 2017). In addition, machine learning can be used to determine whether the 
data from the document is correct, as demonstrated by Larsson and Segerås (2016:37). 
 
Task 7. Processing and storage of data: Bose and Mahapatra (2001:212) indicate that 
data warehousing technology enables the organisation and storage of large amounts of 
financial information in a form that can be analysed using machine learning techniques . 
 
The classification of the problems or tasks that can be addressed by machine learning, as 
well as the description of the specific techniques available for use, are described in chapter 
3 section 3.8. 
 
2.2.3 Data available in document translation 
Printed documents may have weak structures caused by the placement of the text on the 
page. This has to be addressed using template specific extraction rules. Other documents 
may be in electronic and structured formats such as XML, from which the information in the 
document can be easily determined (Sorio, 2013:13).  
 
Documents, and therefore the data used in training machine learning technology, are easily 
accessible and the input data either already exists in digital form or it can be easily digitised 
(SMACC, 2017:6). However, variations in the components of data input indicate the need 
for solutions to be adaptable. This consideration is discussed in chapter 5 section 5.4.1. 
 
Stellenbosch University  https://scholar.sun.ac.za
 15 
 
2.3 Reconciliation of financial information 
This section will address the second accounting process, which is the reconciliation of 
financial information. The purpose of the account reconciliation process is to verify the 
integrity of a business’s account balances. An example of the process is illustrated in 
Figure 3. 
 
Figure 3: Tasks in the reconciliation accounting process (adapted from BlackLine, 
2014) 
 
These tasks will be performed for all the reconciliations a business has to perform as part of 
its overall financial reporting process and will depend on the industry in which the business 
operates and the nature of the transactions. Not all reconciliations are financial, for example 
businesses may require industry-specific reconciliations, but these will still support the 
reporting process (Trintech, 2017). Table 2 (Trintech, 2017) presents a summary of different 
types of reconciliation. 
 
 
 
 
Task 3: Matching 
Task 4: Exceptions identified 
Task 5: Exceptions investigated 
Task 6: Corrective action 
Task 7: Documentation 
↓ 
Task 1: Data preparation 
Task 2: Comparison 
↓ 
↓ 
↓ 
↓ 
↓ 
Stellenbosch University  https://scholar.sun.ac.za
 16 
 
Table 2: Types of reconciliation  
Category Type of reconciliation 
Bank reconciliations Bank accounts 
Credit cards 
Gift cards 
Currencies 
Other reconciliations Inventory 
Supplier statements 
Goods receipt invoice receipt 
Suspense accounts 
System to system 
Intercompany 
Payroll 
Inventory 
Industry specific reconciliations Airliners (bag drop) 
Insurance (policy) 
Hospital (patient count) 
Source: Adapted from Trintech (2017)  
 
2.3.1 Breaking down the account reconciliation process into tasks 
BlackLine (2014), a company which is endorsed by SAP for its accounting automation 
software, describes the account reconciliation process. The tasks performed in the account 
reconciliation process are as follows: 
 
Task 1. Data preparation: Data is collected and processed into an appropriately 
comparable format, which depends on the technology employed to perform the comparison. 
 
Task 2. Comparison: The transactional data contained in the account balance and the 
information produced by an independent system are compared. For example, for the bank 
general ledger account, a comparison is made between the information contained in the 
general ledger bank account and the information as per the bank statements, which are 
produced by an independent system. 
 
Stellenbosch University  https://scholar.sun.ac.za
 17 
 
Task 3. Matching: Transactional data records contained in the one set of information are 
matched to the corresponding record in the second set of information. These records are 
linked and marked as reconciled, verifying the integrity of the data. 
 
Task 4. Exceptions identified: Discrepancies are identified where differences are detected 
between the data being compared in each set of information, or where data available in one 
system has no corresponding data in the other system.  
 
Task 5. Exception investigation: The discrepancies identified are investigated by 
scrutinising the origin of the data records and inspecting the supporting sources for the 
existing data in order understand the cause of the discrepancies. 
 
Task 6. Corrective action: Once the cause of discrepancies is determined, corrective 
action is taken. This may involve making journal entries to correct balances or transactional 
errors. 
 
Task 7. Documentation: The investigation process is documented together with the 
corrective action and any supporting documentation, after which all of this information is 
stored for audit purposes.  
 
2.3.2 Technology used in the account reconciliation process 
The technologies required in the reconciliation process are set out in accordance with the 
different tasks identified in section 2.3.1. 
 
Task 1. Data preparation: The datasets that are compared may be imported from a variety 
of sources, including accounting software packages, enterprise resources planning (ERP) 
software (described in section 2.4.2), external platforms such as banking or supplier 
platforms or data converted from scanned manually generated documents (BlackLine, 
2014). The technology used in the preparation of data is discussed in section 2.2.2 above. 
 
Task 2 and 3. Comparison and matching: Technologies employed to perform data 
matching have included record linkage approaches. These record linkage approaches 
include rules-based approaches, which rely on heuristics. However, the drawback of this 
method is that heuristics developed for one application are not likely to work for another. For 
automated reconciliation, deep learning and statistical methods are recommended (Chew & 
Stellenbosch University  https://scholar.sun.ac.za
 18 
 
Robinson, 2012:326). The problems that can be addressed with machine learning are 
described in chapter 3 section 3.8. 
 
Task 4 to 7. Exception investigation, correction and documentation: The software will 
produce a report displaying both the unmatched and the matched items (Chew, 2014:2). 
The user will have to investigate, correct and document any unmatched items. 
 
2.3.3 Data available in the account reconciliation process 
An account reconciliation will consist of two transaction datasets, each of which may be 
from a different source and in a different format (Chew, 2014:1). Before the software can 
perform a reconciliation, the transactional data must be converted into the correct format. 
 
2.3.3.1 Sources of data 
Financial datasets may be sourced from the entity’s own ERP system, accounting software 
package, or other internal system, while other datasets may comprise third-party 
documents, such as bank statements and supplier statements obtained from external 
platforms such as those of banks and suppliers (BlackLine, 2014). 
 
2.3.3.2 Data format 
The data, namely, the transactions being reconciled, consist of a set of transaction features. 
These transaction features may comprise only one data field or may be separated into a 
number of fields. The features represent the information that makes up the transaction and 
enables the software to distinguish one transaction from another. For example, the feature 
may comprise the transaction description, transaction date, account number or transaction 
value, represented as separate fields or one single field, depending on the source from 
which the data is obtained (Chew, 2014:3). 
 
The data in each dataset may appear in a structured format such as categorical or 
continuous data or it may be in an unstructured format, with free-form text. Free-form 
transaction descriptions mean that the descriptions are at the user’s discretion and not 
limited by a category such as an account number or invoice number, thus making this 
unstructured format more difficult to reconcile as result of the variation (Chew & Robinson, 
2012:324). 
 
Stellenbosch University  https://scholar.sun.ac.za
 19 
 
2.4 Preparation of management accounts 
The final process to be addressed is the preparation of management accounts. Management 
accounts are a summary of a business’s accounting data which is prepared for the firm’s 
management. The purpose of preparing management accounts is to provide information 
that can assist management with decision-making (Whittington, 2007:198; Gorbunova & 
Bochkarev, 2011:25). An example of the process is illustrated in Figure 4. 
 
 
Figure 4 Tasks in the management reporting process (adapted from DBASS Chartered 
Accountants, 2014) 
 
Management accounts have three components, namely, financial reporting, performance 
measurement and cost planning (Cokins, 2013:27). Figure 5 illustrates the different 
components and gives examples of the different types of report that make up each 
component, in line with Trigo, Belfo and Estébanez (2014:120). 
 
The reports contained in management accounts may be either descriptive and retrospective, 
providing an overall view of the business’s historical performance, or they may be predictive 
Task 3: Account allocation 
Task 4: Report templates 
selection 
Task 5: Posting of key journals 
Task 6: Period selection 
Task 7: Report generation 
Task 8: Report descriptions 
↓ 
Task 1: Set up of the chart of 
accounts 
Task 2: Validation of data 
↓ 
↓ 
↓ 
↓ 
↓ 
↓ 
Stellenbosch University  https://scholar.sun.ac.za
 20 
 
and prospective, making use of historical data to predict future events (Amani & Fadlalla, 
2017:35; Appelbaum, Kogan, Vasarhelyi & Yan, 2017:36). The differences inherent in the 
reports affect the tasks involved in their preparation.  
 
 
Figure 5: The components of management accounts (adapted from Cokins, 2013:27)  
 
Business needs may vary, and small businesses in particular are unlikely to produce all the 
reports mentioned here. Those that are produced would be dependent on management’s 
decision-making needs (CPA Australia Ltd, 2011:4). 
 
2.4.1 Breaking down the management accounts reporting process into tasks 
The first set of reports included in the management accounts is the financial reports, which 
are generated using internal data (Appelbaum et al., 2017:35). In order to produce an 
accurate set of financial reports, the process followed in the Sage Line 50 software was 
considered (DBASS Chartered Accountants, 2014). The tasks included in the preparation 
of financial reports are as follows: 
 
Reporting 
Financial reporting 
Performance 
measurement 
Cost planning • Transaction processing 
• Accounts payable 
• Accounts receivable 
• Costing of goods sold 
• Inventory valuation 
 
• Variance reports 
• Budget vs. actual 
• Profitability 
• Cash flow 
management 
• Key personnel 
indicators 
 
• Budgeting 
• Forecasting 
• What-if analysis 
• Analysis of strategic 
options 
• Benchmarking 
• Strategic management 
accounting 
• Business risk 
management 
Management reporting 
Stellenbosch University  https://scholar.sun.ac.za
 21 
 
Task 1. Set up of the chart of accounts: The chart of accounts (COA) is an organised 
listing of the individual accounts that are used to record transactions and make up the 
reporting lines of the ledger system. The COA therefore provides the structure for 
classification of financial information. Classification structures will vary from company to 
company. Management accounts could, for example, break down financial information into 
underlying segments such as departments, products, employees, geographical locations, 
projects and customers (Cooper & Pattanayak, 2011:3). 
 
Task 2. Validation of data: The data used to prepare the management accounts must be 
accurate, valid and complete. Therefore, prior to preparing the accounts, all transactions 
should be posted and reconciliations performed as described in sections 2.2 and 2.3 above. 
 
Task 3. Account allocation: The transactions used in preparing the reports may be stored 
in a database and will need to be allocated to the specific chart of accounts by the user. The 
account allocation will be based on the nature of the transaction.  
 
Task 4. Report templates selection: Reporting templates may be used in the preparation 
of management accounts. These are templates used for creating financial reports with 
predefined fields and formats. The templates can be predefined or blank and customisable 
(Roy, 2005:3). 
 
Task 5. Posting of key journals: Period end journals may need to be posted, including for 
example wage journals, stock journals, prepayments and accrual journals. 
 
Task 6. Period selection: The period for which the management accounts are prepared is 
selected. 
 
Task 7. Report generation: The reports that make up the management accounts are 
produced by summarising the transactions for each reporting line in accordance with the 
chart of accounts mapped onto reporting templates. 
 
Task 8. Report descriptions: Notes and descriptions may be added to the quantitative 
information contained in the management accounts to assist users’ understanding of the 
information. 
 
Stellenbosch University  https://scholar.sun.ac.za
 22 
 
Next, performance measurement is carried out using insights and inferences, as well as 
analysis of the processes or events that have taken place, to evaluate corporate 
performance. This process uses mainly internal data, although some external data such as 
industry benchmark information is also used to process cost analysis reports.  
 
Cost planning reports are subsequently produced using both the financial reports and the 
analysis performed. Among other things, these cost planning reports forecast the future 
business performance, determine a budget to achieve the desired forecast and evaluate 
strategic options (Appelbaum et al., 2017:35).  
 
2.4.2 Technology used in the management accounts reporting process 
In this section, the technologies required for preparing the management accounts are 
discussed in line with the various tasks identified in section 2.4.1. In this process, technology 
is used to prepare the data, carry out the matching, produce the report of any exceptions 
identified and process any corrective actions. 
 
User guidelines are discussed in chapter 5. As part of these guidelines, section 5.5 
discusses the investment in technology required to enable the use of machine learning 
techniques. This section identifies those technologies that may enable the use of machine 
learning techniques in reporting. 
 
Management accounts can be prepared either using information from the business’s 
accounting information system or extracting it from the accounting module of a larger 
enterprise resource planning application.  
 
An accounting information system (AIS) is an application which works together with other 
information technology systems to record accounting transactions. An AIS collects, stores 
and processes financial and accounting data which then used to prepare reports for use by 
management; data may also include nonfinancial transactions that may impact the 
processing of financial transactions (Belfo & Trigo, 2013:537). 
 
An enterprise resource planning (ERP) system is an information system that integrates 
resources, business process activities and information. It consists of various modules, of 
which accounting is one. The processes performed in the accounting module include the 
Stellenbosch University  https://scholar.sun.ac.za
 23 
 
creation of a chart of accounts, posting to journals and the generation of financial statements 
(Adhitama, Sarno & Sarwosri, 2016:20). 
 
An ERP system enables management to access the operational data required for decision-
making and business control (Appelbaum et al., 2017:31). 
 
Extensible Business reporting language (XBRL) assists in the integration of applications. 
XBRL, a XML-based language, is a global standard for communicating business information. 
Accordingly, its use will ensure interoperability, thus allowing accounting information 
systems be more integrated with other systems. This is important as the automation of 
management accounting reporting will be dependent on the level of integration of the 
different applications producing information for the reports (Belfo & Trigo, 2013:542).  
 
Natural language generation (NLG) used in producing report descriptions is a 
technology that converts structured data into written or spoken language. By incorporating 
an inference engine, the NLG system can perform tasks like summarising large amounts of 
data, explaining why datasets change, and making recommendations (Yseop, 2017:6). 
 
2.4.3 Data available in the management reporting process 
During the analysis of the document translation and reconciliation processes, the 
recommended format identified for data was XBRL. The Companies and Intellectual 
Property Commission (CIPC) acknowledges that XBRL can be used to integrate back-end 
processes in companies when automating the preparation of financial statements 
(Companies and Intellectual Property Commission, 2017b).  
 
The CIPC is the central government agency in South Africa responsible for the registration 
of all companies. The CIPC indicates that the use of XBRL enables the automatic verification 
of compliance by means of a validation engine, with the aim of improving the efficiency and 
accuracy of the reporting process (Companies and Intellectual Property Commission, 
2017b). 
 
Big data may enable the preparation of more complex reports as it ensures sufficient data 
for making decisions. Big data can consist of data gathered inside the business – because 
this data is usually stored in a database it is generally structured – and external data, which 
is gathered from outside the business. External data is generally unstructured and therefore 
Stellenbosch University  https://scholar.sun.ac.za
 24 
 
often requires analytical tools to extract the information required for decision-making 
(Appelbaum et al., 2017:35). 
 
2.5 Conclusion 
This chapter described the three accounting processes, as well as the tasks performed in 
each process, the supporting technologies that would enable the inclusion of machine 
learning techniques in the process and the data available in the respective accounting 
processes.  
 
An understanding of the tasks performed is the key to identifying the areas that can be 
addressed by machine learning technology. Each of the tasks that can be addressed using 
machine learning techniques will present a learning problem. Their machine learning 
solutions will be identified in chapter 3 and will be organised in the same sequence as the 
identified tasks. 
  
Stellenbosch University  https://scholar.sun.ac.za
 25 
 
Chapter 3: Overview of machine learning 
 
3.1 Introduction 
Having identified the different tasks in the accounting processes in chapter 2, this chapter 
describes the tasks that are considered problem areas and which are to be addressed by 
machine learning technology. The chapter also considers areas where machine learning 
techniques may be employed to enhance the capabilities of existing technologies in the 
respective processes (identified in chapter 2).  
 
The objectives of this chapter of the study are to describe the components of the machine 
learning technology and the different machine learning techniques, as well as to identify the 
learning problems that machine learning techniques can address and that can be applied to 
each of the identified learning problems. This is in line with one of the research objectives 
of this study: to identify machine learning techniques that can be applied to the tasks in the 
accounting process. 
 
The first section, section 3.2, provides a context for machine learning, which is considered 
important in view of the aim identified for this study, namely, to enhance users’ 
understanding of machine learning technology specifically in the performance of accounting 
processes.  
 
The next section provides a description of the types of machine learning and the 
components of a machine learning architecture. Various machine learning techniques are 
then described and, in conclusion, the final section indicates the learning problems that can 
be applied to these techniques.  
 
The machine learning techniques identified in this chapter are then discussed in more detail 
in chapter 4, where the risks and benefits associated with the specific machine learning 
techniques are described, as well as the risks, benefits and limitations of machine learning 
technology. These identified risks, benefits and limitations are then used in chapter 5 to 
formulate guidelines for implementing machine learning technology in an accounting 
context. 
 
 
Stellenbosch University  https://scholar.sun.ac.za
 26 
 
3.2 Context and framework of machine learning 
Machine learning is one of the technologies that can be used in the accounting process to 
assist in the automation of tasks. Tools used in automation can be divided into different 
classes, namely, rules-based automation (robotic), knowledge-based cognitive intelligence 
and artificial intelligence. Combining artificial intelligence tools such as machine learning 
with robotic tools can automate the processing of unstructured inputs from beginning to end 
(Everest Group, 2014:11).  
 
Machine learning is a subset of artificial intelligence, where patterns in data are learnt and 
applied in a changing environment. The technology does not require all possible situations 
to be known during development. Machine learning can be used in two ways: to detect the 
patterns that explain a process, known as explanatory machine learning technology, and to 
make predictions; this is known as predictive machine learning technology (Ayodele, 
2010a:2; Sainani, 2014:841). 
 
Machine learning technology is able to predict solutions or detect patterns despite 
uncertainty. This differs from knowledge-based systems which can only solve problems 
using stored knowledge and facts, as well as heuristics and other elements such as models 
and known patterns provided by human experts (Valavanis, Kokkinaki & Tzafestas, 
1994:114).  
 
Figure 6 (Sutton et al., 2016:62) illustrates the different branches of artificial intelligence and 
therefore provides the context of machine learning in relation to other artificial intelligence 
technologies. 
 
Stellenbosch University  https://scholar.sun.ac.za
 27 
 
 
Figure 6: The artificial intelligence tree: the many branches of artificial intelligence 
application (adapted from Sutton et al., 2016:62)  
 
It is important to be able to distinguish between the different artificial intelligence 
technologies, as different technologies may be able to address different user needs in 
business processes (Everest Group, 2014). However, knowledge-based systems are 
beyond the scope of this study. 
 
Having provided a context for machine learning technology, the different types of machine 
learning technique are discussed in the next section. This is intended to contribute to 
understanding the framework for machine learning technology and was considered 
important, as non-expert users of machine learning techniques do not usually understand 
the terms that distinguish the different types of machine learning (Someren & Urbancic, 
2006:366). 
 
3.3 Types of machine learning algorithm 
Having provided the context for machine learning within the field of artificial intelligence, as 
well as a description of the different components of machine learning technology, this 
section describes the different types of machine learning algorithm. The different machine 
learning types are distinguished by considering the objective of the algorithm, how the 
machine learning algorithm learns, as well as the structure and volume of the data used for 
learning (Ayodele, 2010:19; Castle, 2018:1). 
Artificial Intelligence 
And 
Intelligent Systems 
Machine 
Learning 
3.3 
 
Knowledge-Based 
Systems 
(Not addressed) 
Supervised 
Learners 
3.5  
Unsupervised 
Learners 
3.6 
 
Classifiers 
3.5.1 
 Predictors 
3.5.2 
 Dual use 
3.5.3  
 
Pattern Detection 
3.6.1 
 Clustering 
3.6.2 
 
Intelligent Decision Support System 
Expert Systems 
Intelligent Decision Aid 
 
Intelligent Agents 
Semi-supervised 
Learners 
3.7 
 
Stellenbosch University  https://scholar.sun.ac.za
 28 
 
Supervised learning algorithms require training. The algorithm is trained by using a labelled 
dataset which consists of examples of input data as well as the labels which indicate 
predicted targets or output data. Labels assist the algorithm in determining which features 
are important. The algorithm then generalises the training set by mapping the inputs to the 
correct responses, which enables it to produce output for new inputs (Ayodele, 2010:19; 
Castle, 2018:1; Larsson & Segerås, 2016:11; Marsland, 2009:6; SMACC, 2017:9).  
 
Unsupervised learning algorithms do not require training. The input data is unlabelled, 
meaning the predicted values are not provided, which may be because they are unknown. 
The algorithm needs to determine the links between the inputs provided to identify patterns 
or commonalities that can be used to categorise new data or solve problems (Ayodele, 
2010:19; Larsson & Segerås, 2016:11; Marsland, 2009:6; SMACC, 2017:9). 
 
Semi-supervised learning algorithms are trained using a combination of labelled and 
unlabelled data to generate an appropriate function. The labelled portion indicates patterns 
which may exist, while the unlabelled data, usually the larger portion of the data, is used to 
establish perceived or unknown patterns for the data (Ayodele, 2010:19; Castle, 2018:1). 
 
Having described the different machine learning technique types, the machine learning 
architecture is described in the next section. The architecture provides an overview of the 
components of the machine learning technology, including the different areas required to 
create a machine learning model that supports the use of the machine learning techniques.  
 
3.4 Machine learning architecture 
This section describes the different components of the machine learning architecture. This 
architecture will be adapted to the needs of the machine learning model as determined by 
the type of machine learning techniques used for a given task.  The different machine 
learning techniques are subsequently described in the next section. 
 
Gartner recommends that the following five functional areas be included in the machine 
learning architecture: data acquisition, data processing, data modelling, execution and 
deployment (Sapp, 2017:19). These areas are illustrated in Figure 7 which diagrammatically 
demonstrates the machine learning architecture. 
 
 
Stellenbosch University  https://scholar.sun.ac.za
 29 
 
 
Figure 7: Machine learning architecture (adapted from Sapp, 2017:20)  
 
The functional areas illustrated in Figure 7 of the machine learning architecture are 
described by Sapp (2017:21) as follows: 
 
Area 1: Data acquisition 
Encompasses the collection and preparation of data for processing from a variety of sources 
and ensures that the data is reliable and adaptable for processing. 
 
Area 2: Data processing including feature analysis 
This area normalises and transforms the data into a structure suitable for machine learning. 
In addition, training sets and testing sets are selected in this area. A training set is a dataset 
which is used by the algorithm to identify relationships (Dataiku, 2017:5), while a testing set 
is a dataset used to assess whether the algorithm functions as desired. Feature analysis is 
performed to assess which features of the data are required for training the machine learning 
algorithm.  
  
Area 3: Model Engineering 
Area 4: Execution 
Area 5: Deployment 
Area 1: Data 
acquisition 
Area 2: Data processing 
(feature engineering) 
• ERP databases 
• Mainframe 
• Devices 
Data ingestion 
Stream processing 
platform 
 
Batch data 
Warehouse 
Pre-processing data 
↓ 
Sample selection 
↓ 
Training set 
↓ 
Testing set 
Machine learning 
algorithms Experimentation 
↓ 
Testing 
↓ 
Tuning 
↓ 
Execution Data storage 
Stellenbosch University  https://scholar.sun.ac.za
 30 
 
Area 3: Data modelling 
This area includes the selection of the machine algorithms and the adaptation of the 
algorithm to the identified learning problems.  
 
Area 4: Execution 
In this area the prepared data and the machine learning algorithm are brought together to 
train the machine learning algorithm, test the model and then make any necessary changes 
to the algorithm to ensure that the machine learning model operates in a way that addresses 
the learning problem. 
 
Area 5: Deployment 
In this area the outputs of the machine learning model are made available for use in the 
applicable business applications or are stored as data to be used in reporting for example. 
 
Having described the different machine learning technique types and the machine learning 
technology architecture, the different machine learning techniques available to address the 
learning problems identified in the form of accounting tasks are described in the next section. 
 
3.5 Description of the supervised learning techniques 
Descriptions of each of the machine learning techniques are provided in sections 3.5 to 3.7. 
The techniques are organised by learning problem type, as shown in Figure 6 of section 3.2. 
In describing the different machine learning techniques, the term “features” in data science 
refers to the independent variables or predictor variables (Dataiku, 2017:5).  
 
The techniques described are applied to the learning problems in section 3.8. The benefits 
and limitations of the different machine learning techniques are identified in chapter 4 section 
4.7. This section describes the various supervised learning techniques, starting with 
classification algorithms. 
 
3.5.1 Classification algorithms 
Classification algorithms are supervised learners, and therefore their development consists 
of a two-step process consisting of training and testing. During training, the algorithm maps 
class labels to data features. These features can predict the class labels of new data by 
learning from a training dataset that will consist of a set of data records and associated class 
Stellenbosch University  https://scholar.sun.ac.za
 31 
 
labels for each record. Once trained, the accuracy of the classifier is assessed by testing its 
ability to predict classes using the test dataset (Chadha & Singh, 2012:51). 
 
The different supervised classification algorithms can be separated into three different types 
of technique, namely, logic-based techniques, perceptron-based techniques and statistical 
techniques (Kotsiantis, 2007:251). These are presented in Figure 8. 
 
 
Figure 8: Types of classification machine learning technique (adapted from Kotsiantis, 
2007:251) 
 
Logic-based techniques use acquired knowledge from examples to classify data (Lopez De 
Mantaras & Armengol, 1998:99), whereas according to Kotsiantis (2007:254), perceptron-
based techniques are based on the ability of a perceptron to determine the weights to assign 
to each identified feature and calculate the appropriate class using the features and 
assigned weights. 
 
Also of importance here is Kotsiantis' (2007:257) description of statistical techniques as 
those techniques that make use of a probability model to determine the probability that an 
instance belongs to a particular class. The first of the techniques to be described will be the 
logic-based technique of the decision tree. 
 
3.5.1.1 Logic-based: decision trees 
The decision tree consists of nodes, each containing a question which relates to a particular 
feature. The algorithm starts at the root node, determines which features are present for that 
root node question and the, depending on the answer, moves on to the next node. The 
Classification 
techniques 
Statistical 
Logic-based 
Perceptron-based 
 
Naïve Bayes 
Decision trees 
K-nearest neighbour 
Bayesian belief networks 
Support vector machines 
(dual-use algorithm) 
 
C4.5 decision trees 
Random forests Transfer learning 
decision forests 
Neural network 
(dual-use algorithm) 
Stellenbosch University  https://scholar.sun.ac.za
 32 
 
information required to train these decision trees takes the form of instances, which consist 
of a set of features. 
The instance moves along the branches to assess different features at each node, ending 
at a leaf node. A leaf node is a group of features that are labelled as a particular class; the 
instance is then classified using the label assigned to that particular leaf (Kotsiantis, 
2007:251; Marsland, 2009:133; Thomassey & Fiordaliso, 2006:410). 
 
The decision tree may be considered as a set of if–then statements or rules. These rules 
are determined by the algorithm based on the training set (Samoil, 2015:35). Table 3 
provides an example of a training set to which a decision tree can be applied. 
 
Table 3: An example of a decision tree training set  
Features Class 
Ft1 Ft2 Ft3 Ft4 Label 
a1 a2 a3 a4 Yes 
a1 a2 a3 b4 Yes 
a1 b2 a3 a4 Yes 
a1 b2 b3 b4 No 
a1 c2 a3 a4 Yes 
a1 c2 a3 b4 No 
b1 b2 b3 b4 No 
c1 b2 b3 b4 No 
(Adapted from Kotsiantis, 2007:251) 
 
Figure 9 presents an example of a decision tree for the training set example in Table 3 
above. In the decision tree structure, the features are considered one at a time, followed by 
the assignment of a class or the consideration of another feature. 
 
Stellenbosch University  https://scholar.sun.ac.za
 33 
 
 
Figure 9: An example of a decision tree (adapted from Kotsiantis, 2007:251)  
 
As can be seen from Figure 9 above, the classification consists of a number of decisions 
which occur at each node, ending at the leaf node. The leaf node does not require a decision 
but rather assigns the instance to the particular class label (Narasimha Murty & Susheela 
Devi, 2011:127). A more advanced decision tree is the C4.5 algorithm, which is described 
below. 
 
3.5.1.2 Logic-based: C4.5 decision trees 
In producing the ID3 decision tree algorithm, the basic decision tree algorithm is adapted to 
ensure that the correct features are selected at each stage of the tree. The ID3 algorithm 
ranks features in such a way that the more informative features are closer to the root. This 
is measured using “entropy”, a term used to describe how informative a feature is 
(Thomassey & Fiordaliso, 2006:410). 
 
The ID3 algorithm is further adapted to the C4.5 decision tree algorithm by pruning it to 
reduce the number of nodes without losing the ability to classify the instance. There are two 
types of pruning, prepruning and post pruning. Prepruning tries to determine when to stop 
building branches by assessing at which point enough features have been considered to 
reasonably classify the case without requiring further branches.  
 
Post pruning takes place after an entire tree has been built, thus branches are removed at 
the end. C4.5 uses post pruning, because it takes an ID3 tree, converts it into a set of if–
Ft1 
Ft2 
c1 a1 b1 
Root node 
Features 
Leaf 
nodes 
Leaf nodes 
Features 
Features 
Leaf 
node Ft3 
a2 b2 c2 
Ft4 Yes 
No No 
Yes No 
a3 b3 a3 b3 
Yes No 
Stellenbosch University  https://scholar.sun.ac.za
 34 
 
then rules and subsequently prunes certain conditions if the accuracy of the rules is 
increased without them (Marsland, 2009:143; Thomassey & Fiordaliso, 2006:410).  
 
3.5.1.3 Logic-based: random forests  
This model consists of a number of decision trees, each composed of a subsample of 
features, and is usually weaker than a full decision tree. The average, or the weighted 
average, of the trees is determined and used to perform the classification, effectively 
combining the power of the individual trees which often produces a higher quality result 
(Bucheli & Thompson, 2014:4; Dataiku, 2017:7). 
 
3.5.1.4 Logic-based: transfer learning decision forests 
This model uses random forests, as described above, where the knowledge produced can 
subsequently be applied or transferred to a given target task. This generates a classifier that 
can be used to exploit the knowledge from other tasks to improve the ability of the classifier 
to perform a target task (Goussies, Ubalde, Fernandez & Mejail, 2014:4312). 
 
3.5.1.5 Perceptron-based: neural networks 
A neural network can provide prediction and classification solutions and is discussed in 
section 3.5.3.2. 
 
3.5.1.6 Statistical: Naïve Bayes 
The Naïve Bayes algorithm is a probabilistic model which determines the probability of 
different classes or outcomes, based on previously encountered examples. These examples 
are identified in the training data. For an instance or event that has been classified, the 
algorithm calculates the probability of each identified feature present in the instance and 
these probabilities are multiplied with each other for all possible classes or outcomes. The 
class or outcome with the highest probability is chosen, being the most likely outcome 
(Larsson & Segerås, 2016:12).  
 
The Naïve Bayes algorithm is derived from the Bayesian theorem and assumes that the 
features in the instance are independent, which implies that the value of the features do not 
influence one another. When considering multiple features, the Naïve Bayes algorithm is a 
more simplified algorithm than the Bayesian theorem (Marsland, 2009:171). 
 
Stellenbosch University  https://scholar.sun.ac.za
 35 
 
The Bayesian theorem is used to calculate the probability of an event based on previous 
knowledge of the probability of an event. The algorithm determines the probability of the 
occurrence of, for example, event A, when having observed event B, taking into account the 
likelihood of event B being present when event A is observed, as well as the probabilities of 
event A and event B as follows: 
𝑃𝑃(𝐴𝐴|𝐵𝐵) = 𝑃𝑃(𝐵𝐵|𝐴𝐴)𝑃𝑃(𝐴𝐴)
𝑃𝑃(𝐵𝐵)  
 
The variables in the above algorithm are as follows: 
• P(A|B) is the probability of occurrence of event A, given that event B (evidence) 
occurred 
• P(A) is the prior probability of event A 
• P(B|A) the conditional probability or likelihood of the occurrence of event B, given 
event A 
• P(B) is the prior probability of the occurrence of event B (Chadha & Singh, 2012:52; 
Samoil, 2015:16).  
 
This algorithm can be used in a normal classification problem where there are multiple 
classes, say C1, C2, …, Ck. Naïve Bayes calculates the conditional probability, that is, the 
probability of a feature conditional on the observed features of an object with a set of multiple 
features (observed features) such as x1, x2, …, xn belonging to a particular class Ci.  
 
The algorithm calculates the probability of the class, given the observed features, by 
multiplying the probability of the features by the probability of the class, using the Bayes 
theorem as follows (Narasimha Murty & Susheela Devi, 2011:93): 
 
𝑃𝑃(𝐶𝐶𝑖𝑖|𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛) = 𝑃𝑃(𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛|𝐶𝐶𝑖𝑖) ∗ 𝑃𝑃(𝐶𝐶𝑖𝑖)𝑃𝑃(𝑥𝑥1, 𝑥𝑥2, … , 𝑥𝑥𝑛𝑛)  
 
To illustrate how this would be used to classify something based on its features, the example 
of classifying vegetables adapted from Larsson and Segerås (2016:12) is provided. Table 4 
contains the training data followed by an example of the observed features and how they 
are used to classify the vegetables using Naïve Bayes. 
 
  
Stellenbosch University  https://scholar.sun.ac.za
 36 
 
Table 4: Naïve Bayes training data  
Vegetable class Long Not Long Purple Not Purple Total 
Tomato 5 24 3 26 29 
Brinjal 14 4 16 2 18 
Total 19 28 19 28 47 
Source: Adapted from Larsson & Segerås (2016:12)  
 
To classify an unknown vegetable having the observed features or evidence Long and Not 
Purple, the probability of each respective vegetable given the features will need to be 
calculated, as shown in formula (1) and formula (2). 
 
 𝑃𝑃(𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇|𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿 𝑇𝑇𝐿𝐿𝑎𝑎 𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃) = 𝑃𝑃�𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿�𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇�∗𝑃𝑃�𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃�𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇�∗𝑃𝑃(𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇)
𝑃𝑃(𝐿𝐿𝑇𝑇𝑛𝑛𝐿𝐿)∗𝑃𝑃(𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃)   (1) 
𝑃𝑃(𝐵𝐵𝑃𝑃𝐵𝐵𝐿𝐿𝐵𝐵𝑇𝑇𝑃𝑃|𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿 𝑇𝑇𝐿𝐿𝑎𝑎 𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃) = 𝑃𝑃�𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿�𝐵𝐵𝑃𝑃𝐵𝐵𝐿𝐿𝐵𝐵𝑇𝑇𝑃𝑃�∗𝑃𝑃�𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃�𝐵𝐵𝑃𝑃𝐵𝐵𝐿𝐿𝐵𝐵𝑇𝑇𝑃𝑃�∗𝑃𝑃(𝐵𝐵𝑃𝑃𝑖𝑖𝑛𝑛𝐵𝐵𝑇𝑇𝑃𝑃)
𝑃𝑃(𝐿𝐿𝑇𝑇𝑛𝑛𝐿𝐿)∗𝑃𝑃(𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃)   (2) 
 
The probabilities calculated in formula (1) and formula (2) will be compared, and the highest 
probability will be selected as the chosen vegetable. 
 
𝑃𝑃(𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇𝑇|𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿 𝑇𝑇𝐿𝐿𝑎𝑎 𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃) = 5 29� ∗26 29� ∗29 47�19
47� ∗
28
47�
= 0.0954
0.2408 = 0.3961 (1) 
 
𝑃𝑃(𝐵𝐵𝑃𝑃𝐵𝐵𝐿𝐿𝐵𝐵𝑇𝑇𝑃𝑃|𝐿𝐿𝑇𝑇𝐿𝐿𝐿𝐿 𝑇𝑇𝐿𝐿𝑎𝑎 𝑁𝑁𝑇𝑇𝑇𝑇 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃) = 14 18� ∗2 18� ∗18 47�19
47� ∗
28
47�
= 0.0331
0.2408 = 0.1374 (2) 
 
Therefore, a long and not purple vegetable will most likely be a tomato based on the 
probability of the prior observed evidence using Bayes theorem and assuming that the 
features are independent. 
 
3.5.1.7 Statistical: Bayesian belief networks (BBN) 
Like the Naïve Bayes algorithm, a Bayesian belief network is modelled on the Bayesian 
theorem. However, in contrast to the Naïve Bayes algorithm which assumes that features 
are independent, a Bayesian belief network takes into consideration the probabilistic 
dependencies among features (Heckerman, 2008:33; Narasimha Murty & Susheela Devi, 
2011:97). 
 
Stellenbosch University  https://scholar.sun.ac.za
 37 
 
Bayesian belief networks plot these probabilistic relationships using a graphical model. The 
graph presents a network of nodes, one for each feature, which are connected by lines going 
in a specific direction. The one feature needs to be present for the possibility of the other, 
therefore they are causally linked and the graph indicates this parent–child relationship. This 
graph is known as a directed acyclic graph (Witten, Frank, Hall & Pal, 2016:340) and is 
illustrated in Figure 10.  
 
 
 
Figure 10: A Bayesian belief network showing causal relationships between events 
(adapted from Heckerman, 2008:45)  
 
The model takes these dependencies into consideration by determining the joint probability 
of causal features. This is done by using the product rule, which states that the probability 
of both event A and event B is the probability of event A, given B multiplied by the probability 
of B (Witten et al., 2016:337). 
 
∴ 𝑃𝑃(𝐴𝐴,𝐵𝐵) = 𝑃𝑃(𝐴𝐴|𝐵𝐵) ∗ 𝑃𝑃(𝐵𝐵) 
 
By using this product rule and taking into account the causal relationship between features, 
the joint probability distribution for a set of features can be determined. The Bayesian belief 
network is able to identify conditional independencies; for example, D is conditionally 
independent of E in Figure 10. Therefore, the conditional probability of event D, given events 
A, B and E, will be as follows: 
 
𝑃𝑃(𝐷𝐷|𝐴𝐴,𝐵𝐵,𝐸𝐸) = 𝑃𝑃(𝐷𝐷|𝐴𝐴,𝐵𝐵). 
 
The conditional probability for each node X will be calculated as the joint probability of the 
parent events, that is, P(X|Parent events (X)), therefore excluding conditional 
independencies (Heckerman, 2008:45; Witten et al., 2016:343).  
 
A 
D E 
B C 
Stellenbosch University  https://scholar.sun.ac.za
 38 
 
Having established a Bayesian belief network, the probabilities of causal events can be 
determined or inferred using the probabilities of the observed events, taking into account 
only the joint probabilities and excluding the conditional independencies (Heckerman, 
2008:46).  
 
3.5.1.8 Statistical: K-nearest neighbour (kNN) 
The nearest neighbour algorithm classifies instances or patterns according to the nearest 
known neighbour class by finding similarities in the instance being classified to patterns or 
features in the training set (Narasimha Murty & Susheela Devi, 2011:48). The k-nearest 
neighbour algorithm is derived from this nearest neighbour algorithm. 
 
For the k-nearest neighbour algorithm, instead of only one nearest neighbour being detected 
by the algorithm, more than one to the amount of k nearest neighbours may be detected. 
The nearest neighbours detected may then indicate that the instance being classified is near 
to more than one class. The class for which the majority of nearest neighbours are identified 
will then be the class to which the instance is classified.  
 
Figure 11 illustrates a linear example of a k-nearest neighbour classification. Where k is 3, 
three of the nearest neighbours will be identified. Instance P can therefore be correctly 
classified as belonging to the circles class, as the majority of the nearest neighbours are 
circles. If the basic nearest neighbour algorithm was used, then P would have been 
incorrectly classified in the class of the stars, even though point number 5 is an outlier 
(Narasimha Murty & Susheela Devi, 2011:51). 
 
 
Figure 11: Accurate classification of P using the k-nearest neighbour algorithm 
(adapted from Narasimha Murty & Susheela Devi, 2011) 
 
P 
5 
1 
2 
3 
4 
6 
7 
9 
8 10 
Stellenbosch University  https://scholar.sun.ac.za
 39 
 
In the design of the k-nearest neighbour algorithm, an important factor is therefore the value 
of k.  This will need to be an odd value in order to avoid an even split between two classes. 
The k-nearest neighbour algorithm may also be modified to assign a weight to each of the 
nearest neighbours based on their proximity to the instance being classified (Narasimha 
Murty & Susheela Devi, 2011:51). 
 
3.5.1.9 Statistical: support vector machines 
A support vector machine can provide prediction and classification solutions and is 
discussed in section 3.5.3.1. 
 
3.5.2 Prediction algorithms 
3.5.2.1 Conditional random fields 
Where normal classifiers predict only one class at a time, conditional random fields use a 
graphical model to plot many interdependent variables, thus determining the conditional 
distribution for multiple predictions (Sutton & Mccallum, 2007:98; Witten et al., 2016:407). 
According to Sutton and Mccallum (2011:269), conditional random fields are able to predict 
outputs by combining discriminative classification with graphical modelling. 
 
As opposed to a generative model, which determines a joint probability distribution, 
conditional random fields use a discriminative model which calculates only the conditional 
distribution. A conditional distribution 𝑃𝑃(𝑦𝑦|𝑥𝑥) where the probability of outputs 𝑦𝑦 is calculated 
given specific inputs 𝑥𝑥, does not consider the modelling of the probability of the input 𝑃𝑃(𝑥𝑥) 
that would consider all the features dependent on 𝑥𝑥. Discriminative models are therefore 
simpler models than generative models (Sutton & Mccallum, 2011:269).  
 
Conditional random fields can determine the probability of possible label sequences which 
are interdependent given an observation sequence, as recommended by Lafferty, Mccallum 
and Pereira (2001:282) for segmenting and labelling sequence data, thus taking context into 
account when predicting the outputs. This context provides information that contributes to 
predicting the outputs (Witten et al., 2016:406). 
 
For this reason conditional random fields are useful for modelling multifaceted outputs 
consisting of interrelated parts, for example for identifying the parts of a sentence (Sutton & 
Mccallum, 2007:106) or image captioning where the components of the sentence or picture 
Stellenbosch University  https://scholar.sun.ac.za
 40 
 
are related and are indicative of what is being classified (Arnab, Zheng, Jayasumana, 
Romera-Paredes, Larsson, Kirillov, et al., 2018:38). 
 
3.5.3 Dual-use algorithms: classification and prediction 
3.5.3.1 Support vector machines 
A support vector machine is a binary classifier that aims to separate data into two classes, 
based on the case features. A set of features that describe one case is known as a vector. 
In order for a support vector machine to perform at its best, the optimal hyper plane needs 
to be identified which separates the two classes of vectors.  
 
The optimal hyper plane would be one which has a maximum margin from each of the 
classes, where this margin is the distance from the hyper plane to the closest vectors in 
each class. This maximum margin is determined by considering the vectors closest to the 
hyper plane in each class. The closest vectors are known as the support vectors. 
 
The support vector machine firstly maps the vectors into a multidimensional (N-dimensional) 
space and then determines the hyperplane which separates the vectors into two classes. 
New instances can then be classified into these two classes (Ayodele, 2010b:25). 
 
Although N-dimensions are used to map vectors, for illustration purposes Ayodele 
(2010b:26) recommends using a two-dimensional example as shown in the Figure 12. In 
the example there are two features for each vector, one represented by the X1 axis and one 
represented by the X2 axis. The vectors are plotted accordingly and each class is 
represented by a shape, resulting in two classes, circles and squares. The maximum margin 
is used to determine the hyperplane, which is at the optimal distance from the support 
vectors.  
 
Stellenbosch University  https://scholar.sun.ac.za
 41 
 
 
Figure 12: A two-dimensional example of a support vector machine (adapted from 
Ayodele, 2010b:26; Kotsiantis, 2007:261) 
 
Despite the linear hyperplane shown in the two-dimensional figure, support vector machines 
usually create non-linear class boundaries, which increases the possibilities for which the 
support vector machines can be used (Witten et al., 2016:252).  
 
3.5.3.2 Artificial neural networks 
An artificial neural network is a combination of mathematically generated neurons, which 
operate in a similar manner to the human brain. These neurons are each assigned a weight 
based on what the artificial neural network learns, collectively forming part of a mathematical 
function (SMACC, 2017:9).  
 
Narasimha Murty and Susheela Devi (2011:169) describe the functioning of a neuron as 
follows: The neuron receives input from a combination of other neurons, for which each input 
carries a specific weight. The total inputs received are summed by adding all the input values 
adjusted for their respective weights. If the cumulative input exceeds a threshold, the neuron 
will produce an output. Figure 13 illustrates an artificial neuron, where the output value will 
be calculated as follows:  𝑇𝑇 = 𝑥𝑥1𝑤𝑤1 + 𝑥𝑥2𝑤𝑤2 + 𝑥𝑥3𝑤𝑤3. 
 
 
Figure 13: A single neuron (adapted from Narasimha Murty & Susheela Devi, 2011:169)  
Support Vector 
Optimal hyper plane 
Support Vector 
Maximum margin 
Output = 
f(a) 
Output 
Input x1 
Input x2 
Input x3 
Stellenbosch University  https://scholar.sun.ac.za
 42 
 
According to Narasimha Murty and Susheela Devi (2011:169), the training of a neural 
network takes place as follows: 
• Random weights are assigned to every link or neuron in the network. 
• Inputs are given to the artificial neural network for which an output is produced. 
• If the output is correct, then no changes are made to neuron weights. 
• If the output is incorrect, the error is used to adjust the weights in the network and 
the process repeated until the correct output is produced for those inputs. 
 
The structure of the neural network, and therefore the arrangement of the neurons, will 
depend on the learning task and the data available (SMACC, 2017:9). A network consisting 
of an input and an output layer will be able to classify linearly separable classes – this is 
known as a feed-forward network.  
 
For more complex classification, a multilayer network will be required. In addition to an input 
and an output layer, these networks have a hidden layer which enables non-linear 
classification. The structure of this network is illustrated in Figure 14 (Narasimha Murty & 
Susheela Devi, 2011:174). 
 
Figure 14: A multilayer neural network with a hidden layer (adapted from Narasimha 
Murty & Susheela Devi, 2011:174) 
 
The artificial neural network can be used to extract information from unfamiliar documents, 
using its knowledge to determine the probability of the identified information being similar to 
known information. OCR is used to extract the information and the characteristics of the 
information are used to determine what the information is. SMACC (2017:10) provides an 
example of how the artificial neural network extracts information from an invoice, as shown 
in Figure 15. 
 
Input layer 
Hidden layer 
Output layer 
Stellenbosch University  https://scholar.sun.ac.za
 43 
 
 
Figure 15: An artificial network classifying elements of an invoice (adapted from 
SMACC, 2017:10)  
 
Neural networks can be trained using supervised learning, since both the inputs and outputs 
are supplied to the neural network. However, neural networks can also be designed using 
unsupervised learning, which would require self-organisation, as is seen in self-organising 
maps (Hadzic, Dillon & Tan, 2007:225; Kohonen, 1990:1464). 
 
3.5.3.3 Convolutional neural networks 
Convolutional neural networks are explained by Albawi, Mohammed and Al-Zawi (2017:1) 
as a particular type of deep neural network consisting of multiple layers. These layers are 
structured in such a way that the network is able to handle complex data such as images, 
and to classify these images based on the combined features identified by the different 
layers of the network. The data input passes through layers that include convolutional, 
pooling and fully connected layers, resulting in an output.  
 
When classifying an image, a convolutional neural network considers each pixel as an input. 
Unlike in other neural networks, the neurons in the convolutional layer do not connect to all 
the inputs (pixels), they only obtain input from regions in the picture. This is done by breaking 
the image down into smaller pieces, consisting of sets of pixels, and then systematically 
connecting the neurons to these smaller pieces. Therefore the same set of neurons is re-
used to detect portions of the picture piece by piece (Albawi et al., 2017:2; Witten et al., 
2016:438). 
 
The convolutional layer passes each piece of the image through a set of filters where each 
filter looks for different aspects in the image. Different filters detect different features of an 
Invoice example 
 
The invoice company 
Fictitious Street 
Stellenbosch 
19582 
 
Further invoices details XXX 
 
The number 19582 appears on the invoice, what does this 
number mean? 
 
A word above the number: Stellenbosch 19582 
So the word provides context: based on probability it is a 
postal code 
 
Line above: Fictitious Street 
This may be an address, which increases the probability of 
19582 being a postal code 
 
The address is also located below a company name, which 
further increases the probability of it being a postal code. 
Stellenbosch University  https://scholar.sun.ac.za
 44 
 
image such as edges and shapes or higher-level features such as faces. Each filter needs 
to be trained to effectively perform its given task, thus these filters are prepared by means 
of supervised learning. 
 
The elements detected in the convolutional layer are then reduced in the pooling layer by 
means of a threshold. As each of the filters are neurons, each neuron will produce a 
weighted output. These outputs will also be analysed systematically by dividing outputs into 
sub-regions. The features in each sub-region with the maximum weighted output will be 
selected by the pooling layer – this is called maximum pooling. In this way the pooling layer 
summarises the outputs of the convolutional layer and makes the features detected more 
robust (Albawi et al., 2017:5; Krizhevsky, Sutskever & Hinton, 2012:4) . 
 
There may be multiple layers of convolution and pooling layers, which each filter and 
measure different aspects such as edges, curves, faces and hands and so on. All successful 
detections are then combined by the fully connected layer, which uses logic to identify the 
image being classified. It is important to note that features in the image cannot be spatially 
dependent. So, for example, in a face detection application, it should not matter where the 
faces are located in the images, the focus is simply on detecting the faces (Albawi et al., 
2017:1). 
 
Figure 16 illustrates the functioning of a convolutional neural network, where the 
convolutional layer interprets sections of the image at a time, followed by a reduction of the 
detections in the pooling layer and then finally a classification of the findings by the fully 
connected layer (Krizhevsky et al., 2012:5; Lawrence, Giles, Tsoi & Back, 1997:103) 
 
Stellenbosch University  https://scholar.sun.ac.za
 45 
 
 
Figure 16: A high-level diagram of the convolutional neural network used for image 
classification (adapted from Krizhevsky et al., 2012:5; Lawrence et al., 1997:103)  
 
Oquab, Bottou, Laptev and Sivic (2014:1717) indicate that convolutional neural networks 
used to classify images can also be used in combination with other technologies to 
determine document types, when sorting documents by their appearance or patterns.  
 
3.6 Description of unsupervised learning techniques 
Having described the various supervised learning techniques in section 3.5, this section 
describes the relevant unsupervised learning techniques. 
 
3.6.1 Pattern detection 
3.6.1.1 Association rules 
Association rules determine the associative relationships between data, where the 
occurrence of one feature may indicate the possible occurrence of another feature 
(Narasimha Murty & Susheela Devi, 2011:55). Instead of predicting a particular class, 
association rules are able to predict combinations of features and which features are 
commonly associated with each other, irrespective of class (Witten et al., 2016:79). 
 
Association rules need to be measured in order to determine whether they can be relied 
upon, and to do this the coverage of the association rule is considered. This is known as the 
support and thus the accuracy of the association rule is determined, which is called the 
confidence (Witten et al., 2016:79). 
 
Convolutional  
layer: 
Image  
sampling 
Pooling  
layer: 
Dimensionality  
reduction 
Fully  
connected  
layer: 
Classification 
Stellenbosch University  https://scholar.sun.ac.za
 46 
 
Berka and Rauch (2010:11) provide an example of how the support and confidence can be 
determined. In the example, 𝑇𝑇 is the number of classes where both 𝑋𝑋 and 𝑌𝑌 are present, 
𝑏𝑏 are the classes where only 𝑋𝑋 is present, similarly 𝑐𝑐 where only 𝑌𝑌 is present and 𝑎𝑎 where 
neither classes have 𝑋𝑋 or 𝑌𝑌.  
 
The support is determined as follows: 
𝑃𝑃(𝑋𝑋 ∧ 𝑌𝑌) = 𝑇𝑇
𝑇𝑇 + 𝑏𝑏 + 𝑐𝑐 + 𝑎𝑎 
The confidence is determined as 
𝑃𝑃(𝑋𝑋|𝑌𝑌) = 𝑇𝑇
𝑇𝑇 + 𝑏𝑏 
Usually the support and confidence for the association rule will need to exceed a specified 
minimum threshold for the rule to be considered for pattern detection. Since some 
associations may be indicative of others, often only the strongest rules are selected in order 
to reduce the number of rules (Witten et al., 2016:79). 
 
3.6.2 Clustering 
Clustering is an unsupervised machine learning method which divides instances into groups 
or clusters. The following different groups may be identified:  
 
• Exclusive – each instance belongs to only one cluster. 
• Non-exclusive – one instance may belong to more than one cluster. 
• Probabilistic or fuzzy – there is a certain probability or degree of membership of 
each cluster to which an instance belongs.  
• Hierarchical – instances are divided into high-level broader clusters, each of which 
are refined into smaller subclusters up to individual instances level (Thomassey & 
Fiordaliso, 2006:411; Witten et al., 2016:88). 
 
Clustering is often the first stage of a hybrid approach, which consists of more than one 
machine learning technique. The next stage may be a decision tree or rule set which is 
derived based on the features of the determined clusters, and this rule set then allocates 
new instances to the appropriate clusters (Thomassey & Fiordaliso, 2006:413; Witten et al., 
2016:88). 
 
Stellenbosch University  https://scholar.sun.ac.za
 47 
 
3.6.2.1 Self-organising maps 
Self-organising maps are a form of neural network that uses unsupervised learning. The 
objective of the self-organising map is to produce its own representation or self-organisation 
of the given data, since outputs are not provided (Hadzic et al., 2007:225; Kohonen, 
1990:1464).  
 
According to Ayodele (2010b:37), the self-organising map aims to learn the structure of the 
data by identifying clusters of data and linking similar clusters to each other. This results in 
feature mapping, where neurons representing similar features are located close to each 
other in a network.  
 
In order to form the network, the neurons of self-organising maps are typically organised 
into a two-dimensional grid, with connections between the neurons in the grid (Ayodele, 
2010b:37; Marsland, 2009:208). This forms the output map. The inputs form another layer, 
as is the case with neural networks. Each node in the input layer is fully connected to the 
output map neurons (Ehsani, Quiel & Malekian, 2010:411). This is illustrated in Figure 17. 
 
Figure 17: The self-organising map network (adapted from Hadzic et al., 2007:228)  
 
The objective the self-organising map is to plot input patterns onto a self-organising map. 
Warwick (2012:97) describes the self-organising map model as follows: 
 
1. Self-organising maps start with an untrained map, which consists of any number of 
neutrons arranged in a grid. This grid usually represents two dimensions, even 
though the input patterns are usually more highly dimensional than 2D, so a self-
organising map enables dimensionality reduction (Kohonen, 1998:1; Marsland, 
2009:208). 
 
Input Layer Output map layer 
Stellenbosch University  https://scholar.sun.ac.za
 48 
 
2. Between the neurons in the network are lateral connections, which means that the 
output from each neuron forms further inputs to each of the other neurons in turn. 
This is in contrast to artificial neural networks where connections only occur between 
the different layers of inputs and outputs and not within the same layer (Marsland, 
2009:208). 
 
3. Each of the signals from the inputs and from the surrounding neurons will have a 
weight attached to them, which may be randomised to begin with between 0 and 1. 
The location of each neuron in the map is important as neurons located close to each 
other need to respond to similar input patterns (Marsland, 2009:210).  
 
4. The self-organising map is organised using competitive learning, where the 
neurons compete to best represent the input data (Ayodele, 2010b:39; Hadzic et al., 
2007:228; Marsland, 2009:210). 
 
5. When a particular input pattern (feature combination) is presented to the algorithm, 
one of the neurons will present an output which is higher in weight than the other 
neurons (different features trigger different weights resulting in an answer for each 
neuron). The neuron with the highest response is considered the winning neuron. 
 
6. The weights of the winning neuron are adjusted so that its weight for that input 
pattern is higher than before. The weights of the neighbouring neurons are also 
adjusted upwards but to a lesser degree for that particular input pattern. The neurons 
therefore adapt to different input patterns, thus ensuring they will be able to recognise 
and classify these input patterns (Kohonen, 2013:52). 
 
7. After various adaptations, neighbouring neurons are organised onto the two-
dimensional map, representing similar features as is required for feature mapping. 
This effectively creates clusters of similar neurons (Kohonen, 1998:1; Marsland, 
2009:208). 
 
8. Each time the self-organising map is presented with an input pattern a specific region 
of the neuron grid will respond. And since neurons are grouped according to their 
similarities for identifying a specific input pattern, by means of feature mapping, the 
Stellenbosch University  https://scholar.sun.ac.za
 49 
 
response of a particular region of the neurons will indicate the type of input pattern 
being presented, thus enabling classification or identification of the input. 
 
3.6.2.2 K-means clustering  
K-means clustering divides the data into k number of categories. In order to perform k-
means clustering, the number of clusters, that is k, needs to be specified and a random initial 
central data point (centroid) needs to be selected for each cluster. The data is then grouped 
based on the distance of each data point from the initial centre.  
 
Once initial assignment of data to clusters and distances has been done, the mean distance 
from the central data point for each cluster can be calculated and the centre moved to the 
mean distance point.  
 
The algorithm runs again until the cluster centres no longer need to move (Ayodele, 
2010b:27; Marsland, 2009:196). Figure 18 represents an example of a K-means flow 
diagram (Ayodele, 2010b:28). 
 
 
 
 
Figure 18: An example of K-means follow diagram (adapted from Ayodele, 2010b:28)  
 
 
 
End 
Start 
Number of 
cluster K 
Centroid 
Distance of objects to 
centroids 
Grouping based on 
minimum distance 
No objects 
move groups 
Stellenbosch University  https://scholar.sun.ac.za
 50 
 
The steps in a K-means clustering algorithm are as follows: 
1. Choose a value for K. 
2. Determine the centre coordinate or centroid (Takaki, Petersen & Ericson, 2018) for 
each group. 
3. Calculate the distance of each data object from each centroid. 
4. Place data points in clusters based on closest centroid, look at minimum distance. 
5. Calculate the mean distance of cluster from the centroid. 
6. Move the centroids for each group to the mean distance point. 
7. Repeat grouping of clusters, calculation of mean distance, and movement of 
centroids until centroids no longer need to move. 
 
3.7 Description of the semi-supervised learning techniques 
Having described the various unsupervised learning techniques in section 3.6, this section 
describes the relevant semi-supervised learning techniques. 
 
3.7.1 Semi-supervised clustering 
As Jain, Jin and Chitta (2014:1) state, clustering algorithms are unsupervised machine 
learning algorithms that work to find a partition in the dataset. Semi-supervised clustering 
assists the algorithm to find a better quality partition by providing the algorithm with any prior 
knowledge about the data. The clustering algorithm is then guided by the prior knowledge 
to find the partition in the data.  
 
Prior knowledge refers to labelled data, as is required for supervised learning, whereas 
unsupervised learning algorithms function using unlabelled data (Zheng, Zhou, Deng & 
Yang, 2017:7447). The labelled data may indicate constraints such as which data must or 
cannot be clustered together, or prior knowledge may mean increasing or decreasing known 
distances between data points (Bezerra, Mattoso & Xexéo, 2006:88).  
 
Having described the different machine learning techniques, certain tasks which are 
considered problem areas to be addressed by machine learning are described in the next 
section, together with the applicable machine learning technique that can be applied to each 
of the identified learning problems. 
 
Stellenbosch University  https://scholar.sun.ac.za
 51 
 
3.8 Tasks that can be addressed by machine learning technology 
In order to achieve the objectives of this section the learning problem for each identified task 
was determined using the findings of prior research. This enabled the identification of the 
machine learning technique that could be applied. The process of selecting machine 
learning techniques to apply to identified problems has been proven to be significantly 
difficult, especially when matching the problems identified with techniques intended to solve 
them (Someren & Urbancic, 2006:365). 
 
The process of matching each problem to the technique to solve it involves firstly 
understanding the task and then defining the learning problem. Identifying the learning 
problem enables a developer to identify the information and algorithm required to address 
the problem (Saitta & Neri, 1998:137; Someren & Urbancic, 2006:366). This process of 
understanding the task and defining the learning problem is crucial as there are often many 
solutions available for addressing a learning problem (Someren & Urbancic, 2006:369). 
 
The different types of learning solutions that are available are defined as follows (Amani & 
Fadlalla, 2017:34): 
• Classification is suitable for mapping data into two or more categories, each with its 
own distinct attributes (Larsson & Segerås, 2016:11). 
• Clustering is suitable for separating data into classes or groups that are similar in 
some meaningful way (Larsson & Segerås, 2016:11). 
• Prediction is suitable for producing a forward-looking numerical prediction 
(forecasting) or non-numerical prediction (classification). 
• Outlier detection is suitable for finding the items or events that significantly deviate 
from the expected pattern or other data considered normal in the dataset. 
 
Having defined the different types of learning solution above, Table 5 sets out the different 
problem areas and then designates the learning solution type for each area. The problems 
are organised by task areas, as described in chapter 2. The key findings of existing machine 
learning techniques identified in research to address the specific learning problem areas are 
then provided.  
 
  
Stellenbosch University  https://scholar.sun.ac.za
 52 
 
Table 5: Accounting problem types and recommended machine learning techniques  
Description of the learning 
problem 
Solutions to 
the learning 
problem 
Machine learning 
techniques 
Source 
Translation of manual and electronic documents into accounting information 
Task 3. Document features extraction 
OCR requires adaptability, a 
characteristic which can be 
enabled by means of machine 
learning. 
Classification Transfer learning decision 
forests  
ABBYY 
Technologies, 
(n.d.); Goussies et 
al. 2014:4309) 
Task 4. Document type recognition and classification( 
Image classification can be 
used to detect the document 
type, which can be enhanced 
by means of machine learning. 
Classification • Convolutional neural 
networks 
• New document class: k-
nearest neighbour 
• Similar known 
documents: support 
vector machine 
ABBYY (2017); 
Oquab et al. 
(2014:1717); 
Sorio (2013:23); 
Sorio, Bartoli, 
Davanzo & 
Medvet 
(2010:187); 
Witten et al. 
(2016:523) 
Irregular document layout 
classification using NLP 
combined with machine 
learning to train the system to 
process flexible or irregular 
document layouts. 
Classification Convolutional neural 
networks 
ABBYY (2017); 
Chen, Wang, Fan, 
Sun, & Satoshi 
(2015:436) 
Text classification is used to 
classify text, using both 
statistical and semantic text 
analysis. 
Clustering • Parallelisation 
MapReduce k-nearest 
neighbour 
• Semi-supervised 
clustering 
ABBYY (2017); 
Du (2017:195); 
Zhang, Tang, & 
Yoshida, 
(2015:152) 
 
Task 6. Validation of document data 
Validation of document 
information can apply 
machine learning to determine 
Classification • Naïve Bayes 
• Support vector machine 
Larsson & 
Segerås 
(2016:33) 
Stellenbosch University  https://scholar.sun.ac.za
 53 
 
Description of the learning 
problem 
Solutions to 
the learning 
problem 
Machine learning 
techniques 
Source 
whether the extracted data 
from the document is correctly 
classified or not. 
Removing of duplicate 
entries and linking 
documents may be achieved 
by using approximate string 
matching, making use of 
machine learning for string 
classification. 
Classification  • Naïve Bayes 
• Decision trees 
• Support vector machine 
• Artificial neural network 
Amtrup,  
Thompson, Kilby 
& Macciola, 
(2015:24); 
Larsson & 
Segerås 
(2016:18); De 
Leone & Minnetti 
(2015:2); Samoil 
(2015:16) 
Reconciliation of financial information 
Task 3. Matching 
Matching records or record-
linkage has been performed 
using a variety of machine 
learning techniques.  
Classification • Naïve Bayes 
• Decision trees 
• Support vector machine 
• Artificial neural network 
Chew & Robinson 
(2012:328); 
Samoil (2015:16) 
Preparation of management accounts 
Task 3. Account allocation 
Account allocation may be 
performed by incorporating 
machine learning, which learns 
to predict the account allocation 
based on probability and can 
recommend which accounts to 
post to. 
 
Classification 
and 
clustering 
Classification: 
Naïve Bayes 
 
Clustering: 
K-means clustering 
Random forests 
 
Bengtsson & 
Jansson 
(2015:40); Brady 
Leider, Resnick,  
Natalia Alfonso & 
Bishai 
(2017:354); 
SMACC 
(2017:12); Takaki 
& Ericson 
(2018:1) 
Task 7. Report generation 
Stellenbosch University  https://scholar.sun.ac.za
 54 
 
Description of the learning 
problem 
Solutions to 
the learning 
problem 
Machine learning 
techniques 
Source 
Error detection in financial 
data and fraud detection can 
be performed by incorporating 
machine learning to identify 
irregularities in datasets. 
Classification
Outlier 
detection and 
clustering 
 
Classification:  
• Bayesian belief network 
and a decision table 
• Naïve Bayes hybrid 
model  
Outlier detection:  
• Association rules 
Clustering:  
• K-means clustering 
• Self-organising maps 
Ahmed, 
Mahmood & Islam 
(2016:283); Alpar 
& Winkelsträter 
(2014:2261); 
Hajek & 
Henriques 
(2017:146); 
Kokina & 
Davenport 
(2017:117) 
 
 
 
Task 8. Report descriptions 
Report descriptions may 
incorporate machine learning 
techniques in natural language 
generation technologies to 
enable a reasoning process to 
be applied to the reported data 
and thus produce required 
explanations in natural 
language. 
Prediction Conditional random fields Gardent & Perez-
Beltrachini 
(2017:15); 
Lafferty et al. 
(2001:283); 
Yseop (2017:7) 
Tasks pertaining to performance measurement 
Forecasting performance 
reports can be prepared using 
predictive analytics, which may 
employ machine learning 
algorithms. These predictive 
analytics can be used for 
forecasting the business’s 
financial performance. 
Prediction • Support vector machine 
• Artificial neural network 
• C4.5 decision trees 
• Bayesian belief network 
Appelbaum et al. 
(2017:36) 
Source: Own observation 
 
Stellenbosch University  https://scholar.sun.ac.za
 55 
 
Table 5 above demonstrates which accounting processes tasks can be performed or 
assisted by machine learning techniques. These techniques were described in sections 3.5, 
3.6 and 3.7 of this chapter. It is clear from this table that there may be more than one method 
available to address a specific task. It is important to note that in certain instances a 
combination of methods may be selected (Someren & Urbancic, 2006:380). 
 
3.9 Conclusion 
This chapter provided a context, framework and recommended architecture for machine 
learning technology, as well as a description of the functioning of various machine learning 
techniques that can be applied to the different accounting tasks.  
 
The functioning of the different machine learning techniques was described as this 
contributes to accounting users’ understanding of machine learning technologies and it 
supports users in understanding the limitations of such technologies. These limitations are 
discussed in chapter 4 and, it is hoped, will ultimately assist users in selecting machine 
learning technologies that are appropriate for their needs. 
 
The findings in this chapter were presented as a table, indicating the different tasks to which 
the various machine learning techniques can be applied and the accompanying machine 
learning techniques that are available to perform these tasks. The benefits of these 
techniques and the limitations to their use will be discussed in chapter 4 section 4.7. 
 
Chapter 4 also describes the risks, benefits and limitations of the use of machine learning 
technology in an accounting context. Subsequently, guidelines for implementing machine 
learning technology in an accounting context, taking into account these risks and benefits, 
are provided in chapter 5. 
  
Stellenbosch University  https://scholar.sun.ac.za
 56 
 
Chapter 4: Risks, benefits and limitations when implementing machine learning 
 
4.1 Introduction 
Chapter 3 placed the different machine learning techniques in context and discussed which 
of the techniques could be applied to the accounting tasks identified in chapter 2. In this 
chapter the risks, benefits and limitations of machine learning technology, as well as the 
benefits and limitations of the identified machine learning techniques, are discussed. 
 
This chapter identifies the risks, benefits and limitations in relation to machine learning in 
line with King IV (Institute of Directors of Southern Africa (IODSA), 2016:30), which states 
that while risks may be negative they also inherently present certain opportunities, which 
could support the business in achieving its objectives. The principles of King IV were 
therefore considered in preparing this chapter.  
 
This chapter is structured in terms of the stages of the software development life cycle and 
the data science life cycle, both of which were considered when evaluating the different 
risks, benefits and limitations of machine learning, as according to Sapp (2017:15), the data 
science life cycle often overlaps with machine learning. Furthermore, the stages of this 
combined life cycle in which the accounting user has to be involved were considered.  
 
The findings of this chapter, being a summary of the risks, benefits and limitations of 
machine learning and the life cycle stages in which the accounting users need to be involved, 
are addressed in chapter 5, where guidelines for implementing the machine learning 
techniques in an accounting context are provided.  
 
4.2 Machine learning technology risks pertaining to the accounting objectives 
Gillion (2017:8) states that in all businesses the objective of accounting processes is to 
produce high quality accounting information for decision-making. As part of identifying the 
risks in the planning phase of the life cycle, the objectives of the technology were considered 
within the context of the accounting process, that is, the accounting objectives.  
 
The framework selected for identifying the accounting objectives was the Conceptual 
Framework for Financial Reporting, as approved by the International Accounting Standards 
Board (2018:6). This framework describes the objectives of financial reporting by providing 
Stellenbosch University  https://scholar.sun.ac.za
 57 
 
a description of the qualitative characteristics of financial information produced by the 
accounting processes (International Accounting Standards Board, 2018:14).  
 
4.2.1 Qualitative characteristics for financial reporting 
In addition to providing a description of the fundamental qualitative characteristics, the 
framework describes the factors that enhance these characteristics. These qualitative 
characteristics and factors are described in this section and, for the purposes of this study, 
are designated the accounting objectives. The applicable risks, benefits and limitations of 
each objective when using machine learning technology to produce the financial information 
are identified in section 4.2.2. 
 
Objective 1: Relevance. Information needs to be relevant to the decisions users are 
making. Information influences decisions if it can be used to predict future outcomes or to 
confirm prior evaluations. 
 
Objective 2: Materiality. Information is material if ignoring it or misstating it could affect 
decisions. Materiality will be determined by the nature or the magnitude of the information 
and materiality will be unique to every business. 
 
Objective 3: Faithful representation. Information must represent the substance of the 
matter being presented and not just the form. To do this the information should be complete, 
neutral and free from error.  
 
Objective 4: Comparability. Accounting information needs to be comparable and enable 
users to identify the similarities and differences in information. Consistency helps to achieve 
this goal.  
 
Objective 5: Verifiability. The information needs to be able to be verified in some way, 
either by direct observation or by being able to recalculate the outputs using the known 
inputs and methods used. 
 
Objective 6: Timeliness. Information needs to be available to users in time to be able to 
make the required decisions.  
 
Stellenbosch University  https://scholar.sun.ac.za
 58 
 
Objective 7: Understandability. Information needs to be presented and classified clearly 
and concisely.  
 
Objective 8: Cost vs benefit. The framework also takes into account the cost constraint 
of useful financial information, as opposed to the benefits to the user. 
 
In the next section, each of the identified accounting objectives for the applicable risks, 
benefits and limitations of machine learning are identified. 
 
4.2.2 Machine learning risks and benefits per accounting objective 
The risks and limitations that affect the respective objectives are presented in Table 7 and 
are organised according to the accounting objectives; if a risk pertains to more than one 
objective, both objectives are noted. The risks  the applicable type of consideration has been 
indicated in the “Type of consideration” column. The identified risks and limitations are 
addressed using user considerations in chapter 5. 
 
Table 6: Machine learning risks mapped to accounting objectives 
Objective 
number 
Risk Source Type of 
consideration 
1 
(Relevance) 
The risk that irrelevant data is included in 
the dataset. This may be due to outliers, 
which are values which are far removed 
from other observations in the data. 
These increase the risk of misleading 
representations. 
Brownlee 
(2013:1) 
Data 
1 & 2 
(Relevance & 
Materiality) 
The risk that outliers are relevant to 
decision-makers and not identified as 
relevant by the machine learning model. 
This could impact decision-making. 
Brownlee 
(2013:1) 
Model 
2 & 3 
(Materiality & 
Faithful 
Representation) 
The limitation relating the fact that 
machine learning uses probability to 
identify patterns used to make 
predictions. This means there is always a 
margin of error in the predictions made by 
the machine learning model. 
 
Ayodele 
(2010a:2); 
Sainani 
(2014:841); 
Vihinen 
(2012:3) 
 
Model 
Stellenbosch University  https://scholar.sun.ac.za
 59 
 
Objective 
number 
Risk Source Type of 
consideration 
The risk that the predictive accuracy of 
the machine learning model is not 
applicable to the task, such as is the case 
when using the information for 
compliance tasks. 
Gillion 2017:7) 
2 & 3 
(Materiality & 
Faithful 
Representation) 
The risk that the machine learning 
algorithm is inaccurate owing to 
insufficient data for training. 
Burrell 
(2016:5) 
Training set 
3 
(Faithful 
Representation) 
The risk that important features are 
missing from the training data, resulting in 
not all relevant features being considered 
when executing solutions. 
 
Similarly, missing features in the input 
data may inhibit model performance. 
Amani & 
Fadlalla 
(2017:47); 
Barreno, 
Nelson, 
Joseph & 
Tygar 
(2010:126) 
Feature 
selection 
3 
(Faithful 
Representation) 
The risk that there are errors in the data 
used to train the machine learning 
algorithm, which may result in incorrect 
processing and outputs. 
 
Similarly, if the data integrity of inputs is 
not maintained it may have a negative 
impact on the model. 
Appelbaum et 
al. (2017:40); 
Gillion 
(2017:9); 
Sculley, Holt, 
Golovin, 
Davydov, 
Phillips, 
Ebner, 
Chaudhary, 
Young, et al.  
(2015:2500) 
Data 
 
3 
(Faithful 
Representation) 
The limitation relating to the fact that 
machine learning algorithms adopt bias 
to generalise the data, as well as the risk 
of machine learning algorithms adopting 
societal bias. This increases the risk of 
errors or misleading results. 
Dietterich & 
Kong (1995:2); 
Gillion 
(2017:7) 
 
Algorithm; 
Feature 
selection 
Stellenbosch University  https://scholar.sun.ac.za
 60 
 
Objective 
number 
Risk Source Type of 
consideration 
4 
(Comparability) 
The limitation relating to the fact that 
machine learning models struggle to 
transfer solutions from one learning 
problem to another, thus limiting 
consistency of information.  
The Royal 
Society 
(2017:30) 
Model 
4 
(Comparability) 
The limitation posed by changes in 
prediction behaviour owing to changes in 
feature weights when further features are 
taught to the machine learning algorithm. 
 
The risk that changes in algorithm 
behaviour cannot be monitored owing to 
complex or incorrect design. 
Sculley et al. 
(2015:2495) 
 
Model 
5 & 7 
(Verifiability & 
Understand-ability) 
The limitation relating to the fact that 
users are unable to understand how 
information is generated by the machine 
learning technology owing to the 
complexity of the algorithms, thus making 
information difficult to verify.  
 
This also highlights the limitation of 
interpretability, as the knowledge that 
machine learning uses or discovers in 
order to perform its tasks may not always 
be available to users. 
Ayodele 
(2010a:2); 
Sainani 
(2014:841); 
The Royal 
Society 
(2017:30) 
Model 
6 
(Timeliness) 
The risk of increased learning times for 
machine learning models as the size and 
complexity of the datasets increase.  
Ghanem 
(2012:161) 
Model 
7 
(Understandability) 
The risk that the users do not understand 
how the machine learning algorithm 
functions and processes information 
owing to a lack of technical skills. 
Burrell 
(2016:4) 
User 
 
 
8 
(Cost vs benefit) 
The risk of costs exceeding the financial 
benefits to the business, since machine 
learning requires advanced data 
Gillion 
(2017:9); Sapp 
(2017:13) 
Infrastructure 
Stellenbosch University  https://scholar.sun.ac.za
 61 
 
Objective 
number 
Risk Source Type of 
consideration 
integration tools and infrastructure, which 
may present significant costs to the 
business to acquire. 
 
 
Source: Own observation 
Having identified the relevant machine learning risks and limitations, the benefits of machine 
learning organised by accounting objective are presented in Table 8. Some benefits address 
more than one accounting objective, in which case both objectives are noted. 
 
Table 7: Machine learning benefits per accounting objective 
Objective 
number 
Benefits Source 
1 & 7 
(Relevance & 
Understand-
ability) 
Machine learning is able to provide the user with valuable 
information that they would otherwise not have had 
access to. The technology is also able to learn which 
information is relevant to users.  
Sapp (2017:12) 
2 & 3 
(Materiality & 
Faithful 
Representation) 
Machine learning models can be programmed to include 
error checks, thus reducing the possibility of omitting 
information or possible misstatements of information. 
Sorio (2013:51) 
2 & 3 
(Materiality & 
Faithful 
Representation) 
By automating accounting information processing, the risk 
of human error will be eliminated; this will increase the 
accuracy of information and reduce the risk that material 
information is misstated or incomplete. 
Aberdeen Group 
(2017:2) 
4 
(Comparability) 
Automation models such as machine learning can 
execute repeated tasks consistently, thus ensuring better 
comparability of the information produced by the tasks. 
Gillion (2017:6); 
Ventana Research 
(2016:7) 
6 
(Timeliness) 
Machine learning can process data more efficiently than 
previous data tools were able to, thus making useful 
information available faster and improving the business’s 
ability to respond to information. 
Aberdeen Group 
(2017:3); Sapp 
(2017:12)  
8 
(Cost vs 
benefit) 
Machine learning will reduce the number of manual tasks 
required in a process which will save users time. It also 
increases the efficiency of processing and supports better 
decision-making. This will leading to cost savings. 
Gorbunova & 
Bochkarev 
(2011:33) 
Source: Own observation 
Stellenbosch University  https://scholar.sun.ac.za
 62 
 
 
The above findings in relation to the risks, limitations and benefits of machine learning will 
be further expanded upon for other areas of the machine learning technology life cycle. The 
risks and limitations identified were mapped to the machine learning life cycle components. 
These components of the machine learning architecture and the specific risks to consider 
for each component are described in the next section. 
 
4.3 Technology governance of the machine learning life cycle 
When preparing this chapter, principle 12 of King IV (Institute of Directors of Southern Africa 
(IODSA), 2016:41) was considered in particular. This requires businesses to govern 
technology in a way that supports the business in achieving its objectives. Accordingly, this 
principle supports one of the stated objectives of this study, namely, to identify the steps to 
take when implementing machine learning technology so as to ensure alignment with the 
goals of the accounting process. 
 
The aim of information technology governance, as described by Alreemy, Chang, Walters 
and Wills (2016:907), is to ensure compatibility between the goals of the business and a 
satisfactory level of risk with the use of the emerging technologies. In order to achieve this 
aim, Alreemy et al. (2016:907) highlighted COBIT 5 as a framework which could be used to 
implement technology governance. 
 
The COBIT 5 framework assists businesses in achieving governance objectives and IT 
enterprise management. COBIT 5 (ISACA, 2012:19) expands on the traditional software 
development life cycle stages that need to be managed. The stages that COBIT 5 describes 
are plan, design, build or acquire and implement, use or operate, evaluate or monitor, and 
lastly, update or dispose.  
 
The technology life cycle stages together with the software development life cycle stages 
are illustrated in Table 6. Sapp (2017:17) states that a slightly adapted life cycle is required 
when developing machine learning to enable more of a focus on model evaluation and 
tuning, and therefore the traditional software development life cycle has been adapted. 
 
The combination of the two different life cycles was done to ensure that all possible areas 
of the machine learning technology were considered when identifying the different risks, 
benefits and limitations. As one of the stated objectives of this study is to explain the role of 
Stellenbosch University  https://scholar.sun.ac.za
 63 
 
the user when using the machine learning technology to address identified risks, Table 6 
also indicates which tasks of the data science life cycle accounting users need to be involved 
in, as recommended by Sapp (2017:16). 
 
Table 8: Technology life cycle user involvement  
Technology 
life cycle 
Data science life 
cycle stage 
Task 
User 
involved 
Related risks, 
benefits & 
limitations 
Plan 
1. Problem 
understanding 
Determine problem objective  
Section 4.2 
Define success criteria  
Assess constraints  Sections 4.4 
and 4.5 
2. Data 
understanding 
Assess available data  
Obtain data (access)  Section 4.10 
Explore data  
Section 4.4 
Design 
3. Data 
preparation 
Filter data  
Clean data  
Training & testing set selection  
4. Modelling 
Select algorithm  
Sections 4.4; 
4.6; 4.7 and 
4.9 
Build/acquire 
Build model  
5. Evaluation of 
results 
Select/train model  
Validate/test/tune model  
Use and 
Evaluate 
Explain model  Sections 4.7 
and 4.8 
6. Deployment 
Deploy model  
Monitor Monitor and maintain  Sections 4.4; 
4.5; 4.9 and 
4.10 
Update/dispo
se 
Terminate 
 
Source: Adapted from Sapp (2017:16)  
 
The planning and design phases require the identification of the accounting objectives and 
technology architecture (Suer, ITIL & Nolan, 2015:1). The risks relating to these objectives 
are described in section 4.2. The risks relating to the technology architecture are described 
in section 4.4.  
 
When training the machine learning model during the build/acquire phase, one of the key 
requirements is the capabilities of the infrastructure (Sapp, 2017:26). Therefore the risks 
relating to the business infrastructure are described in section 4.5 and the specific risks 
Stellenbosch University  https://scholar.sun.ac.za
 64 
 
when acquiring the machine learning technology are described in section 4.6. For the use 
and evaluate phase, the benefits and limitations of the different machine learning techniques 
are described in section 4.7 and the user-related risks are described in section 4.8.  
 
Security risks for the monitor and maintain phase are described in section 4.10 and further 
monitoring risks are described in sections 4.4 and 4.5. Maintenance risks are described in 
section 4.9 and finally, the update/dispose phase is addressed in section 4.10 as part of 
security risks. 
 
4.4 Machine learning architecture risks 
This section describes the risks pertaining to the identified components of the machine 
learning architecture that the user must consider. The different machine learning 
architecture components were described in chapter 3 section 3.4. For ease of reference, 
Figure 7 in chapter 3 section 3.4 is repeated as Figure 19.  
 
 
Figure 19: Machine learning architecture (copy of Figure 7) (adapted from Sapp, 
2017:20) 
 
The component specific risks for each area, as illustrated above, are provided in Table 9 
and are grouped by component. 
Area 3: Model engineering Area 4: Execution 
Area 5: Deployment 
Area 1: Data 
acquisition 
Area 2: Data processing 
(feature engineering) 
• ERP databases 
• Mainframe 
• Devices 
Data Ingestion 
Stream processing 
platform 
 
Batch data 
warehouse 
Pre-processing data 
↓ 
Feature selection 
↓ 
Training set 
↓ 
Testing set 
Machine learning 
algorithms 
Experimentation 
↓ 
Testing 
↓ 
Tuning 
↓ 
Execution Data storage 
Stellenbosch University  https://scholar.sun.ac.za
 65 
 
 
 
Table 9: Machine learning architecture risks 
Identified 
components 
Component specific risks Source 
Area 1: Data acquisition and Area 2: Pre-processing 
Data ingestion The risk that the input data source is unstable. Obtaining 
input from other systems may be convenient but if not 
monitored there may be unexpected changes in the 
quality of the input data over time. 
Sculley et al. 
(2015:2496) 
 
The risk that important features are missing from the 
input data.  
Amani & Fadlalla 
(2017:47) 
Area 2: Data processing (feature engineering) 
Pre-processing 
of data 
The risk of errors in the training data used to train the 
model. 
Breck, Polyzotis, 
Roy, Whang & 
Zinkevich (2018:1) 
The risk of errors in the input data remaining undetected, 
thus affecting the quality and the integrity of the data and 
the outputs. 
Appelbaum et al., 
(2017:40; Gillion 
(2017:9); Sculley 
et al. (2015:2500)  
Feature 
engineering 
(feature 
analysis and 
selection) 
The risk that input features that have little modelling 
benefit are included in the training data. These features 
may increase the sensitivity of the technology to 
changes in the inputs, even though they could be 
excluded with no disadvantages. This is known as 
overfitting. 
Hawkins (2004:1); 
Sculley et al. 
(2015:2496) 
 
 
The risk that features selected lead to discriminatory 
predictions or outcomes.  
Sapp (2017:13) 
 
The risk that important features are missing from the 
training data. 
Amani & Fadlalla 
(2017:47) 
Sample 
selection: 
training set 
The risk that the training set is not large enough. This 
may result in some necessary features not being 
represented in the training set or a possible class 
imbalance where the training set represents a large 
number of one class and very few of the other class. 
Japkowicz & 
Stephen 
(2002:435) 
 
 
Stellenbosch University  https://scholar.sun.ac.za
 66 
 
Identified 
components 
Component specific risks Source 
The risk of the unethical use of data for training or use 
that infringes on privacy when using confidential data to 
train machine learning technology. 
Gillion (2017:9) 
Sample 
selection: 
testing set 
Risk that real-world data is unavailable for testing 
purposes. 
Ahmed et al. 
(2016:278) 
Area 3: Model engineering 
Algorithm and 
model 
The risk that the algorithm has adopted inappropriate 
bias. This increases the risk of errors. Bias is however 
necessary in order to be able to generalise beyond the 
training data.  
Dietterich & Kong 
(1995:11) 
 
 
The risk that the algorithm type applied is unable to 
discover the required pattern. The model is therefore not 
appropriate for the identified learning problem. 
Someren & 
Urbancic 
(2006:371) 
The risk that changes in algorithm behaviour cannot be 
monitored owing to complex or incorrect design. 
Sculley et al. 
(2015:2494) 
The risk that the machine learning models are not 
achieving the desired business objectives, as these 
change, and therefore that the model is not adaptable to 
changing business needs. 
Amani & Fadlalla 
(2017:48); Gillion 
(2017:7); Sapp 
(201718) 
Area 4: Execution 
Experimentation 
(training) 
The risk that the company is unable to process training 
or experimentation owing to immense computer and 
storage requirements. 
Sapp (2017:6) 
 
Testing The risk that errors in the algorithm go undetected owing 
to users not being involved in testing. In the case of 
accounting tasks, it would be accounting users not being 
involved in the testing of the accounting machine 
learning models. 
Gillion (2017:10) 
 
The risk that incorrect assessment measures are used 
to determine the adequacy of the machine learning 
model. 
Amani & Fadlalla 
(2017:47) 
Tuning The risk of overfitting the algorithm, where a machine 
learning model is too closely linked to the actual training 
data from used to train it. 
Witten et al. 
(2016:286). 
Stellenbosch University  https://scholar.sun.ac.za
 67 
 
Identified 
components 
Component specific risks Source 
Execution 
 
 
 
The risk that processing power and hardware are not 
able to meet execution needs.  
Gillion (2017:9); 
Sapp (2017:26) 
The risk of overreliance on models and the tasks 
performed by machine learning.  
Gillion (2017:7) 
 
Area 5: Deployment 
Data storage The risk that the output may be used by the incorrect 
systems or unauthorised users, presenting a security 
risk. 
Sculley et al. 
(2015:2495) 
Interoperability of the machine learning technology with 
the business applications providing the inputs or 
applications using the outputs. 
Daecher & Schmid 
(2016:42) 
Source: Own observation 
 
While considering the risks relating to the machine learning architecture, risks pertaining to 
the company system that supports this architecture were also identified. These include data 
availability, interoperability, processing power and hardware requirement risks. Accordingly, 
the next section, which assesses business infrastructure risks, considers the machine 
learning business support system . 
 
4.5 Business infrastructure risks when building machine learning models 
This section describes the risks relating to the business infrastructure when building and 
integrating machine learning technology into the accounting processes. Two different ways 
were identified to integrate the technology into the accounting processes. Firstly, businesses 
could decide to develop machine learning solutions for specific accounting processes 
themselves (Gillion, 2017:9). Alternatively, the technology could be integrated into the 
enterprise accounting software. This is generally done by the software service provider and 
sometimes without the user realising it (Sapp, 2017:9). The specific risks pertaining to 
acquiring machine learning technology from a service provider are described in section 4.6. 
 
According to The Royal Society (2017:48), an environment which allows for the effective 
use of data will be crucial to enabling machine learning, as the technology requires large 
amounts of data to create the machine learning methods and to train the machine learning 
systems and such large amounts of data require complex infrastructure. An example of the 
Stellenbosch University  https://scholar.sun.ac.za
 68 
 
business infrastructure components required to support machine learning and data 
availability is given in Figure 20 (Sculley et al., 2015:2497). 
 
Figure 20: Machine learning support infrastructure (adapted from Sculley et al., 
2015:2497) 
 
Figure 20 illustrates some of the components that business infrastructure required to support 
machine learning should have. The business will also need to consider the risks related to 
such an infrastructure platform. The risks attached to each element of the infrastructure 
platform are discussed below. 
 
4.5.1 Configuration of machine learning architecture  
Configuration determines how data is selected (data collection), the features used (feature 
extraction), algorithm settings (machine learning code) and verification methods (data 
verification). There is a risk that configuration options have been incorrectly selected, leading 
to errors (Sculley et al., 2015:2499). Specific configuration errors relate to the risks identified 
for the different machine learning architecture components already described in section 4.4. 
 
4.5.2 Interoperability of analysis tools  
The risk that other data analytics platforms and the selected machine learning framework 
are not interoperable (Sapp, 2017:13). 
 
4.5.3 Serving architecture 
Machine learning algorithms require extensive computing power and data management 
capabilities, therefore the risks that may need to be considered regarding the serving 
architecture of the business are as follows: 
  
Configuration 
Data 
collection 
Feature extraction 
Data verification 
Machine 
learning 
code 
Process 
management 
tools 
Machine 
resource 
management 
Analysis tools Serving 
infrastructure 
Monitoring 
Stellenbosch University  https://scholar.sun.ac.za
 69 
 
 
• Data management capabilities 
The risk that the architecture is not able to supply the required large and varying 
amounts of data (Sapp, 2017:12). This in turn presents two more risks, namely, that 
the amount of data available is insufficient and that the data is not managed 
adequately to enable it to be used by the machine learning technology. 
 
• Computing power 
The risk that the architecture does not have adequate processing power (Sapp, 
2017:13). 
 
• Scalability 
The risk that the infrastructure is not scalable to accommodate the changing business 
needs, including the needs of the machine learning model (Corless, De Villiers, 
Garibaldi & Norton, 2018:6). 
 
• Flexibility 
The risk that the infrastructure is not flexible enough to accommodate the changing 
learning patterns and processing demands of the machine learning model (Sapp, 
2017:18). 
 
• High costs of modernising core systems 
The risk of disruption and high costs to the business resulting from the need to update 
or replace core systems to support machine learning technology, as these systems 
enable the underlying data and processes on which the machine learning technology 
relies (Buchholz, Jones & Krumkachev, 2016:49). 
 
4.5.4 Monitoring 
There is a risk that the infrastructure does not support the monitoring of the machine learning 
models (Sapp, 2017:18). 
 
4.6 Acquiring machine learning from a service provider 
Certain risks and benefits need to be considered when acquiring machine learning enabled 
software or obtaining machine learning tools or pre-trained models from a service provider. 
 
Stellenbosch University  https://scholar.sun.ac.za
 70 
 
Such risks may include the following: 
• The software purchased may not meet business-specific needs, especially where 
there are regulatory requirements for accounting (Gillion, 2017:9). 
• Software developers may not develop specialist accounting tasks if the market 
demand for such products is low (Gillion, 2017:9). 
• Off-the-shelf models may still require significant computing power, depending on the 
model (Sapp, 2017:34). 
• Professionals who purchase machine learning algorithms may not have the basic 
understanding of machine learning technology to use the technology effectively to 
achieve the business objectives (Sapp, 2017:13). 
• Outsourced models may be maliciously trained, for example the backdoored neural 
network described by Gu, Dolan-Gavitt and Garg (2017:1), which performs badly on 
specific attacker-chosen inputs, thus posing a security risk. 
 
Benefits may include: 
• The skills barrier to incorporating machine learning into a business presented by 
purchased software is significantly lower (Sapp, 2017:13). 
• Fewer integration challenges are experienced compared to the in-house 
development of machine learning technology (Sapp, 2017:35). 
• Implementation costs are lower, especially when considering infrastructure risks as 
described. 
 
4.7 The benefits and limitations of various machine learning techniques 
Specific benefits and limitations were identified for each of the machine learning techniques 
described in chapter 3. The findings are presented in Table 10.  
 
Table 10: Benefits and limitations of respective machine learning techniques 
Machine 
learning 
technique 
Benefits Limitations Research 
Decision 
trees 
(section 
3.5.1.1) 
• Easy to interpret 
 
• Bias towards a certain 
category  
• Overfitting  
Marsland 
(2009:133); 
Thomassey & 
Fiordaliso 
(2006:410); Samoil 
Stellenbosch University  https://scholar.sun.ac.za
 71 
 
Machine 
learning 
technique 
Benefits Limitations Research 
• Too simple a model for 
complex data 
(2015:35, 42); 
Dataiku (2017:7) 
C4.5 decision 
trees 
(section 
3.5.1.2) 
• Can process data with 
missing features 
• Can process continuous 
feature values by using 
thresholds to create 
value intervals 
• Pruning may lead to loss of 
accuracy 
Dataiku (2017:7) 
Samoil (2015:17) 
Random 
forests 
(section 
3.5.1.3) 
• Greater performance 
than individual decision 
trees 
• Quick to train 
• Difficult to interpret 
• Overfitting 
Dataiku (2017:7) 
Naïve Bayes  
(section 
3.5.1.6) 
• Performs well in multi 
class prediction 
• Where variables are 
independent it performs 
better than other 
classification models and 
less training data is 
required 
• The algorithm is scalable 
and can adapt additional 
features  
• The algorithm assumes 
that features are 
independent, while this is 
unlikely 
• If a feature has a category 
which was not observed in 
training dataset, then a 
zero probability will be 
assigned to that category, 
thus resulting in the 
algorithm not being able to 
make a prediction, known 
as zero frequency. 
Samoil (2015:10) 
 
Witten et al. 
(2016:99); Samoil 
(2015:16); Larsson & 
Segerås (2016:34) 
Bayes belief 
network 
(section 
3.5.1.7) 
• Can handle missing data 
• Provides knowledge 
about causal 
relationships between 
variables 
• Provides a method for 
avoiding data overfitting 
• Complex calculations are 
required to train the 
network, which are 
expensive and take time 
• Highly dependent on the 
prior knowledge used to 
assume prior probabilities 
Heckerman 
(2008:33); 
Niedermayer 
(2008:128) 
Stellenbosch University  https://scholar.sun.ac.za
 72 
 
Machine 
learning 
technique 
Benefits Limitations Research 
k-nearest 
neighbour 
(kNN) 
(section 
3.5.1.8) 
• Handles database noise 
or outliers better than a 
nearest neighbour 
algorithm by considering 
more than one item to 
determine a class  
• Poor interpretability 
• Complex which makes it 
slow 
Kotsiantis 
(2007:263); Witten et 
al. (2016:87, 136, 
141)  
Conditional 
random fields 
(section 
3.5.2.1) 
• Can make predictions 
despite haphazard 
complex features in the 
input sequence 
• These discriminative 
models are more suited 
than other techniques 
where there are 
overlapping features 
• Trade-off between large 
feature datasets, which 
are more accurate but 
require more memory to 
store and increased risk of 
overfitting  
Witten et al. (2016); 
Sutton & Mccallum 
(2007:282); Sutton & 
Mccallum (2007:293) 
Support 
vector 
machines 
(section 
3.5.3.1) 
• Reduced risk of 
overfitting 
• Data training is simple 
• In the case of large 
datasets, it is able to 
simplify the problem 
• Poor interpretability Kotsiantis 
(2007:263); 
Karamizadeh, 
Abdullah, Halimi, 
Shayan & 
Rajabi(2014:65); 
Witten et al. 
(2016:257) 
Artificial 
neural 
networks 
(section 
3.5.3.2) 
• Very effective at handling 
complex tasks 
• Poor interpretability 
• The process is extremely 
computing intensive and 
requires modern, powerful 
computers  
Kotsiantis, 
(2007:263); 
Dataiku (2017:7); 
SMACC (2017:9) 
 
Convolutional 
neural 
networks 
(section 
3.5.3.3) 
• Ideal for image 
recognition 
• Takes a long time to train 
• Poor interpretability 
Dataiku (2017:7) 
Stellenbosch University  https://scholar.sun.ac.za
 73 
 
Machine 
learning 
technique 
Benefits Limitations Research 
Association 
rules 
(section 
3.6.1.1) 
• Association rules can be 
assessed by the 
coverage and accuracy 
of the rule; assisting 
decision-making on 
which rules to use 
• The number of rules 
discovered may be 
excessive 
• The algorithm takes a long 
time to produce outputs 
Witten et al. 
(2016:79); 
Kaur (2014:2322) 
Self-
organising 
maps 
(section 
3.6.2.1) 
• Easy to understand • The neural networks 
process is extremely 
computing intensive and 
requires modern, powerful 
computers  
• Need a value for every 
dimension of the map so it 
is difficult to obtain 
adequate data 
Ayodele (2010b:46); 
SMACC (2017:9) 
K-means 
clustering 
(section 
3.6.2.2) 
• Simple and effective 
method of clustering. 
• Difficult to determine the 
correct number of clusters, 
therefore this algorithm will 
need to be run a number of 
times to get the correct 
number, taking lots of time 
Ayodele (2010b:30); 
Witten et al. 
(2016:144) 
Semi-
supervised 
clustering 
(section 
3.7.1) 
• More accurate than 
unsupervised clustering 
• The rate at which these 
algorithms function is 
highly dependent on the 
size of the available 
labelled data  
Zheng et al. (2017) 
Source: Own observation 
 
The benefits and limitations presented in Table 10 were used to identify how these machine 
learning techniques address the accounting objectives identified in section 4.2.1. Table 11 
presents the constraints and benefits of the different machine learning techniques and maps 
these to the different accounting objectives, based on the definition of these as provided in 
section 4.2.1. The table also indicates in which accounting tasks the different machine 
learning techniques can use, as identified in table in chapter 3 section 3.8. 
Stellenbosch University  https://scholar.sun.ac.za
 74 
 
 
Table 11: Benefits and limitations of machine learning techniques mapped to objectives 
Machine learning 
techniques 
Benefits 
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Limitations  
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Tasks in the accounting 
process  
(Chapter 3 section 3.8) 
Transfer learning 
decision forests and 
random forests 
• Greater 
performance than 
individual decision 
trees 
Materiality & 
faithful 
representation 
• Poor 
interpretability 
Verifiability & 
Understandability 
• Adaptability of OCR 
• Account allocation  
• Fast training Timeliness • Overfitting Relevance 
Support vector machine • Reduced risk of 
overfitting 
Relevance • Poor 
interpretability. 
Verifiability & 
Understandability 
• Image classification 
• Validation of document 
information 
• Forecasting performance 
• Removing of duplicate entries 
and linking documents 
• Matching records or record-
linkage 
• Data training is 
simple 
Understandability 
• Simplifies the 
problem 
Understandability 
Convolutional neural 
networks 
• Ideal for image 
recognition 
NA • Poor 
interpretability 
Verifiability & 
Understandability 
• Irregular document layout 
classification using NLP 
k-Nearest neighbour • Adaptable to outliers Faithful 
representation 
• Poor 
interpretability 
Verifiability & 
Understandability 
• Image classification 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 75 
 
Machine learning 
techniques 
Benefits 
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Limitations  
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Tasks in the accounting 
process  
(Chapter 3 section 3.8) 
• Complex which 
makes it slow 
Timeliness • Text classification 
Semi-supervised 
clustering 
• More accurate than 
unsupervised 
clustering 
Materiality & 
Faithful 
representation 
• Training rate may 
be slow 
Timeliness • Text classification 
Naïve Bayes • Multi-class 
prediction 
Comparability • Requires 
independent 
variables 
Faithful 
representation 
• Validation of document 
information 
• Removing of duplicate entries 
and linking documents 
• Matching records or record-
linkage 
• Account allocation  
• Error detection in financial 
data and fraud detection 
• Excellent classifier 
for independent 
variables 
Faithful 
representation 
• Training set 
sensitive 
Materiality & 
faithful 
representation 
• The algorithm is 
scalable and 
adaptable 
Comparability 
Artificial neural network • Handles complex 
tasks effectively 
Relevance • Poor 
interpretability 
Verifiability & 
Understandability 
• Removing of duplicate entries 
and linking documents 
• Matching records or record-
linkage 
• Forecasting performance 
• Computing 
intensive 
Cost saving 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 76 
 
Machine learning 
techniques 
Benefits 
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Limitations  
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Tasks in the accounting 
process  
(Chapter 3 section 3.8) 
Bayesian Belief 
network  
• Can handle missing 
data 
Relevance & 
Materiality 
• Computing 
intensive. 
 
Cost saving • Error detection in financial 
data and fraud detection 
• Forecasting performance 
• Provides knowledge 
about causal 
relationships 
between variables 
Relevance & 
Understandability 
• Provides a method 
for avoiding data 
overfitting 
Relevance • Dependent on the 
prior knowledge 
Materiality & 
Faithful 
representation 
Association rules • Association rules 
can be assessed by 
the coverage and 
accuracy of the rule, 
assisting decision-
making on which 
rules to use 
Verifiability & 
Understandability 
• Excessive output Relevance • Error detection in financial 
data and fraud detection 
• Requires lots of 
time 
Timeliness 
K-means clustering • Simple and effective 
method of clustering 
Understandability • Requires lots of 
time 
Timeliness • Account allocation 
• Error detection in financial 
data and fraud detection 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 77 
 
Machine learning 
techniques 
Benefits 
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Limitations  
(section 4.7) 
Accounting 
objective 
(section 4.2) 
Tasks in the accounting 
process  
(Chapter 3 section 3.8) 
Self-organising maps • Easy to understand Understandability • Computing 
intensive  
Cost saving • Error detection in financial 
data and fraud detection 
• Requires 
adequate data 
Materiality & 
Faithful 
representation 
Conditional random 
fields 
• Manages 
haphazardly 
complex features 
Relevance • Trade-off 
between 
accuracy which 
requires memory 
and overfitting 
Relevance & 
Faithful 
representation 
• Report descriptions 
• Suited to 
overlapping features 
Faithful 
representation 
C4.5 decision trees • Can process data 
with missing 
features 
Relevance & 
Materiality 
• Pruning may lead 
to loss of 
accuracy 
Faithful 
representation 
• Forecasting performance 
• Can process 
continuous values 
Relevance 
Source: Own observation 
 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 78 
 
The table enables the user to consider the accounting objectives and assess the constraints 
of the technology. This forms part of the accounting user’s role when implementing and 
using machine learning technology (Sapp, 2017:16). 
 
4.8 User-related risks 
This section describes the risks related to the users of machine learning technology. Certain 
user risks have already been identified when examining other stages of the machine learning 
life cycle. In addition to these risks and benefits, further areas will be addressed in this 
section.  
 
The user-related risks that have already been identified are the following: 
• The risks relating to users not having the necessary technical skills and interpretability 
of machine learning algorithms as linked to accounting objective 7 in section 4.2.2. 
• The risk that errors in the algorithm go undetected owing to accounting users not 
being involved in testing (Gillion, 2017:7), as identified in section 4.4.  
• The risk of societal and unethical or discriminative biases in the machine learning 
model owing to bias in the training data (Gillion, 2017:7), as identified in section 4.4. 
 
Apart from these risks, there may be further risks related to the users of machine learning 
technology in the accounting process. These include the following: 
• The risk of overreliance on machine learning models because the limitations of the 
machine learning models are not understood (Gillion, 2017:7; Vihinen, 2012:3). 
• The risk that accounting users are unable to adapt to the new ways of thinking 
required for machine learning, leading to value loss for the business as users are 
unable to utilise the machine learning capabilities (Gillion, 2017:10). 
 
4.9 Maintenance risks 
• The risk that the desired functioning of the machine learning model is not maintained 
owing to changes in the model caused by input data (Sculley et al., 2015:2500). This 
may be the result of a lack of monitoring of the changes in the machine learning 
model. 
 
Stellenbosch University  https://scholar.sun.ac.za
 79 
 
• As described in section 4.4, there is a risk that the machine learning models are not 
updated regularly to remain relevant to the changing business environment (Amani 
& Fadlalla, 2017:48). 
 
• The risk of significant maintenance costs owing to the complexities that machine 
learning models present. Furthermore, maintenance is not only has to be performed 
on the machine learning code but also on the entire machine learning system, as data 
influences and changes the machine learning model (Sculley et al., 2015:2494). 
 
4.10 Security risks 
Barreno et al. (2010:126) suggest that from a machine learning security perspective, the risk 
is that an attacker may attempt to use the adaptive aspect of a machine learning model to 
cause problems. This would generally be achieved by targeting the data used by the 
machine learning model. The following security risks were identified: 
 
• Erroneous data not detected leading to errors 
If malicious false negative input data is processed by the machine learning model it 
could lead to the production of erroneous information, thus affecting the integrity of 
the information available to the accounting user. For example, the data features could 
be set up in such a way that the algorithm is unable to classify the data as erroneous 
and thus processes it as correct. 
 
• System compromise 
Risk of unauthorised access entering the system via viruses from input false negative 
data. 
 
• Corruption of model during training 
Risk of malicious data disrupting the machine learning process training with false 
positive data, causing the machine learning model to operate in a manner that differs 
from the objectives set by the accounting user; for example, causing the algorithm to 
classify correct data as incorrect or irrelevant. 
  
Stellenbosch University  https://scholar.sun.ac.za
 80 
 
• Disruption of service attack 
Risk of malicious data disrupting the operation of the machine learning model, in most 
cases with false positive data, also known as a denial of service attack. This may take 
the form of the machine learning model receiving an overwhelming amount of false 
data to the point that the algorithm is unable to process all the false inputs, resulting 
in downtime of the system. 
 
• Eavesdropping on training data 
Risk of an attacker eavesdropping on all network traffic while the learner gathers 
training data, thus being able to determine which data the business has available for 
training purposes. 
 
Apart from the security risks posed by malicious attackers, further security risks may include: 
• Data protection rights 
Risks surrounding privacy of data used in training machine learning algorithms, where 
non-authorised users have access to sensitive information (Gillion, 2017:9), as well 
as risks related to infringing data protection requirements (European Union, 2018:3). 
 
• Data ownership risks 
Risks pertaining to access to sensitive data or data not owned by the company that 
is legally protected, as a variety of data is necessary for training machine learning 
models. This may include data that the user or service provider may not necessarily 
own (The Royal Society, 2017:49). 
 
• Acquiring machine learning software 
Security risks identified in the machine learning model supply chain are described as 
part of the risks that are present when acquiring machine learning technology from a 
service provider in section 4.6. 
 
• Application and software risks  
Security risks relating to the application or system in which the machine learning 
model operates. These include policy enforcement risks, confidentiality risks, access 
control risks and data transmission risks. These risks are however not unique in a 
machine learning environment (Demchenko, De Laat & Membrey, 2014:110). 
 
Stellenbosch University  https://scholar.sun.ac.za
 81 
 
 
• Secure disposal of assets 
The risks that machine learning technology is not securely disposed of or terminated 
at the end of its useful life, which may lead to unauthorised access to data (ISACA, 
2012:165) or unintended code behaviour, where a machine learning code is not 
properly removed from the system (Sculley et al., 2015:2498). 
 
Apart from the possible security risks, machine learning can also hold benefits for security. 
The ability to adapt to changing and complex situations has meant that machine learning 
has also become a fundamental tool for computer security (Barreno et al., 2010:121). 
 
4.11 Conclusion 
This chapter identified the risks, benefits and limitations associated with machine learning 
and specific machine learning techniques as identified in chapter 3. The risks were mapped 
to each stage of the technology life cycle, thus indicating where user involvement was 
required. These risks include risks relating to achievement of the accounting objectives, 
machine learning architecture risks, business infrastructure risks, user-related risks and 
security risks.  
 
On the other hand, the benefits identified included ways in which machine learning could 
assist the user in achieving the accounting objectives. The limitations of machine learning 
in regard to achieving the accounting objectives were also described. In addition, the 
benefits and the limitations of the different machine learning techniques identified in chapter 
3 were discussed.  
 
The findings of this chapter, namely, the risks, benefits and limitations of machine learning 
technology, especially in an accounting context, are used in chapter 5 to develop guidelines 
for implementing machine learning techniques in an accounting context. A summary of the 
identified risks per category and the relevant guidelines presented in chapter 5 are listed in 
Table 12.
Stellenbosch University  https://scholar.sun.ac.za
 82 
 
Table 12: Identified risks and relevant user considerations 
Risk category 
Accounting 
objective 
Risk Consideration 
Guideline 
section 
Data ingestion  Unstable input data source Monitor stability of input data 5.4.2 
Data ingestion Faithful representation Missing features in input data 
Monitor quality of input data and user 
involvement in preparation of data 5.4.2 
Pre-processing data Faithful representation Errors in input data 
Monitor quality of input data and user 
involvement in preparation of data 5.4.2 
Pre-processing data Faithful representation Errors in the training data 
Data validation techniques on training 
data 5.4.2 
Pre-processing 
data/security 
Faithful 
representation 
Data integrity, privacy and data 
protection Access controls 5.4.2 
Model Relevance & Materiality Impact of outliers 
Users to specify when outliers are 
relevant 5.4.1 
Feature selection Faithful representation Redundant features in the data. 
Enquire which features have been 
included and perform leave-one-feature-
out training 
5.4.3 
Feature selection Faithful representation 
Discriminatory features or 
unethical data in training set 
Enquire which features have been 
included and remove or filter 
discriminatory features 
5.4.3 
Feature selection Faithful representation Missing features in training data 
Enquire about feature selection 
techniques and analysis tools 5.4.3 
Training and testing 
set 
Materiality & 
Faithful 
representation 
Insufficient real-world data for 
training and testing Synthetic data and data preparation tools 5.4.3 
Algorithm Faithful representation Incorrect bias 
Bias will need to be assessed by 
technical professionals 5.4.4 
Algorithm/ 
maintenance Comparability Undetected changes in behaviour Enquire about monitoring mechanisms 5.4.4 
Model  Does not address learning problem 
There needs to be a degree of 
repeatability or structural pattern in the 
learning problem 
5.4.1 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 83 
 
Risk category 
Accounting 
objective 
Risk Consideration 
Guideline 
section 
Model Verifiability & Understandability 
Complex algorithm affecting 
interpretability 
Model certification and input to output 
mappings 5.4.1 
Model/ maintenance  Not adaptable to changing business needs Regular recalibration of model 5.4.1 
Model Comparability Solutions not transferable to new problems Improve model interpretability 5.4.1 
Model 
Materiality & 
Faithful 
representation 
Predictive accuracy of the model Confidence levels or probability measurements 5.4.1 
Model  Scalability of the model Consider incorporating cloud technologies 5.4.1 
Model/ maintenance Cost vs benefit Monitoring and maintenance costs of the model 
Real-time monitoring and automated 
responses and obtaining independent 
assurance as well as appropriate system 
design 
5.4.1 
Model and 
experimentation Timeliness Long training and operating times 
Enquire about operating times and 
volume of transactions that model can 
process 
5.4.1 
Tuning/testing  Overfitting 
Separate dataset for testing and 
evaluation of the model as well as cross-
validation techniques 
5.4.5 
Training  Undetected accounting errors in model 
Accounting users should be involved in 
testing 5.4.5 
Testing/ user risk  User not involved in testing Accounting users should be involved in testing 5.4.5 
Testing  Incorrect assessment measures 
Testing environment should mimic real 
environment and guaranteed 
performance levels 
5.4.5 
Execution  Overreliance on models Be aware of limits of the model. 5.4.1 
Experimentation/ 
infrastructure  Insufficient storage Consider cloud technology 5.5 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 84 
 
Risk category 
Accounting 
objective 
Risk Consideration 
Guideline 
section 
Experimentation/ 
infrastructure/ service 
provider 
 Insufficient processing power Ensure adequate on-site power or use of cloud technology 5.5 
Infrastructure Cost vs benefit High costs of adoption Cost-benefit analysis 5.5 
Data storage/ security  Unauthorised access to output Settings, encryption, read only rights 5.7 
Data storage/ 
infrastructure  Interoperability 
Update data analytic architectures and 
define data and service standards 5.5 
Infrastructure  Incorrect configuration options selected Adopting good configuration principles 5.5 
Infrastructure  Insufficient supply of data 
Agreements with external parties, 
assessing external data quality and data 
analytics architectures and investment in 
enabling technologies 
5.5 
Infrastructure  Not scalable or flexible Modernise infrastructure and consider cloud technologies 5.5 
Infrastructure Cost vs benefit Outdated core systems leading to costs of updating Cost-benefit analysis 5.5 
Infrastructure  Lack of monitoring of machine learning model 
Ask technical professionals about 
monitoring capabilities of the model and 
obtaining independent assurance 
5.5 
Using service provider  Does not meet accounting needs 
Use reputable suppliers and select 
providers that interoperate with multiple 
frameworks 
5.5.1 
Using service provider  Unavailable in the market Users perform cost-benefit analysis before in-house development 5.5.1 
Using service 
provider/ security 
Faithful 
representation Security risks such as backdoors 
Use reputable suppliers, integrity in 
transit guarantees and machine learning 
models come with digital signatures, 
independent assurance and the use of a 
service level agreement 
5.5.1 
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 85 
 
Risk category 
Accounting 
objective 
Risk Consideration 
Guideline 
section 
Using service 
provider/ user risks Understandability 
Insufficient technical skills to 
understand model Train users in machine learning basics 5.6 
User risks  Unethical or discriminative models 
Enquire which features have been 
included and remove or filter 
discriminatory and unethical features 
5.4.3 
User risks  Overreliance on models Be aware of the limits of the model 5.4.1 
User risks  Users not adaptable leading to value loss 
Accountants need to adapt their thinking 
and improve their communication skills 5.6 
Security Faithful representation Unauthorised access-viruses Virus detection system 5.7 
Security Faithful representation Malicious training data 
Incorporate identification capabilities into 
algorithm 5.7 
Security  Denial of service attack Machine learning adaptive security 5.7 
Security Faithful representation Access to training data Access controls 5.7 
Security  Data privacy and protection Access controls 5.4.2 
Security  Data protection risks where not owned by company 
Access controls, encryption, scanning of 
data for threats and read-only rights 
5.4.2 & 
5.7 
Security/ terminate/ 
maintenance  
Inadequate disposal of data or 
assets 
Secure disposal of data and review 
codes to identify unnecessary codes 5.7 
Source: Own observation
Stellenbosch University  https://scholar.sun.ac.za
Stellenbosch University  https://scholar.sun.ac.za
 86 
 
Chapter 5: Guidelines for implementing machine learning in an accounting 
context 
 
5.1 Introduction 
In chapter 4 the risks, benefits and limitations when implementing machine learning were 
identified. These findings are used to achieve a further objective of this study, namely, to 
identify the steps to take when implementing machine learning technology to ensure 
alignment with the goals of the accounting process. 
 
As determined in chapter 4, principle 12 of King IV (Institute of Directors of Southern Africa 
(IODSA), 2016:41) requires businesses to govern technology in a way that supports the 
business in achieving its objectives. Therefore, the findings of chapter 4 together with the 
principles of King IV will be used to develop guidelines for implementing machine learning 
technology in the accounting processes.  
 
To achieve the objective, this chapter is structured in the form of steps that users could take 
when implementing machine learning technology. These steps are aligned to the stages of 
the data science life cycle as described in section 4.3 of chapter 4. The first step involves 
assigning the responsibility for implementing machine learning technology and for the 
governance of the technology. The second step requires users to consider the impact of the 
machine learning technology on the accounting objectives. This is important as the 
accounting objectives are in fact the goals of the accounting process. 
 
The third step involves users considering different aspects of the machine learning 
architecture, while the fourth step requires users to consider the various requirements of the 
business infrastructure. The fifth step involves determining user requirements and the final 
step involves the consideration of security requirements. 
 
5.2 Step 1: Assigning responsibility for implementing machine learning 
technology 
According to King IV (Institute of Directors of Southern Africa (IODSA), 2016:62), those 
responsible for the governance of the business should set the direction for technology 
governance. This may include drawing up a policy that describes the direction of the 
businesses approach to the technology and incorporates plans for managing the risks 
Stellenbosch University  https://scholar.sun.ac.za
 87 
 
surrounding machine learning technology as part of the business’s risk management 
procedures.  
 
The responsibility for implementing the technology can then be delegated to management. 
When users develop their own machine learning models or acquire software packages that 
incorporate machine learning technology, the associated risks will need to be addressed. 
 
When considering the role of the user in implementing machine learning technology, there 
are certain tasks in the data science life cycle that Sapp (2017:16) recommends accounting 
users be involved in. These tasks are listed in Table 6 in section 4.3. An extract of this table 
is provided as Table 13, where the tasks that users need to be involved in are mapped to 
the user considerations section. 
 
Table 13: Sections applicable to accounting user tasks in data science life cycle 
Task 
Accounting 
user 
involvement 
Related risks, 
benefits & 
limitations 
Consideration type 
Relevant 
section 
Determine problem 
objective  
Section 4.3 
Accounting 
objectives 
5.3 
Define success 
criteria  
Section 4.3 
Testing 
5.4.5 
Assess constraints 
 
Sections 4.4 and 4.5 
Sections 4.4 and 4.5 
Machine learning 
technique limitations 
5.3 
Assess available 
data  
Data 
5.4.2 
Explore data  
Section 4.4 
Section 4.4 
Data 5.4.2 
Training & testing set 
selection  
Feature selection 
5.4.3 
Explain model  Sections 4.7 and 4.8 Model 5.4.1 
Monitor and maintain  Sections 4.4; 4.5; 4.9 
and 4.10 
Infrastructure 5.5 
Terminate  Security 5.7 
Source: Own observation 
 
Table 13 will assist users to understand their role in implementing machine learning 
technology and what they should consider when addressing the identified risks. The sections 
that follow present these considerations and the guidelines that address the identified risks. 
Stellenbosch University  https://scholar.sun.ac.za
 88 
 
Table 12 in chapter 4 section 4.11 provides a summary of the identified risks per category 
and the relevant guideline section. 
 
5.3 Step 2: Consider the accounting objectives 
Risks are assessed based on their significance to the accounting users in terms of the 
specific processes the machine learning technology will be used for. The most significant 
risks for the purposes of user considerations are those that affect the achievement of the 
accounting objectives. The risks identified in chapter 4 section 4.2.2 pertaining to the 
accounting objectives may have their origin in the machine learning architecture or the 
machine learning model or they may be user related.  
 
Table 7 in chapter 4 section 4.2.2 indicates the type of consideration pertaining to each risk. 
When considering the accounting objectives, users need to take into account the specific 
risks and benefits described in chapter 4 section 4.2.2, as well as the benefits and limitations 
of the machine learning techniques, as described in chapter 4 section 4.7. Table 11 in 
chapter 4 section 4.7 maps the respective benefits and limitations of the different machine 
learning techniques to the accounting objectives for each accounting task. 
 
In general, accounting tasks are suited to machine learning owing to the following 
accounting attributes, as described by SMACC (2017:6): 
• Financial information has an organised data structure. 
• Data inputs such as invoices and bank accounts are readily available. 
• Data inputs are easy to transform into a digital form. 
• Accounting has rules that must be followed for the verification of data. 
• Accounting lends itself to the processing time that is sometimes required for 
processing that uses machine learning. Outputs do not have to be instantly available, 
rather they should be available in a timely manner. 
 
When a user has established that the learning problem is suited to machine learning and 
the objectives a set by the accounting users, there will be additional steps in the form of 
considerations and controls that users will have to put in place in order to implement machine 
learning in the accounting process. These steps are expanded on in the following sections. 
 
Stellenbosch University  https://scholar.sun.ac.za
 89 
 
5.4 Step 3: Consider the machine learning model and architectural components 
This section sets out the user considerations pertaining to the various components of the 
machine learning architecture. Most of the risks that affect the achievement of the 
accounting objectives are risks pertaining to components of the machine learning 
architecture. These risks include the machine learning model, data considerations, feature 
selection, the training set and algorithm considerations and testing, as set out in Table 7 of 
chapter 4 section 4.2.2. 
 
This section is organised according to the different machine learning architecture 
components and their respective considerations. 
 
5.4.1 Machine learning model considerations 
The considerations listed below will need to be taken into account by accounting users when 
asking technical professionals about the machine learning model. 
 
• The limits of the model  
Gillion (2017:7) recommends that users be aware of the limits of the model to ensure 
that these models are not overly relied on and that human involvement is retained in 
the accounting decision processes. 
 
• The predictive accuracy of the model 
One control to assist users in understanding the predictive accuracy of a machine 
learning model and the implications for decision-making would be to provide explicit 
confidence levels or a measure of the probability of the model outputs (Gillion, 
2017:7; Vihinen, 2012:3). 
 
• The impact of outliers on the model  
Users should specify whether outliers are relevant in the task being performed, such 
as in error detection in task 7 of the preparation of management accounts as 
described in chapter 2 section 2.4.3. 
 
 
 
 
Stellenbosch University  https://scholar.sun.ac.za
 90 
 
• Adaptability of the model to new problems  
The Royal Society (2017:30) suggests that improving the interpretability of the model 
will increase the possibility of being able to transfer the model between learning 
problems.  
 
• The interpretability of the model  
The Royal Society (2017:94) makes the following recommendations to address the 
problem of interpretability: 
o Model certification – this would indicate the competence of the machine 
learning model. 
o Input to output data mappings – this would indicate the influence of the 
different inputs on the outputs. 
 
• The adaptability of the model to changing business needs  
Amani and Fadlalla (2017:48) recommend that models be recalibrated regularly to 
ensure that they remain valid and are able to perform the required tasks over time. 
 
• Model training and operating times 
To address risks related to timeliness, users will need to ask technical professionals 
about the speed at which the model operates, the volume of transactions the model 
can process and the training time required to train the model.  
 
• The scalability of the model 
Sapp (2017:2) recommends that to assist model scalability, cloud-based capabilities 
be incorporated when designing machine learning models. This is because the cloud 
platform has elastic characteristics that assist in scaling algorithms. 
 
• Appropriateness of the machine learning approach 
The learning problem will need to have a certain amount of repeatability or a 
structured pattern in order to be suited to machine learning (Gillion, 2017:7; Someren 
& Urbancic, 2006:371). 
  
Stellenbosch University  https://scholar.sun.ac.za
 91 
 
• Monitoring requirements 
Users will want to ask technical professionals about capabilities for monitoring the 
machine learning models (Sapp, 2017:18). Sculley et al. (2015:2500) recommend 
real-time monitoring of the entire machine learning system and state that an 
automated response is important to sustain the reliability of the system.  
 
The following specific items will need to be monitored by users, based on the risks 
identified in chapter 4: 
o Unauthorised access to the machine learning model 
o Unauthorised or malicious data being processed by the machine learning 
technology 
o Quality of input data 
o Errors and exceptions 
o Outliers 
o Algorithm changes. 
 
• Assurance requirements 
The business may consider periodic independent assurance on the effectiveness of 
the machine learning model, including where the technology is provided by a service 
provider (Institute of Directors of Southern Africa (IODSA), 2016:63). 
 
• Maintenance costs 
Maintenance costs may be high as a result of a number of factors, including but not 
limited to configuration problems, changes in features or data changes affecting 
algorithm performance and data dependencies. Ensuring that a system is adequately 
designed and monitored can reduce unnecessary maintenance costs caused by 
these risk factors (Sculley et al., 2015:2494). 
 
5.4.2 Data considerations 
Data considerations pertain to risks such as errors in training or input data, missing features 
in the input data, the impact of outliers and risks relating to unstable input data sources. 
Specific considerations and possible controls users can employ to address the identified 
data risks are described below.  
 
Stellenbosch University  https://scholar.sun.ac.za
 92 
 
To address the risk of errors in the training data, Breck et al. (2018:2) recommend using 
data validation techniques to monitor the data quality. Similarly, Sculley et al. (2015:2500) 
recommend that in order to maintain a well-functioning system, some input data should be 
tested to address the risk of errors, changes in the data or incomplete data. 
 
In addition, it may be necessary to monitor the stability of the input data, especially in 
situations where data is produced by other machine learning algorithms. This may happen, 
for example, when integrating machine learning into the various accounting processes 
(Sculley et al., 2015:2496). 
 
Accounting users may need to be directly involved in managing the inputs or outputs of 
machine learning models, such as exception-handling or preparation of the data inputs 
(Gillion, 2017:10). Users may also need to enquire about the input data sources required to 
use the model. This data could come from a number of sources and may be structured or 
unstructured (Sapp, 2017:6). 
 
Lastly, adequate controls will need to be put in place for the governance of data, including 
access controls to maintain the integrity of input data and to protect privacy rights (Sapp, 
2017:23). 
 
5.4.3 Feature selection and training and testing set considerations 
Feature selection and training set risks include risks of redundant, discriminatory and 
unethical or missing features. In addition, there is the risk of insufficient data for training 
leading to an imbalance of features.  
 
To identify redundant or discriminatory features, users may want to ask technical 
professionals which features have been selected to train algorithms to perform the 
accounting tasks and use their accounting knowledge to determine whether those features 
are relevant. To be able to perform this function users may require an understanding of 
machine learning techniques (Gillion, 2017:10). 
 
Sapp (2017:23) also recommends that users should try and remove features affecting 
privacy or ethical rights from the dataset used to train machine learning models. This can be 
done by filtering data that may infringe on privacy rights or support unethical predictions. 
 
Stellenbosch University  https://scholar.sun.ac.za
 93 
 
Furthermore, there may be undetected unnecessary features in the data set which can be 
addressed by evaluations where leave-one-feature-out trainings are done (Sculley et al., 
2015:2496). Users may also need to ask technical professionals about the feature selection 
techniques, variable clustering, and analysis tools used to ensure that no important features 
are missing from the training data set (Amani & Fadlalla, 2017:47). 
 
Data for training may also be insufficient, in which case Ahmed et al. (2016:285) recommend 
the use of synthetic data to train models. Furthermore, to ensure that data is available for 
machine learning models, users may want to consider obtaining self-service data 
preparation tools to support technical professionals in preparing and manipulating data 
(Sapp, 2017:23). 
 
5.4.4 Algorithm considerations 
Algorithm considerations will be based on the identified risks, which included incorrect 
algorithm bias and undetected changes in algorithm behaviour. Accounting users may not 
be directly involved in addressing these risks but they will still need to consider the risks and 
ensure that they are addressed by technical professionals. 
 
Accounting users may want to ask about the way changes in algorithm behaviour are 
monitored, including the available detection mechanisms and whether these have been 
taken into consideration in the model design (Sculley et al., 2015:2494). 
 
Lastly, in order to avoid errors, appropriate data tools may need to be employed to detect 
and diagnose overly strong, weak or inappropriate biases in the machine learning algorithm 
(Dietterich & Kong, 1995:11). 
 
5.4.5 Testing considerations 
The main risks regarding testing considerations are that the wrong assessment measures 
are used to test the machine learning model and that errors go undetected because 
accounting users are not involved in testing the model. There is also the risk of overfitting, 
where the machine learning model predictions are too closely linked to the specific training 
data used to train it. 
 
With regard to testing the machine learning models, Gillion (2017:10) recommends that 
accountant users should be involved in training and testing the models and possibly in 
Stellenbosch University  https://scholar.sun.ac.za
 94 
 
auditing the machine learning algorithms. In addition, Sapp (2017:27) recommends that the 
testing environment should be as close to the real environment as possible. 
 
Users will need to ask technical professionals about how and whether the credibility of the 
model has been assessed to address overfitting. If overfitting is addressed during the 
development of the machine learning model, it will require the tuning of the model and cross 
validation methods, as well as the use of a large dataset separate from the training or testing 
data (Witten et al., 2016:286). 
 
The Royal Society (2017:112) recommends asking technical professionals about the 
guaranteed minimum level of performance of the model, which could be achieved by 
including the theoretically worst possible observable data during the training phase. 
 
5.5 Step 4: Consider infrastructure needs 
In order to support the machine learning architecture, the machine learning infrastructure 
which supports the architecture will need to be considered. The risks identified in this regard 
are insufficient storage, insufficient processing power, high adoption costs, lack of 
interoperability, and insufficient supply of data, scalability and flexibility and lack of 
monitoring capabilities. Based on the identified business infrastructure risks, users should 
consider the following infrastructure requirements: 
 
• Enabling technologies 
In chapter 2, various technologies were identified that would enable the use of, or 
could be combined with, machine learning technologies in the accounting processes. 
Users should consider whether investing in these technologies is necessary to be 
able to use machine learning technology in the desired processes. 
 
• The storage requirements of the model 
One option that enables that economical storage of data is cloud computing; 
however, it does expose the business to additional risks that are outside the scope 
of this study (Richins, Stapleton, Stratopoulos & Wong, 2017:74). 
 
• Processing power and hardware requirements 
Businesses will need the correct infrastructure in terms of hardware and processing 
power to ensure that machine learning processing can be properly executed. This will 
Stellenbosch University  https://scholar.sun.ac.za
 95 
 
infrastructure will depend on how advanced the machine learning technology is and 
may mean ensuring that the business has enough power on the premises or obtain 
the service from a cloud service provider (Gillion, 2017:9; Sapp, 2017:26). 
 
• The financial effects of adopting a machine learning model 
The business will need to perform a cost-benefit analysis to assess the economic 
benefits of replacing existing processes with machine learning technology, and 
thereby assess the business case for adopting machine learning. This will depend on 
whether users develop the technology in house or purchase the technology as part 
of accounting software (Gillion, 2017:9). 
 
• Interoperability of data analytics architecture  
Sapp (2017:1) recommends updating data analytics architectures to support data 
preparation for machine learning algorithms and to ensure adequate data supply and 
interoperability. 
 
• Standards that assist interoperability 
Daecher and Schmid (2016:42) recommend defining data and service standards to 
help ensure interoperability when implementing new technology in a business. 
 
• Sufficient data supply 
Accounting users will need to consider new ways of accessing data. The Royal 
Society (2017:49) highlights the need for users to enter into agreements with external 
parties to access the data required for machine learning models. Furthermore, users 
may need to monitor the quality of data obtained from external sources. 
 
• Configuration requirements 
For configuration, Sculley et al. (2015:2499) recommend that there are certain 
principles of good configuration in machine learning systems that should be adhered 
to, including enabling transparency with regard to number of features used, data 
dependencies, detection of unused settings, and controls to ensure that omissions or 
errors are detected. Finally, a full code review of the system configuration should be 
performed.  
 
Stellenbosch University  https://scholar.sun.ac.za
 96 
 
• Scalable and flexible infrastructure 
When a business is considering the adoption of machine learning technology and in 
ensuring its infrastructure is scalable and flexible, Corless et al. (2018:8) maintain it 
should continually modernise its infrastructure to incorporate the use of cloud 
technologies (Sapp, 2017:19).  
 
• Integration requirements. 
To integrate the technology into the existing system, Sapp (2017:13) highlights the 
need for businesses to have data integration tools and to ensure they have a 
thorough data integration strategy. 
 
5.5.1 Service provider and purchased machine learning considerations 
The risks of purchasing machine learning technology from a service provider include the risk 
that the technology required to meet business needs is not available, as well as the inherent 
security risks when using such technology.  
 
In situations where certain specialised accounting products are not supplied by service 
providers in the market, it may be that the cost of producing the product exceeds the benefit 
of solving the business problem. In such cases, users will need to perform a cost-benefit 
analysis before developing in-house solutions (Gillion, 2017:9). 
 
Users should also ensure that reputable software suppliers are chosen, especially when  
regulatory or legal requirements, as required with accounting information, are at issue 
(Gillion, 2017:9). 
 
When selecting a machine learning platform provider, users need to ensure that they select 
one than interoperates with multiple frameworks as this will assist the business in 
incorporating additional machine learning business solutions as these become available 
(Sapp, 2017:2). 
 
Even if a reputable service provider is chosen, there are still security risks posed by 
malicious attackers on outsourced machine learning technology. This emphasises the need 
to (Gu et al., 2017:11) 
o ensure that the channels used to obtain machine learning technology provide 
guarantees of integrity in transit 
Stellenbosch University  https://scholar.sun.ac.za
 97 
 
o ensure that repositories require the use of digital signatures for the machine learning 
models. 
 
Businesses will also need to consider assurance requirements such independent evaluation 
of service providers’ internal control environments. Furthermore, business may want to have 
a service level agreement in place to set out the responsibilities of the service provider in 
terms of errors, upgrades, downtime, security and integration. This would support the 
achievement of the King IV recommendation, which requires the performance of and risks 
related to outsourced service providers to be managed (Institute of Directors of Southern 
Africa (IODSA), 2016:62). 
 
5.6 Step 5: Consider user requirements 
Users also need to be aware of their own requirements when implementing machine 
learning technology, especially when considering the risks related to interpretability, errors 
not being detected and overreliance on machine learning models. Gillion (2017:10) 
recommends the following considerations be taken into account with regard to user 
requirements: 
 
• The skills required by accountants may need to be adapted to machine learning. 
Although accountants will not be able to train machine learning models, which 
requires a deep understanding and knowledge of machine learning techniques, they 
may need a basic understanding of machine learning to be able to perform their role 
when working with experts. 
 
• Furthermore, Gillion (2017:10) recommends that accountants should change their 
way of thinking, improve their critical thinking skills and communication abilities and 
become more adaptable to change. 
 
5.7 Step 6: Consider the security requirements 
The considerations and recommendations below address the security requirements for 
machine learning and are focused mainly on data governance and protection, as most of 
the security risks identified for machine learning technology pertain to malicious data.  
 
• Best practices for data protection may need to be included in a governance policy 
(“Considerations for senstive data within machine learning datasets”, 2017). 
Stellenbosch University  https://scholar.sun.ac.za
 98 
 
• Barreno et al. (2010:134) recommend the following defences when considering the 
security of machine learning technology: 
o Identify the components of the system to which access needs to be controlled, 
such as the training data, feature selection and model code, and ensure there are 
adequate access controls to sensitive technology resources (Oracle Corporation, 
2018:5).  
o Limit the feedback and therefore the output from the machine learning model to 
which unauthorised users have access. 
o Build resilience into the learning algorithm to identify contaminated training data 
or input data; in some cases increasing the complexity of the learning algorithm 
may defend against security attacks. 
o A virus detection system may be able to reduce the risk of a virus infection. 
 
• Ensure adequate security considerations are in place when acquiring machine 
learning technology from a service provider, as described in section 5.5.1. 
 
• Adequate controls surrounding the governance of data need to be in place, including 
access controls to maintain data integrity and to protect privacy rights on sensitive 
data (Sapp, 2017:23). 
 
• For sensitive data where ownership of the data is at risk, there may be settings that 
enable safe data sharing and use (The Royal Society, 2017:51). These settings may 
include encrypting sensitive data fields, processes that scan for sensitive and risky 
data and providing certain users with read-only rights (“Considerations for sensitive 
data within machine learning datasets”, 2017).  
 
• For data disposal, controls must ensure that machine learning technology is 
adequately disposed of or data securely deleted at the end of its useful life (ISACA, 
2012:165). In addition, users must ensure that machine learning code is periodically 
examined to determine any unnecessary code that can be removed (Sculley et al., 
2015:2498). 
 
Stellenbosch University  https://scholar.sun.ac.za
 99 
 
• The Oracle Corporation (2018:5) recommends identifying potential security risks 
through alerts from system and application logs, as these indicate user activities and 
changes in security configurations. 
 
• Users should be allowed the minimal rights and permissions required to complete 
their required actions (Oracle Corporation, 2018:5). 
 
• Finally, users may want to consider incorporating machine learning-based adaptive 
intelligence into their internal control environment as part of their risk assessment 
procedures and internal controls to provide an intelligent security framework (Oracle 
Corporation, 2018:10). 
 
5.8 Conclusion 
The guidelines presented in this chapter provided the various considerations to be made 
when implementing machine learning technology, based on the risks, benefits and 
limitations identified in chapter 4. These guidelines can assist users in determining whether 
to proceed with implementing machine learning technology, as well as in aligning the 
technology to the accounting process goals, and highlight the user’s role when implementing 
this technology.
Stellenbosch University  https://scholar.sun.ac.za
 100 
 
Chapter 6: Conclusion 
 
Prior research has shown a developing need for users to obtain an understanding of 
machine learning. For professional accountants, PwC (2015:16) even recommends that 
undergraduate accounting programmes should include advanced topics on machine 
learning as part of the curriculum. 
 
The aim of this study was to enhance users’ understanding of machine learning technology 
specifically in the performance of the accounting processes. This was achieved by 
identifying the accounting tasks that machine learning could perform and describing how 
this technology functions, as well as the risks, benefits and limitations associated with 
machine learning, including those that have a specific impact on the achievement of the 
accounting objectives. Based on the risks identified, steps to take when implementing 
machine learning technology in the accounting process were developed. 
 
This study focused on three accounting processes, namely, the translation of manual and 
electronic documents into accounting information, the reconciliation of financial information 
and the preparation of management accounts. As demonstrated in chapter 2, these 
processes consist of numerous tasks, many of which are enabled by existing technologies. 
Without the capabilities of these technologies, much of the functionality of machine learning 
could not be utilised.  
 
Having identified the accounting tasks, certain of these tasks presented learning problems 
to which machine learning techniques could provide a solution. Chapter 3 discussed the 
learning problems that could be addressed by machine learning, as well as the different 
machine learning techniques available to address these problems. It was shown that there 
may often be more than one machine learning technique available to address a learning 
problem and, in certain cases, the most beneficial solution may even be a combination of 
various machine learning techniques.  
 
Subsequently, each of the functions of the relevant machine learning techniques was 
discussed with the aim of providing accounting users with an understanding of them. The 
design and functionality of the technology was explained not only for the purpose of 
understanding it but also for identifying the associated risks, benefits and limitations. It is, 
Stellenbosch University  https://scholar.sun.ac.za
 101 
 
however, important here to bear in mind that the technology was not explained at the 
technical level required to develop the technology.  
 
The study then considered the risks, benefits and limitations of the machine learning 
technology. In trying to assist users in understanding the technology, chapter 4 considered 
the risks and benefits of each of the machine learning techniques and mapped those to the 
different accounting objectives as determined by the Conceptual Framework for Financial 
Reporting, as approved by the International Accounting Standards Board. These risks were 
summarised and linked to the relevant user considerations. The majority of the risks 
identified in chapter 4 were data risks, which included risks pertaining to data ingestion, pre-
processing of data, impact on the machine learning model, feature selection, the training 
and testing set, infrastructure requirements, data security risks, and termination and 
maintenance risks. 
 
Finally, the identified risks, benefits and limitations were used to develop guidelines for 
accounting users when implementing and using machine learning technology, including the 
areas where user involvement is required. The findings of chapters 4 and 5 have shown that 
users would need to give broader consideration to other components in addition to the actual 
machine learning model. One important broader consideration was the user requirements, 
which once again highlighted and confirmed the need for users to have an understanding of 
the technology, thus re-establishing the motivation for this study. 
 
When evaluating the considerations and possible options that users have available to 
address the identified risks, it was interesting to note that in certain instances machine 
learning technology gives rise to risks and is also able address the risk. For example, in the 
case of security risks, as described in chapter 4, users could consider incorporating machine 
learning-based adaptive intelligence in their security framework (Oracle Corporation, 
2018:10). 
 
In summary, the user has a key role to play when implementing and using machine learning 
technology in the accounting processes and should be equipped with an understanding of 
the technology and the risks and limitations, as well as the benefits of using the technology. 
In doing so, consideration should be given not only to machine learning technology but also 
to addressing the risks pertaining to all the components that enable the functioning of the 
technology in order to ensure alignment with the accounting process goals.  
Stellenbosch University  https://scholar.sun.ac.za
 102 
 
 
Further research which may be of value is the adaptation of a data governance framework 
applicable to machine learning technology. It should be noted that this study has just 
emphasised the need for adequate data governance and provided data governance 
considerations. Further research may therefore be required to assess the impact on an 
existing business when implementing machine learning in its current accounting processes 
and applying the steps provided in this research. There may also be a need to assess the 
impact of machine learning on the accounting profession by considering specific case 
studies of companies that have adopted the technology. 
  
Stellenbosch University  https://scholar.sun.ac.za
 103 
 
List of references 
ABBYY. 2017. [Online], Available: https://www.abbyy.com/en-apac/flexicapture/features/. 
ABBYY Technologies. n.d. What is OCR and OCR technology? [Online], Available: 
https://www.abbyy.com/en-apac/finereader/what-is-ocr/ [2017, August 06]. 
Aberdeen Group. 2017. The financial close: automation, efficiency, and the emergence of 
RPA. [Online], Available: http://www.kofax.com/~/media/Files/Kofax/whitepaper/rp-the-
financial-close-automation-efficiency-and-the-emergence-of-rpa-aberdeen-en.pdf [2017, 
August 20]. 
Adhitama, M.A., Sarno, R. & Sarwosri. 2016. Account charting and financial reporting at 
accounting module on Enterprise Resource Planning using tree traversal algorithm. In 
Surabaya, Indonesia: IEEE Proceedings of 2016 International Conference on Information 
and Communication Technology and Systems, ICTS. 20–25. 
Ahmed, M., Mahmood, A.N. & Islam, M.R. 2016. A survey of anomaly detection techniques 
in financial domain. Future Generation Computer Systems. 55:278–288. 
Albawi, S., Mohammed, T.A. & Al-Zawi, S. 2017. Understanding of a Convolutional Neural 
Network. In Anatalya: IEEE International Conference on Engineering and Technology 
(ICET). 1–6. 
Alpar, P. & Winkelsträter, S. 2014. Assessment of data quality in accounting data with 
association rules. Expert Systems with Applications. 41(5):2259–2268. 
Alreemy, Z., Chang, V., Walters, R. & Wills, G. 2016. Critical success factors (CSFs) for 
information technology governance (ITG). International Journal of Information Management. 
36(6):907–916. 
Amani, F.A. & Fadlalla, A.M. 2017. Data mining applications in accounting : A review of the 
literature and organizing framework. International Journal of Accounting Information 
Systems. 24:32–58. 
Amtrup, J.W., Thompson, S.M., Kilby, S. & Macciola, A. 2015. Patent No. 9058580 B1. 
United States. 
Appelbaum, D., Kogan, A., Vasarhelyi, M. & Yan, Z. 2017. Impact of business analytics and 
enterprise systems on managerial accounting. International Journal of Accounting 
Information Systems. 25(April):29–44. 
Arnab, A., Zheng, S., Jayasumana, S., Romera-Paredes, B., Larsson, M., Kirillov, A., 
Savchynskyy, B., Rother, C., et al. 2018. Conditional Random Fields Meet Deep Neural 
Networks for Semantic Segmentation. [Online], Available: 
http://www2.maths.lth.se/vision/publdb/reports/pdf/arnab-etal-2017.pdf. [2018, March 23]. 
Ayodele, T.O. 2010a. Introduction to Machine Learning. In Intech New Advances in Machine 
Learning. 1–8. 
Ayodele, T.O. 2010b. Types of Machine Learning Algorithms. In Y. Zhang (ed.). Intech New 
Advances in Machine Learning. 19–48. 
Stellenbosch University  https://scholar.sun.ac.za
 104 
 
Bailey, J.E. & Pearson, S.W. 1983. Development of a Tool for Measuring and Analyzing 
Computer User Satisfaction. Institute of Management Sciences. 29(5):530–545. 
Barreno, M., Nelson, B., Joseph, A.D. & Tygar, J.D. 2010. The security of machine learning. 
Machine Learning. 81(2):121–148. 
Belfo, F. & Trigo, A. 2013. Accounting Information Systems: Tradition and Future Directions. 
Procedia Technology. 9:536–546. 
Bengtsson, H. & Jansson, J. 2015. Using Classification Algorithms for Smart Suggestions 
in Accounting Systems. Unpublished masters thesis. Gothenburg: Chalmers University of 
Technology. [Online], Available: 
http://publications.lib.chalmers.se/records/fulltext/219162/219162.pdf [2018, February 7]. 
Berka, P. & Rauch, J. 2010. Machine Learning and Association Rules. In 19th International 
Conference On Computational Statistics COMPSTAT 2010. 1–29. [Online], Available: 
https://www.rocq.inria.fr/axis/COMPSTAT2010/TU_Berka-Rauch_paper.pdf. [2018, March 
23]. 
Bezerra, E., Mattoso, M. & Xexéo, G. 2006. Semi-Supervised clustering of XML documents: 
Getting the most from structural information. In Atlanta Proceedings of the 22nd International 
Conference on Data Engineering Workshops. 88. 
BlackLine. 2014. What are account reconciliations? [Online], Available: 
https://www.blackline.com/blog/account-reconciliation/ [2017, June 11]. 
Bose, I. & Mahapatra, R.K. 2001. Business data mining: a machine learning perspective. 
Information & Management. 39:211–225. 
Brady, E.S., Leider, J.P., Resnick, B.A., Natalia Alfonso, Y. & Bishai, D. 2017. Machine-
learning algorithms to code public health spending accounts. Public Health Reports. 
132(3):350–356. 
Bräuning, M., Hüllermeier, E., Keller, T. & Glaum, M. 2016. Lexicographic preferences for 
predictive modeling of human decision-making: A new machine learning method with an 
application in accounting. European Journal of Operational Research. 258:295–306. 
Breck, E., Polyzotis, N., Roy, S., Whang, S.E. & Zinkevich, M. 2018. Data Infrastructure for 
Machine Learning. [Online], Available: http://www.sysml.cc/doc/9.pdf. [2018, May 20]. 
Brownlee, J. 2013. How to Identify Outliers in your Data. [Online], Available: 
https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/ [2018, May 28]. 
Bryant, A. 2002. Re-grounding Grounded Theory. Journal of Information Technology Theory 
and Application. 4(1):25–42. [Online], Available: 
http://aisel.aisnet.org/cgi/viewcontent.cgi?article=1186. [2017, August 7]. 
Bucheli, H. & Thompson, W. 2014. Statistics and Machine Learning at Scale. [Online], 
Available: https://media.bitpipe.com/io_12x/io_123306/item_1144704/Statistics and 
Machine Learning at Scale-discovery.pdf [2018, May 27]. 
Buchholz, S., Jones, B. & Krumkachev, P. 2016. Reimagining core systems Emerging 
Technologies. [Online], Available: http://d27n205l7rookf.cloudfront.net/wp-
content/uploads/2016/02/DUP_TechTrends2016.pdf [2018, May 27]. 
Stellenbosch University  https://scholar.sun.ac.za
 105 
 
Burrell, J. 2016. How the machine ‘thinks’: Understanding opacity in machine learning 
algorithms. Big Data & Society. 3(1):1–12. 
Castle, N. 2018. What is Semi-Supervised Learning? [Online], Available: 
https://www.datascience.com/blog/what-is-semi-supervised-learning [2018, February 15]. 
Cearly, D.W., Walker, M.J. & Burke, B. 2016. Top 10 Strategic Technology Trends for 2017. 
[Online], Available: https://www.gartner.com/doc/3471559?ref=unauthreader&srcId=1-
6595640685 [2017, March 08]. 
Chadha, P. & Singh, G.N. 2012. Classification Rules and Genetic Algorithm in Data Mining. 
Global Journal of Computer Science and Technology Software & Data Engineering. 
12(15):51–54. [Online], Available: https://globaljournals.org/GJCST_Volume12/6-
Classification-Rules-and-Genetic-Algorithm.pdf [2018, January 23]. 
Chen, L., Wang, S., Fan, W., Sun, J. & Satoshi, N. 2015. Deep learning based language 
and orientation recognition in document analysis. In Tunis, Tunisia: IEEE 2015 13th 
International Conference on Document Analysis and Recognition. 436–440. 
Chew, P.A. 2014. Patent No. 8,639,596 B2. United States. 
Chew, P.A. & Robinson, D.G. 2012. Automated account reconciliation using probabilistic 
and statistical techniques. International Journal of Accounting & Information Management 
20(4):322–334. 
Cokins, G. 2013. Top 7 trends in management accounting. Strategic Finance. 95(6):21–30. 
Collobert, R. & Weston, J. 2008. A unified architecture for natural language processing: 
Deep neural networks with multitask learning. In Proceedings of the 25th international 
conference on Machine learning 160–167. 
Companies and Intellectual Property Commission. 2017. Why the CIPC Decided to Mandate 
XBRL. [Online], Available: http://www.cipc.co.za/files/8214/9139/3411/Why_XBRL_2.pdf 
[2017, August 10]. 
Considerations for senstive data within machine learning datasets. 2017. [Online], Available: 
https://cloud.google.com/solutions/sensitive-data-and-ml-datasets [2018, June 16]. 
Cooper, J. & Pattanayak, S. 2011. Chart of Accounts : A Critical Element of the Public 
Financial Management Framework. 
Corless, K., De Villiers, J., Garibaldi, C. & Norton, K. 2018. Reengineering technology. 
[Online], Available: https://www2.deloitte.com/content/dam/insights/us/articles/Tech-
Trends-2018/4109_TechTrends-2018_FINAL.pdf [2018, March 27]. 
CPA Australia Ltd. 2011. Dashboard Reporting. Melbourne. 
Creswell, J.W. 2009. Qualitative, Quantitative, and Mixed Methods Approaches. Third ed. 
Los Angeles: Sage Publications. 
Daecher, A. & Schmid, R. 2016. Internet of Things: From sensing to doing. [Online], 
Available: 
https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology/gx-tech-
trends-2016-innovating-digital-era.pdf [2018, May 27]. 
Stellenbosch University  https://scholar.sun.ac.za
 106 
 
Dataiku. 2017. Machine learning Basics: An Illustrated Guide for Non-Technical Readers. 
[Online], Available: https://pages.dataiku.com/machine-learning-basics-thank-
you?submissionGuid=80d21f82-ac46-45d0-969a-cd9914d06af9 [2018, February 06]. 
DBASS Chartered Accountants. 2014. How to Produce Valuable Management Accounts 
Using Sage Line 50. [Online], Available: http://www.dbass.ie/advice-resources/advice/how-
to-produce-valuable-management-accounts-using-sage-line-50 [2017, August 20]. 
De Leone, R. & Minnetti, V. 2015. Electre Tri-Machine Learning Approach to the Record 
Linkage Problem. [Online], Available: http://arxiv.org/abs/1505.06614.[2017, November 22] 
Demchenko, Y., De Laat, C. & Membrey, P. 2014. Defining Architecture Components of the 
Big Data Ecosystem. In Minneapolis, MN: IEEE 2014 International Conference on 
Collaboration Technologies and Systems (CTS). 104–112. [Online], Available: 
https://ieeexplore.ieee.org/abstract/document/6867550/.[2018, May 21] 
Deming, Z. n.d. [Online], Available: 
https://www.blackline.com/assets/docs/uploads/continuous-accounting-ebook.pdf.[2017, 
November 21]. 
Dietterich, T.G. & Kong, E.B. 1995. Machine Learning Bias, Statistical Bias, and Statistical 
Variance of Decision Tree Algorithms. Citeseer. Vol. 255. 0-13. 
Du, J. 2017. Automatic Text Classification Algorithm based on Gauss Improved 
Convolutional Neural Network. Journal of Computational Science. 21:195–200. 
Ehsani, A.H., Quiel, F. & Malekian, A. 2010. Effect of SRTM resolution on morphometric 
feature identification using neural network-self organizing map. GeoInformatica. 14(4):405–
424. 
Emmanuel, A.S. & Nithyanandam, S. 2014. An Optimal Text Recognition and Translation 
System for Smart phones Using Genetic Programming and Cloud. International Journal of 
Engineering Science and Innovative Technology (IJESIT). 3(2):437–443. 
European Union. 2018. Seven Steps for business to get ready for the General Data 
Protection Regulation. [Online], Available: https://ec.europa.eu/commission/sites/beta-
political/files/data-protection-factsheet-business-7-steps_en.pdf [2018, June 12]. 
Everest Group. 2014. Service Delivery Automation (SDA) Market in 2014: Moving Business 
Process Services Beyond Labor Arbitrage. (White paper EGR-2014-1-R-1264). [Online], 
Available: http://www.everestgrp.com/wp-content/uploads/2014/10/Service-Delivery-
Automation-Market-in-2014-Everest-Group-Report.pdf. [2017, November 15] 
Fedyk, A. 2016. How to Tell If Machine Learning Can Solve Your Business Problem. 
[Online], Available: https://hbr.org/2016/11/how-to-tell-if-machine-learning-can-solve-your-
business-problem [2017, July 29]. 
Gardent, C. & Perez-Beltrachini, L. 2017. A Statistical, Grammar-Based Approach to 
Microplanning. Computational Linguistics. 43(1):1–30. 
Ghanem, K. 2012. A simple process to speed up machine learning methods: Application to 
Hidden Markov Models. In S. Vaidyanathan & D. Nagamalai (eds.). Dubai The First 
International Conference on Advanced Information Technologies & Applications. 161–171. 
Stellenbosch University  https://scholar.sun.ac.za
 107 
 
Gillion, K. 2017. Artificial intelligence and the future of accountancy. 
Gorbunova, A. & Bochkarev, S. 2011. Automation of the functions of management 
accounting and controlling for industrial enterprises. In U. Fissgus, B. Krause, A. Kostygov, 
A. Leonid Mylnikov, & D.-I. Vladimir Alikin (eds.). Koethen: Perm State Technical University 
Solutions of applied problems in control, data processing and data analysis. 23–34. [Online], 
Available: http://www.inf.hs-
anhalt.de/~zischner/ic2/doc/Proceedings_of_the_Conference.pdf. [2017, June 12]. 
Goussies, N.A., Ubalde, S., Fernandez, F.G. & Mejail, M.E. 2014. Optical Character 
recognition using transfer learning decision forests. In Paris, France: IEEE IEEE 
International Conference on Image Processing(ICIP). 4309–4313. [Online], Available: 
http://ieeexplore.ieee.org.ez.sun.ac.za/document/7025875/. [2017, August 27] 
Gu, T., Dolan-Gavitt, B. & Garg, S. 2017. BadNets: Identifying Vulnerabilities in the Machine 
Learning Model Supply Chain. [Online], Available: http://arxiv.org/abs/1708.06733 [2018, 
June 16]. 
Hadzic, F., Dillon, T.S. & Tan, H. 2007. Outlier Detection Strategy using self-organising 
maps. In X. Zhu & I. Davidson (eds.). Hershey: IGI Global Knowledge Discovery and Data 
Mining: Challenges and Realities. 224–243. 
Hajek, P. & Henriques, R. 2017. Mining corporate annual reports for intelligent detection of 
financial statement fraud: A comparative study of machine learning methods. Knowledge-
Based Systems. 128(2017):139–152. 
Hamza, H., Belaïd, Y. & Belaïd, A. 2007. A case-based reasoning approach for invoice 
structure extraction. In Parana, Brazil: IEEE Ninth International Conference on Document 
Analysis and Recognition, ICDAR. 327–331. 
Hawkins, D.M. 2004. The Problem of Overfitting. Journal of Chemical Information and 
Computer Sciences. 44(1):1–12. 
Heckerman, D. 2008. Innovations in Bayesian Networks. In D.E. Holmes & L.C. Jain (eds.). 
Innovations in Bayesian Networks. Berlin Heidelberg: Spring-Verlag. 33–82. 
Institute of Directors of Southern Africa (IODSA). 2016. King IV Report on corporate 
governance for south africa 2016. [Online], Available: 
https://c.ymcdn.com/sites/www.iodsa.co.za/resource/resmgr/king_iv/King_IV_Report/IoDS
A_King_IV_Report_-_WebVe.pdf [2018, May 13]. 
International Accounting Standards Board. 2018. Conceptual Framework for Financial 
Reporting. IFRS Foundation. 
ISACA. 2012. COBIT 5 Enabling Processes. [Online], Available: 
http://www.isaca.org/COBIT/Pages/COBIT-5-Framework-product-page.aspx [2018, April 
04]. 
Jain, A., Jin, R. & Chitta, R. 2014. Semi-supervised Clustering. In C. Hennig, M. Meila, F. 
Murtagh, & R. Rocci (eds.) Handbook of Cluster Analysis. 1–29. 
Japkowicz, N. & Stephen, S. 2002. The class imbalance problem: A systematic study. 
Intelligent Data Analysis. 6(5):429–449. 
Stellenbosch University  https://scholar.sun.ac.za
 108 
 
Karamizadeh, S., Abdullah, S.M., Halimi, M., Shayan, J. & Rajabi, M.J. 2014. Advantage 
and drawback of support vector machine functionality. In 1st International Conference on 
Computer, Communications, and Control Technology, Proceedings. 63–65. 
Kaur, G. 2014. Association rule mining: A survey. International Journal of Computer Science 
and Information Technologies. 5(2):2320–2324. [Online], Available: 
http://sci2s.ugr.es/keel/pdf/specific/report/zhao03ars.pdf. [2018, June 4]. 
Kohlmaier, K., Hess, E. & Klehr, B. 2006. Patent No. 2006/00899087 A1. United States. 
Kohonen, T. 1990. The self-organizing map. Proceedings of the IEEE. 78(9):1464–1480. 
Kohonen, T. 1998. The self-organizing map. Neurocomputing. 21(1–3):1–6. 
Kohonen, T. 2013. Essentials of the self-organizing map. Neural Networks. 37:52–65. 
Kokina, J. & Davenport, T.H. 2017. The Emergence of Artificial Intelligence: How Automation 
is Changing Auditing. Journal of Emerging Technologies in Accounting. 14(1):115–122. 
Kotsiantis, S.B. 2007. Supervised Machine Learning: A Review of Classification 
Techniques. Informatica. 31:249–268. 
Krizhevsky, A., Sutskever, I. & Hinton, G.E. 2012. ImageNet Classification with Deep 
Convolutional Neural Networks. Advances In Neural Information Processing Systems. 1–9. 
Krutova, A.S. & Yanchev, A. V. 2014. Accounting process technology: Structural and Logical 
Aspects. Practical Science Edition “Independent Auditor”. 2(8):12–19. 
Lafferty, J., Mccallum, A. & Pereira, F. 2001. Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. Proceedings of the 18th International 
Conference on Machine Learning: (ICML). 282–289. 
Larsson, A. & Segerås, T. 2016. Automated invoice handling with machine learning and 
OCR. KTH Royal Institute of Technology. [Online], Available: https://www.diva-
portal.org/smash/get/diva2:934351/FULLTEXT01.pdf. [2017, June 7]. 
Lawrence, S., Giles, C.L., Tsoi, A.C. & Back, A.D. 1997. Face recognition: A convolutional 
neural-network approach. IEEE Transactions on Neural Networks. 8(1):98–113. 
Lee, D., Tsatsoulis, C. & Perry, S.M. 2009. Patent No. 7,596,545 B1. United States. 
Lopez De Mantaras, R. & Armengol, E. 1998. Machine learning from examples: Inductive 
and Lazy methods. Data & Knowledge Engineering. 25:99–123. 
Marsland, S. 2009. Machine Learning An Algortihmic Perspective. Boca Raton: CRC Press. 
Ming, D., Liu, J. & Tian, J. 2003. Research on Chinese financial invoice recognition 
technology. Pattern Recognition Letters. 24:489–497. 
Narasimha Murty, M. & Susheela Devi, V. 2011. Pattern Recognition: An Algorithmic 
Approach. I. Mackie (ed.). London: Springer. 
Niedermayer, D. 2008. An Introduction to Bayesian Networks and their Contemporary 
Applications. Studies in Computational Intelligence. 156:117–129. 
Stellenbosch University  https://scholar.sun.ac.za
 109 
 
OCREX. 2017. Features | Receipt Scanner App | Accounting Invoice Software. [Online], 
Available: https://autoentry.com/features/#accuracy-verified-extraction. [2017, August 09]. 
Oquab, M., Bottou, L., Laptev, I. & Sivic, J. 2014. Learning and transferring mid-level image 
representations using convolutional neural networks. In Columbus, OH, USA: IEEE 2014 
IEEE Conference Computer Vision and Pattern Recognition. 1717–1724. 
Oracle Corporation. 2018. Machine learning-based adaptive intelligence: The future of 
cybersecurity Executive summary. [Online], Available: 
http://www.oracle.com/us/solutions/cloud/future-of-cyber-security-4302684.pdf [2018, June 
12]. 
PWC. 2015. Data driven: What students need to succeed in a rapidly changing business 
world. [Online], Available: https://www.pwc.com/us/en/faculty-resource/assets/pwc-data-
driven-paper-feb2015.pdf. [2017, March 5]. 
Rhodes, J.J. & Wheat, D.A. 2015. Patent No. 20150117721 A1. United States. [Online], 
Available: https://www.google.com/patents/US20150117721. [2017, August 11]. 
Richins, G., Stapleton, A., Stratopoulos, T.C. & Wong, C. 2017. Big Data Analytics: 
Opportunity or Threat for the Accounting Profession? Journal of Information Systems. 
31(3):63–79. 
Roy, E. 2005. Patent No. 2005/0102212 A1. United States. [Online], Available: 
https://patents.google.com/patent/US20050102212A1/en [2017, October 29]. 
Sainani, K.L. 2014. Explanatory Versus Predictive Modeling. American Academy of Physical 
Medicine and Rehabilitation. 6(9):841–844. 
Saitta, L. & Neri, F. 1998. Learning in the “Real World”. Machine Learning. 30:133–163. 
Samoil, L.A. 2015. Multiple Entity Reconciliation. Stockholm: KTH Royal Institute of 
Technology. [Online], Available: http://www.diva-
portal.org/smash/get/diva2:928531/FULLTEXT01.pdf. [2018, January 18] 
Sapp, C.E. 2017. Preparing and Architecting for Machine Learning. [Online], Available: 
https://www.gartner.com/binaries/content/assets/events/keywords/catalyst/catus8/preparin
g_an. [2018, April 22]. 
Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaudhary, V., Young, 
M., et al. 2015. Hidden technical debt in machine learning systems. [Online], Available: 
http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf. 
[2018, May 20]. 
SMACC. 2017. Are SMEs able to keep an eye on their corporate finances in the same way 
as large corporates do ? [Online], Available: https://www.smacc.io/wp-
content/uploads/2017/04/finance-with-artificial-intelligence.pdf [2018, February 05]. 
Someren, M. Van & Urbancic, T. 2006. Applications of machine learning: matching problems 
to tasks and methods. The Knowledge Engineering Review. 20(4):363–402. 
Sorio, E. 2013. Machine Learning Techniques for Document Processing and Web Security. 
Published doctors dissertation. Trieste: University of Trieste. [Online], Available: 
http://hdl.handle.net/10077/8533 [2017, August 29] 
Stellenbosch University  https://scholar.sun.ac.za
 110 
 
Sorio, E., Bartoli, A., Davanzo, G. & Medvet, E. 2010. Open world classification of printed 
invoices. In Manchester, United Kingdom: ACM Proceedings of the 10th ACM symposium 
on Document engineering. 187–190. 
Suer, M., ITIL & Nolan, R. 2015. Using COBIT to deliver information and data governance. 
[Online], Available: http://www.isaca.org/COBIT/focus/Pages/Using-COBIT-5-to-Deliver-
Information-and-Data-Governance.aspx [2018, May 13]. 
Sutton, C. & Mccallum, A. 2007. An Introduction to Conditional Random Fields for Relational 
Learning. In L. Getoor & B. Taskar (eds.). Cambridge, Massachusetts: The MIT Press 
Introduction to Statistical Relational Learning. 93–128. 
Sutton, C. & Mccallum, A. 2011. An Introduction to Conditional Random Fields. Foundations 
and Trends in Machine Learning. 4(4):267–373. 
Sutton, S.G., Reinking, J. & Arnold, V. 2011. On the Use of Grounded Theory as a Basis for 
Research on Strategic and Emerging Technologies in Accounting. Journal of Emerging 
Technologies in Accounting. 8(1):45–63. 
Sutton, S.G., Holt, M. & Arnold, V. 2016. “The reports of my death are greatly exaggerated”: 
Artificial intelligence research in accounting. International Journal of Accounting Information 
Systems. 22:60–73. 
Takaki, J. & Ericson, G. 2018. Assign Data to Clusters. [Online], Available: 
https://cloud.google.com/blog/big-data/2018/01/problem-solving-with-ml-automatic-
document-classification [2018, February 07]. 
Takaki, J., Petersen, T. & Ericson, G. 2018. K-Means Clustering. [Online], Available: 
https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/k-
means-clustering [2018, February 07]. 
The Royal Society. 2017. Machine learning: the power and promise of computers that learn 
by example. Report. [Online], Available:  
https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-
learning-report.pdf. [2018, April 6]. 
Thomassey, S. & Fiordaliso, A. 2006. A hybrid sales forecasting system based on clustering 
and decision trees. Decision Support Systems. 42(1):408–421. 
Torraco, R.J. 2005. Writing Integrative Literature Reviews: Guidelines and Examples. 
Human Resource Development Review. 4(3):356–367. 
Trigo, A., Belfo, F. & Estébanez, R.P. 2014. Accounting Information Systems: The Challenge 
of the Real-time Reporting. Procedia Technology. 16:118–127. 
Trintech. 2017. There’s More to Operational Reconciliations than Matching. [Online], 
Available: https://www.trintech.com/resources/record-report-insights/theres-more-to-
operational-recs-than-matching/. [2017, July 23]. 
USC Libraries. n.d. [Online], Available: 
http://libguides.usc.edu/writingguide/researchdesigns. [2018, Januaray 18]. 
Valavanis, K.P., Kokkinaki, A.I. & Tzafestas, S.G. 1994. Knowledge-based (expert) systems 
in engineering applications: A survey. Journal of Intelligent and Robotic Systems. 10(2):113–
Stellenbosch University  https://scholar.sun.ac.za
 111 
 
145. 
Ventana Research. 2016. The Value of Continuous Accounting for Business. [Online], 
Available: 
https://www.ventanaresearch.com/white_paper/office_of_finance/the_value_of_continuous
_accounting_for_business [2017, July 30]. 
Vihinen, M. 2012. How to evaluate performance of prediction methods? Measures and their 
interpretation in variation effect analysis. BMC Genomics. 13(Supplement 4):1–10. 
Warwick, K. 2012. Artificial Intelligence: the basics. New York: Routledge. 
Watson, I. 1999. Case-based reasoning is a methodology not a technology. Knowledge-
Based Systems. 12(5–6):303–308. 
Whittington, G. 2007. Profitability, Accounting Theory and Methodology: The Selected 
Essays of Geoffrey Whittington. Abingdon: Routledge. [Online], Available: 
https://books.google.co.za/books?id=mH9nysHp0TgC&pg=PA198&dq=define+%22Manag
ement+accounts%22&hl=en&sa=X&ved=0ahUKEwjU0ZX4tubVAhVIL8AKHR_lB2oQ6AEI
LzAB#v=onepage&q&f=false [2017, August 20]. 
Winkler, W.E. 2014. Matching and record linkage. Wires Computational Statistics. 6(5):313–
325. [Online], Available: http://onlinelibrary.wiley.com/doi/10.1002/wics.1317/full. [2017, 
November 22]. 
Witten, I.H., Frank, E., Hall, M.A. & Pal, C.J. 2016. Data mining: Practical Machine Learning 
Tools and Techniques. Fourth ed. Cambridge: Morgan Kaufman. 
Yseop. 2017. Integrating Natural Language Generation Software Into Finance Workflows. 
[Online], Available: http://compose.yseop.com/downloads/integrating-natural-language-
generation-finance-workflows/ [2018, September 10]. 
Zhang, W., Tang, X. & Yoshida, T. 2015. TESC: An approach to text classification using 
Semi-supervised Clustering. Knowledge-Based Systems. 75:152–160. 
Zheng, J., Zhou, Y., Deng, T. & Yang, X. 2017. A self-trained semi supervised fuzzy 
clustering based on label propagation with variable weights. Proceedings of the 29th 
Chinese Control and Decision Conference, CCDC 2017. 7447–7452. 
 
Stellenbosch University  https://scholar.sun.ac.za
",188221025,"{'doi': None, 'oai': 'oai:scholar.sun.ac.za:10019.1/105005'}",User considerations when applying machine learning technology to accounting tasks,,2018-12-01T00:00:00+00:00,Stellenbosch : Stellenbosch University,[],['https://scholar.sun.ac.za:443/bitstream/handle/10019.1/105005/smith_user_2018.pdf?sequence=2&isAllowed=y'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/188221025.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/188221025'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/188221025/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/188221025/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/188221025'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/188221025?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=1&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Thesis (MCom)--Stellenbosch University, 2018.ENGLISH SUMMARY : Machine learning is a strategic technology that can have an important effect on business, as it is able to perform tasks efficiently that were previously only performed by humans. When implementing this technology in the relevant business processes and utilising it effectively, users have to understand both it as well as other aspects have to be considered. It was found that one area that is well suited to the adoption of machine learning, is accounting. In addition, prior research has shown a need for accounting users to be educated in machine learning as part of their professional training. Therefore, the aim of this study was to enhance users’ understanding of machine learning technology specifically in the performance of accounting processes.
A grounded theory methodology was employed to identifying the accounting tasks machine learning could perform, to describe how machine learning functions and to identify the risks, benefits and limitations associated with the technology. Finally, steps and considerations when implementing machine learning technology in the accounting process were provided.
The findings of this research are that the user has a key role to play when using machine learning technology in the accounting processes and thus has to understand the technology, the risks and limitations, as well as the benefits of the technology. The risks discussed relate not only to machine learning technology but also to all the components that enable the functioning of the technology to ensure alignment with the accounting process goals.
Based on these findings, this research presents the user considerations and steps to take when implementing machine learning in selected accounting processes. These can be used to identify areas that may require attention when a business is adopting machine learning. One important consideration is the implementation of adequate data governance. This is because most of the risks identified for machine learning technology are data risks. Further research could therefore be directed at developing a data governance framework for machine learning technologies.AFRIKAANSE OPSOMMING : Masjienleer is 'n strategiese tegnologie wat 'n belangrike uitwerking kan hê op besigheid, aangesien dit take doeltreffend kan uitvoer wat voorheen net deur mense uitgevoer is. Wanneer hierdie tegnologie in die toepaslike besigheids prosesse geïmplementeer en doeltreffend benut word, moet gebruikers dit verstaan en verskeie ander aspekte oorweeg. Daar is bevind dat Rekeningkunde een area is wat goed geskik is vir die aanneming van masjienleer. Daarbenewens, het vorige navorsing bevind dat rekeningkundige gebruikers opgelei moet word in masjienleer as deel van hul professionele opleiding. Die doel van hierdie studie was dus om gebruikers se begrip van masjienleertegnologie te verbeter, spesifiek in die uitvoering van rekeningkundige prosesse.
'n Gefundeerde teorie navorsingsmetodologie is gebruik om die rekeningkundige take wat masjienleer kan uitvoer te identifiseer, te beskryf hoe masjienleer funksioneer en om die risiko's, voordele en beperkings wat met die tegnologie verband hou, te identifiseer. Ten slotte is stappe en oorwegings tydens die implementering van masjienleertegnologie in die rekeningkundige proses verskaf.
Die bevindinge van hierdie navorsing is dat die gebruiker 'n sleutelrol speel wanneer masjienleertegnologie in die rekeningkundige prosesse gebruik word en dus moet die gebruiker die tegnologie, die risiko's en beperkings, sowel as die voordele van die tegnologie verstaan. Die risiko's wat bespreek word, hou nie net verband met masjienleertegnologie nie, maar ook met al die komponente wat die funksionering van die tegnologie moontlik maak om belyning met die doelwitte van die rekeningkundige proses te verseker.
Op grond van hierdie bevindinge, bied hierdie navorsing die gebruikersoorwegings en die stappe om te neem wanneer masjienleer in geselekteerde rekeningkundige prosesse geïmplementeer word. Hierdie oorwegings en stappe kan gebruik word om areas te identifiseer wat aandag benodig wanneer 'n besigheid masjienleer implementeer. Een belangrike oorweging is die implementering van voldoende databeheer, aangesien die meeste van die risiko's wat vir masjienleertegnologie geïdentifiseer is, data-risiko's is. Verdere navorsing kan dus gerig word op die ontwikkeling van 'n data-beheerraamwerk vir masjienleertegnologieë","['Thesis', 'Accounting -- Technological innovations -- Risk assessment', 'Machine learning', 'Accountants -- Training of', 'UCTD']",disabled
,"[{'name': 'Wilka, Rachel'}, {'name': 'Landy, Rachel'}, {'name': 'McKinney, Scott A.'}]",[],2019-12-15T01:46:36+00:00,"{'name': 'UW Law Digital Commons (University of Washington)', 'url': 'https://api.core.ac.uk/v3/data-providers/12132'}",,,,https://core.ac.uk/download/267982567.pdf,"Washington Journal of Law, Technology & Arts
Volume 13 | Issue 3 Article 2
4-1-2018
How Machines Learn: Where Do Companies Get
Data for Machine Learning and What Licenses Do
They Need?
Rachel Wilka
Rachel Landy
Scott A. McKinney
Follow this and additional works at: https://digitalcommons.law.uw.edu/wjlta
Part of the Computer Law Commons, and the Science and Technology Law Commons
This Article is brought to you for free and open access by the Law Reviews and Journals at UW Law Digital Commons. It has been accepted for
inclusion in Washington Journal of Law, Technology & Arts by an authorized editor of UW Law Digital Commons. For more information, please
contact cnyberg@uw.edu.
Recommended Citation
Rachel Wilka, Rachel Landy & Scott A. McKinney, How Machines Learn: Where Do Companies Get Data for Machine Learning and
What Licenses Do They Need?, 13 Wash. J. L. Tech. & Arts 217 (2018).
Available at: https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS 
VOLUME 13, ISSUE 3 SPRING 2018 
 
HOW MACHINES LEARN: WHERE DO COMPANIES GET 
DATA FOR MACHINE LEARNING AND WHAT LICENSES DO 
THEY NEED? 
 
Rachel Wilka, Rachel Landy, and Scott A. McKinney* 
© Rachel Wilka, Rachel Landy, and Scott A. McKinney 
 
Cite as: 13 Wash. J.L. Tech. & Arts 217 (2018) 
 
http://digital.law.washington.edu/dspace-law/handle/1773.1/1815 
 
ABSTRACT 
 
Machine learning services ingest customer data in order to 
provide refined, customized services. Machine learning algorithms 
are increasingly prominent in multiple sectors within the software-
as-a-service industry including online advertising, health 
diagnostics, and travel. However, very little has been written on 
the rights a company utilizing machine learning needs to obtain in 
order to use customer data to improve its own products or 
services. 
Machine learning encompasses multiple types of data use and 
analysis, including (a) supervised machine learning algorithms, 
which take specific data provided in a tagged and classified format 
to deliver specific predictable output; and (b) unsupervised 
machine learning algorithms, where untagged data is processed in 
                                                                                                         
* Scott McKinney and Rachel Landy are senior associates in the Technology 
Transactions practice at Wilson Sonsini Goodrich & Rosati, P.C. (WSGR). Scott 
is an adjunct professor at Georgetown Law and a guest lecturer for Cornell 
University’s Cornell Tech grad program. Rachel represents numerous technology 
companies on matters relating to intellectual property and commercial contracts. 
Rachel Wilka is Corporate Counsel at Zillow Group, Inc. and lead counsel for 
Zillow Rentals, hotpads, Inc., and dotlop, Inc., supporting all product counseling, 
licensing, commercial partnership, and risk management matters. Rachel was 
previously a technology transactions associate at Wilson Sonsini Goodrich & 
Rosati. The views expressed in this article are the authors’ own and do not 
represent the views of WSGR, Zillow, or any of the authors’ other clients. Thank 
you to Manja Sachet and Rob Philbrick. 
1
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
order to look for patterns and correlations without a specified 
output. 
This Article introduces the reader to the types of data use 
involved in various machine learning models, the level of data 
retention normally required for each model, and the risks of using 
personal information or re-identifiable data in connection with 
machine learning. The paper also discusses the type of license a 
commercial provider and consumer would need to enter into for 
various types of machine learning software. Finally, the paper 
proposes best practices for ensuring adequate rights are obtained 
through legal agreements so that machines may self-improve and 
innovate. 
 
  
TABLE OF CONTENTS 
 
Introduction .............................................................................. 219 
I. Background ....................................................................... 220 
A. Definition of Machine Learning .................................... 220 
B. Types of Machine Learning ........................................... 222 
1. Supervised ........................................................... 222 
2. Unsupervised ....................................................... 223 
3. Reinforcement ..................................................... 224 
II. Levels of Data Use Associated With Different Machine 
Learning Models ................................................................ 225 
A. Supervised .................................................................... 226 
B. Unsupervised ................................................................ 227 
III. Retention ........................................................................... 229 
IV. Sources of Data ................................................................. 231 
A. Data Sets Sold Through Data Brokers ........................... 231 
B. Ongoing Customer Data Collection From Network-
Connecting Software as a Service Offering ................. 232 
C. Batch Uploaded Data From Software Installed On-Premises 
for Customers ............................................................. 232 
D. Open Source Public Data Sets ....................................... 233 
V. Laws/Legal Risks Around Use of Data/PII in Machine Learning
 .......................................................................................... 233 
A. Use of Sensitive Data .................................................... 234 
B. The Output Use Case..................................................... 235 
2
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 219 
C. Breach of Contract/License ........................................... 236 
D. Impact on the Larger Market/Industry ........................... 237 
VI. What Needs to be Considered When Drafting an Agreement for 
a Machine Learning Service............................................... 237 
A. Predictions Versus Algorithm Improvements ................ 238 
B. Source of Data .............................................................. 238 
C. Output ........................................................................... 239 
D. Recommendations for Drafting ..................................... 239 
1. License Duration.................................................. 240 
2. Ownership of Created Output............................... 240 
3. Requirement for Data to be Provided in a De-
Identified/Non-Sensitive Format .......................... 241 
4. No Prohibition on Combining Data With Other Data 
Sets ...................................................................... 241 
5. Representation That Data was Gathered in 
Accordance With Applicable Law........................ 242 
Conclusion ............................................................................... 242 
Practice Pointers ....................................................................... 243 
 
INTRODUCTION 
 
Machine learning—it’s been a technology catch-phrase for at 
least five years, a tagline for any company purporting to “innovate 
a new future,” but what does it actually mean? Machine learning 
services ingest data in order to provide refined, customized services 
to users.1 
Real world utilization of machine learning increases daily, as 
more and more companies use the technology for market trend 
analysis, price setting, development of company (or industry) best-
practices, medical diagnoses, insurance—virtually any industry that 
has representable and analyzable output information can be 
optimized through machine learning.2  
                                                                                                         
1 See What is Machine Learning?, COURSERA, https://www.coursera.org/
learn/machine-learning/lecture/Ujm7v/what-is-machine-learning (last visited 
4/19/2018). 
2 See Louis Columbus, 10 Ways Machine Learning is Revolutionizing 
Marketing, FORBES (Feb. 25, 2018), https://www.forbes.com/sites/louiscolumbus
/2018/02/25/10-ways-machine-learning-is-revolutionizing-marketing/#803e5fe5
 
3
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
The algorithms that drive machine learning are increasingly 
prominent within the software-as-a-service industry, where machine 
learning can be leveraged for multiple industries, including online 
advertising, health diagnostics, and travel.3 Despite the increased 
use of machine learning across business sectors, the rights a 
company utilizing machine learning needs to obtain in order to use 
outside data to improve its own products are often amorphous and 
misunderstood. As machine learning becomes integral to companies 
across all industries and those companies become more and more 
reliant upon datasets for use in their machine learning analysis, the 
data itself (and the corresponding rights in such data) becomes 
increasingly important.  
This Article examines the legal data rights a company needs to 
obtain in order to use data for machine learning, and how those 
rights change depending on the machine learning model and 
business application. Part I of this Article defines machine learning 
and analyzes the various use cases for machine learning based on 
differing data rights. Part II discusses how companies may use data 
for different purposes. Part III discusses the varying degrees of data 
retention a company may undertake. In Part IV, we follow that 
discussion with an overview of data sources a machine learning 
company could access. Part V discusses the laws and legal risks 
relating to the use of data (including personally identifiable 
information (“PII”)) in machine learning applications across 
commercial sectors. Lastly, Part VI provides recommendations and 
considerations for drafting data licenses.  
 
I. BACKGROUND 
 
A.  Definition of Machine Learning 
 
The term “machine learning”, which is widely credited to ex-
                                                                                                         
bb64.  
3 See Forbes Technology Council, Looking Ahead: The Industries That Will 
Change The Most As Machine Learning Grows, FORBES, 
https://www.forbes.com/sites/forbestechcouncil/2017/03/08/looking-ahead-the-
industries-that-will-change-the-most-as-machine-learning-grows/#4c45248
c647b  
 
4
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 221 
IBM employee Arthur Samuel,4 is the ability of computers 
(“machines”) to learn without being guided or re-programmed.5 
Samuel’s initial machine learning example was a machine that can 
be programmed to play checkers better than the person who 
designed the program. Remarkably, a computer could be trained to 
do this in eight to ten hours of playing time over sixty years ago 
using machine learning.6 All that was necessary to train the 
computer was to provide it with the rules of the game, a general 
sense of direction regarding how the game worked, and a list of 
parameters that were thought to have something to do with the game, 
but whose correct background signs and relative importance were 
unknown and unspecified to the computer.7 In relatively short order, 
the machine learned how to play checkers better than its 
programmer, without the programmer having to revise the initial 
computer code or manually train the computer in strategy.8  
The use cases for modern machine learning are virtually 
boundless. Machine learning is best used in tasks for which 
designing code with explicit task-specific instructions is difficult or 
impossible, such as ranking, optical recognition, complex problem 
solving, and filtering.9 Machine learning applications typically 
involve feeding (relatively) automated programs a large data set of 
inputs, and solving problems or identifying issues using results-
driven decisions based on the data set.  
To be clear, machine learning (in the classic sense) is not 
artificial intelligence. Although machine learning does involve 
learning by experience, a machine learning algorithm does not act 
intelligently,10 and is not flexible in changing environments.11   
However, we see the concepts become increasingly conflated, as 
                                                                                                         
4 See A.L. Samuel, Some Studies in Machine Learning Using the Game of 
Checkers, 3 IBM JOURNAL OF RESEARCH AND DEVELOPMENT 210 (1959). 
5 Id. 
6 Id 
7 Id. 
8 Id. 
9 ETHEM ALPAYDIN, INTRODUCTION TO MACHINE LEARNING 6–8 (3rd ed. 
2014). 
10 See discussion infra Part I.B.  
11 DAVID POOLE ET AL., COMPUTATIONAL INTELLIGENCE: A LOGICAL 
APPROACH 1 (1998).  
 
5
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
algorithms are commonly programmed with artificial intelligence, 
and as machine learning algorithms come to make up a greater part 
of the artificial-intelligence ecosystem.12 Machine learning should 
not be conflated with data mining, either.13 Unlike data mining, 
which usually focuses on uncovering previously unknown 
properties of a dataset, machine learning typically focuses on better-
predicting outcomes or revising an algorithm based on already-
known properties of that dataset.  
Below we discuss the common types of machine learning and 
the different levels of data use associated with different machine 
learning models. 
 
B.  Types of Machine Learning 
 
Machine learning can be split into three major categories: (1) 
supervised, (2) reinforcement, and (3) unsupervised.14 We discuss 
each in turn below. 
 
1. Supervised 
 
With supervised machine learning, one knows the desired output 
of the algorithm based on a dataset, usually referred to as “training 
data,” that is used to optimize a performance criterion.15 Supervised 
machine learning algorithms are typically “taught” using a training 
dataset. If the algorithm provides unexpected or incorrect results 
                                                                                                         
12 See, e.g., Fred Jacquet, Exploring the Artificial Intelligence Ecosystem: AI, 
Machine Learning, and Deep Learning, DZONE/ AI ZONE (Jul. 4, 2017), 
https://dzone.com/articles/exploring-the-artificial-intelligence-ecosystem-fr. 
13 But see ALPAYDIN, supra note 9, at 2 (describing the application of 
machine learning methods to a database as “data mining.”). Opinions regarding, 
and semantical definitions of the term “machine learning” vary.  
14 See generally OLIVIER CHAPELLE, ET AL., SEMI-SUPERVISED LEARNING 
(2006). available at http://www.acad.bg/ebook/ml/MITPress-
%20SemiSupervised%20Learning.pdf; see also Vishal Maini, Machine 
Learning for Humans, Part 5: Reinforcement Learning, MEDIUM.COM (Aug. 19, 
2017), https://medium.com/machine-learning-for-humans/reinforcement-
learning-6eacf258b265.  
15 Id.; see also Data Sets and Machine Learning, DEEP LEARNING FOR JAVA 
https://deeplearning4j.org/data-sets-ml (last visited Mar. 31, 2018); ALPAYDIN, 
supra note 9, at 3. 
 
6
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 223 
after analyzing the base data using the training dataset, the 
programmer can make algorithmic tweaks (or changes to the 
training data) to right the course. In supervised machine learning, all 
of the data within a training data set is “labeled” (or assigned a 
value), which allows the machine to easily compare analysis data 
against the training set baseline.16 The algorithm generates 
information based on its analysis of the training data, and uses that 
information to produce inferred or revised functions. These revised 
functions can be used by the end user to discern new trends 
regarding a dataset, or to refine the algorithmic analysis itself.17 
Analyzing enormous data sets at a speed only computers can 
achieve, the algorithm can identify trends, flag otherwise 
unidentified issues, and give the algorithm operator other desired 
results that can be tweaked using variations in the algorithm or 
training data.   
 
2. Unsupervised 
 
In unsupervised machine learning, there is no training data, and 
the outcomes are unpredictable.18 Unsupervised machine learning 
algorithms can solve problems using input datasets alone, with no 
reference or training data, by recognizing patterns in the data and 
grouping together reoccurring or common data characteristics.19 
Unlike supervised algorithms, which rely on labeled data, 
unsupervised machine learning uses functions to uncover previously 
unknown properties of a dataset using unlabeled data. For example, 
say you had a dataset comprised of apples, oranges, and bananas, 
and want to analyze and identify trends in the fruit. The problems 
are: the data set is huge, the fruit are all jumbled together, and none 
of the data is labeled as an “apple,” an “orange,” or a “banana.” In a 
supervised machine learning scenario, if the algorithm was not 
“taught” to identify an apple, it would not know to look for, nor 
group together, apples. In contrast, an unsupervised machine 
learning algorithm is able, over time, to recognize that data across 
the datasets have similar characteristics, such as being shiny, red, 
                                                                                                         
16 Id. 
17 Id.; see also DEEP LEARNING FOR JAVA., supra note 15.  
18 ALPAYDIN, supra note 9, at 11. 
19 Id.  
7
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
and generally apple-shaped. Unsupervised algorithms can identify 
these similarities and group together the apples with the apples, the 
oranges with the oranges, and the bananas with the bananas. 
Unsupervised machine learning can seem to border on artificial 
intelligence,20 and companies often use it to analyze large datasets 
of customer transactions, generate common trends or characteristics 
based on the past transactions, group those customers into clusters, 
and use that cluster of information to refine the company’s business 
model.21  
There is a sub-class of supervised machine learning called 
“semi-supervised” machine learning, in which an algorithm-
operator uses a small amount of labeled training data to inform a 
much larger unlabeled dataset.22 Semi-supervised machine learning 
is usually thought of as halfway between unsupervised and 
supervised learning.23 Both supervised and semi-supervised 
machine learning tend to lend themselves to relatively predictable 
outcomes, and are often used by companies to optimize user 
experiences based on predicted or predetermined outcomes.  
 
3. Reinforcement 
 
Reinforcement learning is based on an algorithm that has a 
concept of how an environment should behave, and learns an 
optimal behavior for such an environment by analyzing repetition 
and repeated failures over time.24 Unlike supervised machine 
learning, reinforcement learning algorithms are not presented with 
input/output pairs for correction—instead, the algorithm is 
performance-driven.25 One well-known example of reinforcement 
                                                                                                         
20 See Bernard Marr, Supervised V Unsupervised Machine Learning – 
What’s The Difference?, FORBES (Mar. 16, 2017, 3:13 AM), 
https://www.forbes.com/sites/bernardmarr/2017/03/16/supervised-v-
unsupervised-machine-learning-whats-the-difference/#4ecd3f80485d. 
21 ALPAYDIN, supra note 9, at 12. 
22 CHAPELLE, ET AL., supra note 14, at 2–3. 
23 Id.  
24 See Leslie Pack Kaelbling, Michael L. Littman & Andrew W. Moore, 
Reinforcement Learning: A Survey, JOURNAL OF ARTIFICIAL INTELLIGENCE 
RESEARCH 4, 237 (1996). 
25 Id. 
 
8
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 225 
learning is the self-driving car industry.26 Many self-driving 
algorithms are not artificially intelligent in the traditional sense, but 
instead use repetition (i.e. driving thousands of test miles and 
tracking driving errors and successes) to optimize the algorithm and 
underlying technology in a way that human programmers could 
never do on their own.27 Another way to think about reinforcement 
learning is “trial-and-error”, but on a massive scale accomplishable 
only by computers.28 Over time, the software learns what to do, and 
what not to do, until its functionality is optimized for the task at 
hand.  
 
II. LEVELS OF DATA USE ASSOCIATED WITH DIFFERENT MACHINE 
LEARNING MODELS 
 
The use case for machine learning implementation dictates the 
data rights that must be obtained, as well as the applicable data 
retention and use policies. For example, consider these three 
different use cases: 
 
● OpenTable recommends restaurants, but can only do so 
based on the information it collects (e.g. where the user has 
dined before, not the actual dish he or she actually eats—
information OpenTable does not have).29 
 
● To predict which show a user will want to binge next, Netflix 
wants to know that user’s viewing history, and some relevant 
demographic information, such as age, gender, and 
location.30 
 
● Accolade’s Maya Intelligence Option inputs information 
                                                                                                         
26 See Will Knight, Reinforcement Learning, MIT TECHNOLOGY REVIEW 
(March/April 2017), https://www.technologyreview.com/s/603501/10-
breakthrough-technologies-2017-reinforcement-learning/. 
27 Id.  
28 Maini, supra note 14. 
29 OpenTable Privacy Policy, OPENTABLE, 
https://www.opentable.com/legal/privacy-policy (last updated May 15, 2017). 
30 Netflix Privacy Statement, NETFLIX, https://help.netflix.com/legal/privacy 
(last updated Nov. 30, 2016). 
 
9
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
about an individual’s health insurance, medical history, 
medications, test results, and other personal health 
information in order to provide personalized healthcare 
support.31 
 
Like all companies that depend on machine learning, these 
companies obtain, use, and retain data in different ways, depending 
on their business model and their machine learning models.  
 
A.  Supervised 
 
Supervised machine learning presents clearer use cases. The 
outcome is predictable, and in fact, programmed. Netflix and 
OpenTable, for example, ingest user preference data to produce 
individualized recommendations to that user. These algorithms do 
not necessarily rely on extraneous data inputs—they are trained to 
provide recommendations if certain inputs are present. But by 
continuously ingesting new data, the engine can be refined and 
perfected on an ongoing basis. For example, over time, Netflix may 
be able to distinguish between medical-drama fanatics who want to 
binge Grey’s Anatomy and those who prefer ER. For this reason, the 
results of supervised machine learning can be highly valuable to 
companies in any industry, but especially those industries that are 
consumer-facing.  
However, for both Netflix and OpenTable, the use of the data 
(recommendations) is not these companies’ core business; rather, it 
is an added feature that has helped propel both companies to the top 
of their respective industries. Without compelling 
recommendations, Netflix would still be a video streaming service. 
However, it relies on data to enhance the user’s experience, thus 
adding value to the service.32 Netflix does this by ingesting and 
inferring from a user’s preferences. For example, it knows if you 
watched one episode of Gilmore Girls, or if you watched every 
                                                                                                         
31 ACCOLADE, https://www.accolade.com/solutions/ (last visited March 30, 
2018). 
32 Chris Raphael, How Machine Learning Fuels Your Netflix Addition, 
RTINSIGHTS (Jan. 5, 2016), https://www.rtinsights.com/netflix-
recommendations-machine-learning-algorithms/. 
 
10
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 227 
season five times, and it can use that information to determine 
whether you were a superfan or lost interest quickly. 
The same is true, to a lesser extent, with OpenTable. OpenTable 
bases its recommendations largely on collections of user ratings.33 
However, OpenTable’s capabilities are limited. Its model does not 
know whether its users actually ate at a restaurant booked through 
OpenTable. It only knows how that user feels about the restaurant if 
he or she rates it on the app. Furthermore, the app does not know, 
for example, whether dietary preferences affected that rating.   
One benefit of supervised machine learning algorithms is that, 
in the early stages, potential data sets can be separated into those 
that are necessary and those that are merely helpful. A company may 
find that data sets with particular characteristics are subject to more 
extensive regulations than the data required to successfully 
implement a machine learning solution. As a result, the company 
will either utilize the data differently, or avoid implementation of 
the data altogether. For example, Netflix, in its early days, may have 
found that age was highly useful. However, unless the appropriate 
controls are in place, gathering other sensitive information, such as 
children’s’ names, can result in significant legal risk.34 
Nevertheless, using machine learning, a start-up company may find 
that it can estimate age based on user habits, thereby making it 
unnecessary to undertake the legal risk of gathering that information 
directly.35  
 
B.  Unsupervised 
 
Using unsupervised machine learning is a process best thought 
of as “high risk, high reward.” Without a clearly defined desired 
                                                                                                         
33 Pablo Delgado & Sudeep Das, Using Data Science to Transform 
OpenTable Into Your Local Dining Expert, presentation at SparkSummit 2015, 
available at https://www.slideshare.net/SparkSummit/using-data-science-to-
transform-opentable-into-delgado-das.  
34 See, e.g., Children’s Online Privacy Protection Act of 1998, 15 U.S.C. §§ 
6501–6506 (1998). 
35 This is contrary to companies operating in the healthcare space, which 
almost always need some level of personal health information—another highly 
regulated category of data. For those companies, the risk is inherent in the 
business and should be priced into the model for customers.  
 
11
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
output, the company may not know what it needs, or even what it is 
likely to get, from the algorithm. On the other hand, a company 
might get results that it did not anticipate or even think were 
possible. Unsupervised machine learning is popular in the health-
tech industry because making a diagnosis requires analyzing many 
variables that human doctors cannot necessarily test for 
individually.36 Machine learning gives doctors the assistance they 
need to take in a large amount of data and then spit out all known 
potential diagnoses. The Maya Intelligence Option, for example, 
could benefit from taking in numerous health data points in order to 
generate a potential treatment plan, the scope of which would not be 
pre-defined.  
Unsupervised machine learning, by its nature, requires that the 
operator have more flexibility in its use of data sets. As a result, the 
data use rights obtained from data providers (discussed in Part V) 
for use in unsupervised machine learning analysis should be broader 
than data use rights for supervised machine learning. For example, 
speech recognition software operators obtain broad rights to use data 
collected through the software (i.e. users’ speech). The Apple Terms 
of Service state: “By using Siri or Dictation, you agree and consent 
to Apple’s and its subsidiaries’ and agents’ transmission, collection, 
maintenance, processing, and use of this information, including 
your voice input and User Data, to provide and improve Siri, 
Dictation, and dictation functionality in other Apple products and 
services.”37 While Apple’s main purpose in collecting this data is 
likely to tune its engine to recognize speech more efficiently, such a 
broad license also allows the operator to use the speech for a number 
of ancillary purposes, such as understanding dialects, intonations, 
and speech impediments. Thus, the operator is not sure what the 
results will be or how those results may be used in the future. Indeed, 
an operator may find that certain data sets once considered vital turn 
out to be useless. Prior to implementation, the machine learning 
algorithm cannot necessarily predict which data is valuable and 
                                                                                                         
36 See, e.g., Chip M. Lynch, Victor H. van Berkel, Hermann B. Frieboes & 
Bin Liu, Application of Unsupervised Analysis Techniques to Lung Cancer 
Patient Data, PLOS ONE (Sept. 2017), available at 
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184370. 
37 Apple Ios Software User Agreement, APPLE INC., at 3 (emphasis added) 
available at https://www.apple.com/legal/sla/docs/ios6.pdf (last revised 2012). 
12
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 229 
which is not. This uncertainty necessitates a broader, less restrictive 
scope of operator rights than in other scenarios. In some cases, this 
may mean that the operator must assume the additional risks of 
using, collecting, or storing data that is subject to regulation. 
Overall, companies’ use cases and data supply needs should help 
inform whether their algorithms are unsupervised, reinforced, or 
supervised. Accordingly, the rights to be obtained to that data, 
discussed in Part V, should reflect those business decisions. 
Moreover, in addition to the data use rights that must be obtained, 
we must also consider the data storage and retention issues 
associated with machine learning.  
 
III. RETENTION 
 
In addition to determining whether an algorithm should be 
supervised or unsupervised, any machine learning company must 
determine the scope of its data retention policy. Data retention 
policies track how data is stored, shared, and deleted to ensure 
consistency of data treatment and compliance with contractual 
obligations, applicable law, and best practices. As discussed in Part 
II, the particulars of a data retention policy for a machine learning 
company rely on the use case for the algorithm and the data-
treatment requirements imposed by the data source.  
For example, a supervised machine learning environment may 
only need to retain training data if it is not using new data to improve 
its capabilities. Or, it may only need to retain the data for a limited 
period of time in order to establish overall patterns or features to 
include in training data. In our Netflix example, it may be helpful 
for Netflix to know that over a two-year period, a user watched all 
of Dawson’s Creek, Gilmore Girls, and 7th Heaven, but not Buffy 
the Vampire Slayer.38 Knowing, in context, that the user prefers 
real-life teen dramas to science-fiction teen dramas can help 
improve the algorithm.  
By contrast, an OpenTable user’s eating habits may not follow 
predictable patterns. The fact that a user ate at a Chinese restaurant 
five days in a row is helpful for understanding the user’s culinary 
tastes that week. But that same user could then decide she’s had 
                                                                                                         
38 This assumes that all of the programs mentioned are available on Netflix.  
13
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
enough Chinese food for a year, and move on to sushi. Thus, for 
OpenTable, pattern analysis is less important than it is for Netflix; it 
can simply build on each data input individually without a longer-
term analysis. Where Netflix may be able to determine that a user 
had a child based on a change in viewing habits (and could adjust 
accordingly), OpenTable’s use case doesn’t require a long data 
retention period to provide a benefit.  
Ultimately, assuming the operator has obtained the requisite 
rights from users (discussed in Part V), the operator ought to retain 
the data for as long as is commercially reasonable (although the 
relevant industry market approach may dictate that data be 
destroyed after a certain amount of time). To mitigate the potential 
harm of data destruction requirements, an operator should always 
retain the training data it used to fix bugs and help tune the 
algorithm. Other than the training data, a company could find that it 
need not retain a lot of individual data inputs so long as the algorithm 
has previously ingested, responded, and reacted to the data. 
Some data providers try to contractually require data destruction 
after the term of an engagement.39 Operators of unsupervised 
algorithms should always push back; the nature of those algorithms 
is such that there could always be a golden needle in a data-haystack, 
so an operator should try to retain the right to continue to mine the 
data for as long as possible. If a customer is insisting on destruction, 
an operator may promise anonymization and aggregation of the data 
so the customer could not be identified. Ultimately, the operator 
must determine at what point the algorithm (and the operator’s 
business) will be able to live without the data, i.e., when it has 
obtained sufficient replacement data to be self-sustaining. In other 
words, what retention term is reasonable for the company? The 
operator may be able to compromise by agreeing to only use a 
customer’s data in perpetuity where that data is anonymized and 
aggregated with other customers’ data sets. A company that destroys 
data will also need to develop an appropriate support policy if the 
original reference set is eventually deleted. 
 
                                                                                                         
39 See, e.g., Data License Agreement, PRACTICAL LAW COMPANY 
INTELLECTUAL PROPERTY & TECHNOLOGY, available at 
https://us.practicallaw.thomsonreuters.com/w-004-3938. 
14
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 231 
IV. SOURCES OF DATA 
 
Companies looking to obtain data to create or train machine 
learning algorithms tend to look to four sources: (a) data sets sold 
through data brokers; (b) batch uploaded data from software 
installed on-premises for customers; (c) ongoing customer data 
collection from network-connected software as a service offering 
(both for customer-facing improvements and other company 
purposes); and (d) open public data sets.40 
 
A.  Data Sets Sold Through Data Brokers 
 
Data brokers are companies that have gradually built databases 
of consumer data. These databases were originally built for 
“marketing, fraud detection, and credit scoring purposes.”41 
Companies can go to data brokers to purchase data sets, usually with 
personally identifiable information removed. Data brokers may 
offer a database (or set of databases) that tracks behaviors the 
operator wants to build a machine-learning algorithm around. Data 
broker databases can include demographic data, court and public 
records data, social media and technology data, consumer interests 
data, financial data, health data, and purchase behavior data.42 
However, some observers doubt whether data broker databases are 
sufficiently anonymized to avoid business or regulatory risk.43 
Another downside of purchased data is that the purchaser runs the 
                                                                                                         
40 See, e.g., SEATTLE OPEN DATA PORTAL, https://data.seattle.gov/ (last 
visited May 10, 2018). 
41 Bernard Marr, Where Can You Buy Big Data? Here Are The Biggest 
Consumer Data Brokers, FORBES (Sept. 7, 2017), https://www.forbes.com/sites/
bernardmarr/2017/09/07/where-can-you-buy-big-data-here-are-the-biggest-
consumer-data-brokers/#48d997096c27.  
42 See Leo Mirani & Max Nisen, The Nine Companies That Know More 
About You Than Google or Facebook, QUARTZ (May 27, 2014), 
https://qz.com/213900/the-nine-companies-that-know-more-about-you-than-
google-or-facebook/.  
43 See Alex Hern, Anonymous Browsing Data can be Easily Exposed, 
Researchers Reveal, THE GUARDIAN (Aug. 1, 2017), 
https://www.theguardian.com/technology/2017/aug/01/data-browsing-habits-
brokers.  
 
15
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
risk of the data not being tailored to its exact needs, thereby making 
it less useful in providing the desired predictive output.44 The largest 
American data brokers include Axciom, Corelogic, and Datalogix.45  
 
B.  Ongoing Customer Data Collection From Network-Connecting 
Software as a Service Offering 
 
The most common method of collecting training data is to 
collect data directly from users of an operator’s service. Data 
collected from consumers can be acquired in different ways: (a) web 
activity, provided when a consumer interacts with the company’s 
website; (b) consumer surveys and other feedback mechanisms; (c) 
mobile user data, provided through consumer interaction with a 
company app; and (d) social media.46 In order to obtain necessary 
rights to consumer data, the operator should include a license in its 
governing user agreement (e.g., the consumer terms and conditions 
of use) and accurately disclose the data collection and use in its 
privacy policy. We discuss obtaining rights to service user data in 
more detail in Part V. 
 
C.  Batch Uploaded Data From Software Installed On-Premises 
for Customers 
 
For customers not connected to the operator’s network 
automatically (i.e., customers that do not use a hosted or software-
as-a-service product), operators can choose to negotiate the right to 
receive a bulk package of use data through a manual upload or other 
transfer mechanism. This type of data collection most often occurs 
where the operator’s product is installed on-premise, which may be 
due to: (a) industry privacy sensitivity, for example, in the medical 
and financial sectors; (b) consumer desire for customized 
                                                                                                         
44 See, e.g., INFOBASE, https://www.acxiom.com/what-we-do/infobase 
(providing a large user database with numerous information points gathered, over 
time, in response to different requests).  
45 Mirani, supra note 42.  
46 See DEALNEWS, How Online Retailers Collect and use Consumer Data, 
CULT OF MAC (May 26, 2016) https://www.cultofmac.com/430158/how-online-
retailers-collect-and-use-consumer-data-deal-news/.  
 
16
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 233 
solutions;47 or (c) the nature of the product lends itself better to on-
site installation.48 On-premise software can involve a negotiated 
paper agreement (instead of a shrink-wrap or click-through 
agreement), so companies need to be careful that the necessary data 
rights are not negotiated out of the agreement. 
 
D.  Open Source Public Data Sets 
 
Finally, academic institutions, individual researchers, and 
‘open-source advocates’49 have created pre-populated data sets for 
common machine-learning algorithm problems. For example, the 
University of California at Irvine currently maintains 413 data sets 
that are open to the public for use in machine learning algorithms.50 
Generally, the rights to these data sets are less restrictive than one 
would find in a negotiated bilateral agreement, as open source 
licenses tend to be permissive by nature. However, operators should 
still evaluate the applicable data license terms to be aware of any 
requirements to contribute developed technology back to the open 
source community, and other requirements of the license (e.g., to 
provide attribution). Descriptions of most common open source 
licenses are maintained by the Open Source Initiative.51 
 
V. LAWS/LEGAL RISKS AROUND USE OF DATA/PII IN MACHINE 
LEARNING 
 
The legal risks of using data generally depend on the following 
                                                                                                         
47 See Thomas Peham, On-Premise vs. Cloud Software: A Comprehensive 
Comparison, USERSNAP, https://usersnap.com/blog/comparison-of-cloud-vs-on-
premise-enterprise-software/ (last visited Mar. 31, 2018). 
48 See HOST ANALYTICS, https://hostanalytics.com/blog/on-premises-versus-
cloud-based-epm-software-which-is-right-for-your-business/. 
49 Open source advocates are generally thought of as zealous individuals, 
who believe that as much of the internet and developing software as possible 
should be made open to the public. See, e.g., CBSNEWS, Oracle names Open-
Source Evengelist, CNET (Sept. 7, 2005), https://www.cnet.com/news/oracle-
names-open-source-evangelist/.  
50 See UCI MACHINE LEARNING REPOSITORY, http://archive.ics.uci.edu/ml/
index.php (last visited Mar. 31, 2018). 
51 See OPEN SOURCE INITIATIVE, https://opensource.org/ (last visited Mar. 31, 
2018). 
17
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
factors: (a) the relative sensitivity of the data; (b) the types of 
predictions to be produced; (c) the agreement governing the 
acquisition and use of the data; and (d) the impact on a broader 
industry or market.  
 
A.  Use of Sensitive Data 
 
The legal risk associated with a machine learning algorithm is 
determined, at least in part, by the sensitivity of the source data. In 
other words, if regulated data is an input, then the output is also 
likely to be regulated (or considered sensitive data of the same 
category). Sensitive data is more often regulated, and penalties for 
non-compliance with regulatory schemes for sensitive (e.g., 
personally identifiable) data often carries harsher penalties.52 In 
addition, data providers (like business-to-business operators or data 
brokers) may be more hesitant to agree to provide sensitive data that 
is subject to extensive regulations, due to their fear of being held 
accountable for misuse by a third party of data they originally 
collected.  
The primary categories of what we often consider sensitive data 
are not surprising: (a) health data; (b) financial data; (c) educational 
data; (d) location data; (e) visual data (photos of a consumer); and 
(f) data regarding children. Importantly, if an operator seeks to use 
sensitive data to make predictions within the given industry, the 
operator will fall under the purview of industry regulators.53 For 
example, if educational data is used to predict educational outcomes 
for students, or financial data is used to determine credit-worthiness, 
the resulting predictions would likely be subject to similar 
regulatory schema. 
In addition, operators may be required to handle data in a 
                                                                                                         
52 See, e.g., Legal Resources, FEDERAL TRADE COMMISSION, 
https://www.ftc.gov/tips-advice/business-center/legal-resources?type=case&
field_consumer_protection_topics_tid=250 (last visited May 10, 2018). 
53 For example, HIPAA will apply to data clearinghouses, processors, and 
clearinghouses, as well as business associates which will include most health-
software providers See Are You a Covered Entity?, CENTERS FOR MEDICARE AND 
MEDICAID SERVICES, https://www.cms.gov/Regulations-and-Guidance/
Administrative-Simplification/HIPAA-ACA/AreYouaCoveredEntity.html (last 
visited May 10, 2018). 
 
18
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 235 
specific way, or even store data for longer periods of time, based on 
the sensitivity of the industry. For example, in the health context, 
the Health Insurance Portability and Accountability Act requires 
that certain health-related data (but not all) be retained for at least 
six years.54 Particular categories of health providers are subject to 
additional retention requirements. For example, Medicare managed 
care providers must retain records for at least ten years.55 While the 
operator itself may not be a managed care provider, it may be a 
subcontractor to one who is required to be bound by the same 
retention policies. In those cases, it is common for the “covered 
entity” (i.e., the entity bound by the law) to contractually “pass 
through” certain data retention requirements under HIPAA to all of 
its subcontractors. 
 
B.  The Output Use Case 
 
Certain machine learning outputs may create undue legal risk, 
even if the data is collected in compliance with any applicable laws. 
For example, an operator’s use of data to predict a consumer’s 
credit-worthiness will result in a company being classified as a 
“Credit Reporting Agency.”56 Credit reporting agencies are subject 
to burdensome regulations.57 As another example, the use of data in 
a device to predict health outcomes can lead to a product or service 
being classified as a medical device, which is subject to regulation 
by the Food and Drug Administration, including things like fitness 
                                                                                                         
54 See Health Insurance Portability and Accountability Act of 1996, Pub. L. 
No. 104-191.  
55 42 C.F.R. § 422.504(d)(2)(iii) (2011). 
56 See Credit Reporting, FEDERAL TRADE COMMISSION, https://www.ftc.
gov/news-events/media-resources/consumer-finance/credit-reporting (last 
visited Apr 1, 2018); see also What is a credit reporting company?, CONSUMER 
FINANCE PROTECTION BUREAU (May 25, 2017), https://www.consumerfinance.
gov/ask-cfpb/what-is-a-credit-reporting-company-en-1251/.  
57 Even those who merely furnish information are subject to reporting and 
notice requirements. See Consumer Reports: What Information Furnishers Need 
to Know, FEDERAL TRADE COMMISSION, https://www.ftc.gov/tips-
advice/business-center/guidance/consumer-reports-what-information-furnishers-
need-know (last updated Mar. 2018).  
 
19
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
trackers and massage chairs.58 As discussed in Part V.A., detection 
of legal wrongdoing in these cases often does not require analyzing 
the actual data use, and can be determined solely from the resulting 
product.  
 
C.  Breach of Contract/License 
 
One of the larger areas of legal risk for operators using data in 
machine learning algorithms is the risk of non-compliance with the 
agreements under which data rights are obtained. If a company relies 
on a small number of customers for the majority of its revenue, just 
one dispute can have an enormous impact on the company, 
especially if the details of the alleged misuse are made public. Such 
an allegation, even if unfounded, could harm the company’s ability 
to attract future customers. For example, the unauthorized use of a 
customer’s data could be considered a breach of confidentiality (if 
the data is identified as being subject to confidentiality terms), 
intellectual property infringement (to the extent any intellectual 
property rights are embodied in the data), or misappropriation of 
trade secrets (depending on how the data is misused), which could 
result in breach of contract claims, claims in tort, or statutory 
damages for copyright infringement. 
Additionally, it is critical that operators relying on a few large 
enterprise customers use that data correctly (i.e., consistent with the 
data use rights in the customer license agreement). The loss of one 
large customer could destroy the viability of the algorithm.  
It is important to keep in mind, however, that private actions 
(e.g., between two private parties) to enforce violations of data use 
terms are limited by the customer’s ability to detect the operator’s 
wrongdoing. It is often difficult or impossible for a customer to 
know, or to prove, that a company uses individual data in machine 
learning algorithmic analyses. To address this information 
imbalance, new methods of detecting illegal collection and use of 
data have evolved over the last few years. For example, to uncover 
                                                                                                         
58 Given the rise of internet of things, new ways to deal with these 
devices/requirements are being explored. See FDA Selects Participants for New 
Digital Health Software Precertification Pilot Program, FOOD AND DRUG 
ADMINISTRATION (September 26, 2017), https://www.fda.gov/NewsEvents/
Newsroom/PressAnnouncements/ucm577480.htm. 
20
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 237 
Bing’s practice of copying data and functionality, Google inserted 
false hits in their search engine functionality and monitored Bing to 
see if the false stories or incorrect results also appeared in Bing’s 
results in the same order. Additionally, parties more frequently 
negotiate contractual auditing rights to allow searching for wrongful 
use of data directly in the service provider’s files.59  
 
D.  Impact on the Larger Market/Industry 
 
Finally, because widely-adapted machine learning algorithms 
are a relatively recent technological development, novel regulations 
and industry controls are being created in an attempt to police new 
concerns as they arise. Outside of the United States, the Australian 
government is looking into whether machine learning should be 
considered anti-competitive in particular use cases because it can 
create the ability to more easily base pricing off of a competitor and 
allow parties without any actual direct communication to participate 
in a tacit price fixing scheme.60  
 
VI. WHAT NEEDS TO BE CONSIDERED WHEN DRAFTING AN 
AGREEMENT FOR A MACHINE LEARNING SERVICE 
 
Different operators will rely on different license terms to obtain 
data depending on the proposed data use. First, an operator must 
determine whether it is interested in the rights to the results output, 
or just improvements to the algorithm. Second, the operator must 
determine if it is attempting to buy data or simply collect data 
through a service it is already offering. Third, the operator must 
visualize the desired machine learning output. The actual output will 
often dictate the terms of the license required to offer the machine 
learning service.  
                                                                                                         
59 See Marc Silverman, The Right to Audit Clause, WITHUM, SMITH & 
BROWN, https://www.withum.com/kc/right-audit-clause/ (last visited Apr. 1, 
2018); see also Danny Sullivan, Google: Bing Is Cheating, Copying Our Search 
Results, SEARCH ENGINE LAND (Feb. 1, 2011), https://searchengineland.com
/google-bing-is-cheating-copying-our-search-results-62914.  
60 See Tas Bindi, Big Data and Machine Learning Algorithms Could 
Increase Risk of Collusion, ZDNET (Nov. 16, 2017), 
http://www.zdnet.com/article/big-data-and-machine-learning-algorithms-could-
increase-risk-of-collusion-accc/.  
21
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
 
A.  Predictions Versus Algorithm Improvements 
 
Not all machine learning operators have the same level of 
interest in using the results of an algorithm in future work. Some 
operators are intimately interested in the accuracy of the result, but 
not the result itself. For example, a marketing platform that predicts 
whether an individual will click on an image with particular 
attributes will not care about whether the consumer goes on to buy 
the linked product. Instead, it cares only about which attributes the 
image contains and whether the attributes had the predicted effect 
(i.e., caused the consumer to click the link). The relevant data are 
image attributes and the user’s “clicks,” rather than the customer’s 
content. In contrast, a medical imagery predictive algorithm would 
want to know if its software successfully or unsuccessfully predicted 
the presence of a medical condition, and all of the specific outcomes 
that were or were not correctly predicted. As a result, that operator 
would need a license to obtain more specific data about each 
diagnosis.  
 
B.  Source of Data 
 
As discussed in Part IV, some consumer-facing companies offer 
data-gathering services and data can also be obtained through 
wholesale acquisitions of databases. Data gathered through 
negotiated agreements with customers can vary depending on: (a) 
whether the company is business-to-business (“B2B”) or business 
to consumer (a business providing a service to an individual 
consumer) (“B2C”); (b) industry norms and data sensitivity; and (c) 
customization of the product and algorithm.61 Operators should be 
cognizant of the different rights negotiated with each customer, and 
maintain minimum acceptable terms to avoid violation of customer 
agreements. By contrast, purchased data generally has fewer 
limitations which may only restrict the purchaser from specific high-
risk activities, like predicting credit-worthiness or re-identifying 
                                                                                                         
61 See Daniel Glazer et al., License Scope and Restrictions and Original 
versus Derived Data, available at https://us.practicallaw.thomsonreuters.com/4-
532-4243. 
 
22
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 239 
individuals.62  
 
C.  Output 
 
Finally, both public perception and potential legal consequences 
of machine learning data use are dependent on the final output of the 
algorithm. Consider the medical industry. Given the public interest 
in improving and refining medical care, consumers may be more 
likely to allow companies to use their data to develop software that 
will diagnose a specific ailment based on individual attributes. The 
customers themselves have a stake in the result and thus less 
resistant to sharing their data. However, information about personal 
health is highly sensitive. Consumers may be willing to allow the 
use of their data, but only if it is anonymized. An operator should be 
aware that in some cases, it is far more likely to get the data sets it 
needs if it promises to protect the consumer’s identity.63  
 
D.  Recommendations for Drafting 
 
When drafting an agreement to acquire data for use in a machine 
learning algorithm, there are several aspects of the license one 
should consider. This Section discusses a number of considerations 
for data licenses, including: (1) license duration; (2) ownership of 
created output; (3) requirement for data to be provided in a de-
identified/non-sensitive format; (4) combining data with other data 
sets; and (5) promises that data is gathered in accordance with 
applicable law. 
 
                                                                                                         
62 As an example, Acxiom states that data sets from their site: “contain 
information on individuals and households in the U.S. and are developed from 
many sources, including public records, publicly available information, and data 
from other information providers. Acxiom’s marketing products are used by 
qualified companies, non-profit organizations and political organizations in their 
marketing, fundraising, customer service and constituent service and outreach 
programs to provide customers and prospects with better service, improved 
offerings and special promotions.” Highlights for US Products Privacy Policy, 
ACXIOM.COM, https://www.acxiom.com/about-us/privacy/highlights-for-us-
products-privacy-policy/ (last visited Apr. 20, 2018). 
63 These promises could, of course, expose the operator to significant legal 
risk if they are broken. 
23
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
1. License Duration 
 
A data license should not be time-limited. This is particularly 
important if the algorithm makes continuing reference to source 
data. If the license itself cannot be perpetual, then the operator 
should retain perpetual rights to any improvements or derivative 
works of the data so that the effectiveness of the algorithm is not 
diminished.  
If an operator must agree to a time-limited license that requires 
the return of data, then it should be aware how difficult it can be to 
identify exactly which machine learning result is attributable to a 
specific data set or individual piece of data. The model should 
improve and evolve with each new data set added. Therefore, the 
ideal data license will be perpetual, notwithstanding termination of 
the underlying agreement.  
Additionally, an operator must be aware that a large enterprise 
customer could insist that a data license be revocable in the event of 
an operator’s breach of the underlying agreement. If the license were 
revoked, the operator would likely be required to return all data. As 
discussed, that can be an incredibly cumbersome task to undertake. 
As a result, it is critical for the operator to ensure compliance with 
its data license agreements to avoid a license revocation that 
compromises the algorithm. Concerns about time limitations in a 
license are less of an issue with data licensed from data brokers, as 
data brokers often grant perpetual licenses.  
 
2. Ownership of Created Output 
 
Ownership of the output of a machine learning algorithm is 
another important consideration. Enterprise customers, particularly 
those with negotiating leverage, will often attempt to claim that any 
technology, intellectual property, or other output developed by 
referencing their original data belongs to them. That approach is 
reasonable in a consulting arrangement with a defined project scope, 
but not necessarily in the machine learning context, where the 
operator continuously uses its customers’ data to offer an improved 
product to every current and future customer.  
Therefore, it is critical that the operator maintains ownership of 
its algorithm, as well as the improvements to the algorithm 
24
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 241 
generated based on its customers’ data in order to protect the 
operator’s key intellectual property. As a fallback position, the 
operator could attempt to transfer ownership of any custom 
developed features for the specific client or consumer-data reliant 
improvement if: (a) that improvement or model alone is unusable by 
the customer in any context other than the operator’s algorithm; and 
(b) the operator is granted a perpetual, unlimited, royalty-free, 
sublicensable license to the developed model or improvements for 
use in its products and services.  
 
3. Requirement for Data to be Provided in a De-Identified/Non-
Sensitive Format 
 
Machine learning operators often do not want to assume the risk 
of hosting a platform which produces predictions that could 
inadvertently reveal an individual’s personally identifiable 
information (“PII”). If the operator gathers data from customers, it 
must ensure that customers strip their data of any PII or otherwise 
take on the risk of removing PII. Some enterprise customers, on the 
other hand, may refuse to provide any PII and will agree to represent 
that no PII is included in their data sets. Data brokers may also agree 
to similar terms, or undertake removal themselves. In any event, the 
customer’s privacy policy (if it is required to have one) should 
ensure that the customer has the right to provide the data to the 
operator. The operator can then ask the customer to represent and 
confirm that all data is provided in compliance with the privacy 
policy.  
 
4. No Prohibition on Combining Data With Other Data Sets 
 
Machine learning algorithms, by their nature, improve with 
exposure to more and more data, regardless of the source. If data is 
collected in bulk from an external source, any prohibition on 
commingling that data with data from other sources undermines the 
usefulness of that data set. This issue often arises when purchasing 
data from data brokers, who may have negotiated no commingling 
provisions with their providers that are passed on to purchasers of 
the data. An operator could address this issue in its agreement with 
a data broker by agreeing that there will be no commingling that 
25
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
results in the identification of individuals or that connects PII to an 
anonymized/de-identified data set.  
Obtaining the rights to combine data sets can be especially 
important since demonstrating compliance with a contractual 
requirement to keep data sets separate can be nearly impossible. 
Certain aspects of data may be present in multiple data sets, and 
machine learning output may be reliant on multiple data sets, so 
showing that particular data came from one source and not another 
is not feasible.  
 
5. Representation That Data was Gathered in Accordance With 
Applicable Law 
 
Finally, when obtaining data from an external data source, a 
machine learning operator will have little control over how the data 
was originally gathered, and very little insight as to whether the 
collection complied with applicable law. As such, the operator must 
rely on the representations and warranties of its data providers as to 
the legality of the data, and should ensure that the applicable 
representations and warranties are in the underlying data agreement. 
The operator should insist on these representations and warranties 
and refuse to deal with any provider that will not agree to them.  
 
CONCLUSION 
 
While the concept of machine learning is not new, the ubiquity 
of machine learning applications has seen a significant upswing over 
the past five to ten years. In the legal sector, drafting appropriate 
license language and associated data use rights for machine learning 
applications requires lawyers to understand what exactly machine 
learning is and how it differs from traditional software licensing or 
service provider scenarios. The most important point to take into 
consideration when drafting a machine learning license is that all 
data use is not created equal.  How data is gathered, processed, and 
stored will depend on the type of machine learning model and the 
goals of the organization using the data. Therefore, to appropriately 
draft a license, attorneys should examine the data cycle with their 
client to understand how data will be gathered, processed, stored, 
and retained. The specifics of the data type, use, processing and 
26
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
2018] HOW MACHINES LEARN 243 
storage will affect a multitude of legal and contractual issues 
relevant to the data use license itself, including, but not limited to, 
breadth of license, data use timeframe, and handling of derivatives. 
Attorneys should also take into consideration sensitivity of data use, 
collection and retention within a given industry, as well as factors 
such as consumer perception and the machine learning algorithms’ 
output to help them better advise clients on the “real-world” risks of 
using different types of data in their business.  
 
PRACTICE POINTERS 
 
§ License duration (term of the agreement versus perpetual): 
Understand how long the company needs to refer back to the 
data (including whether data will be needed for fixing later-
discovered flawed outcomes) and whether the data can be 
separated from the algorithm without affecting functionality.  
§ Ownership of created output (customer-owned or company-
owned): Understand whether output is customer specific or 
increases the value of the algorithm as a whole, and whether 
the algorithm using training data continues to process 
improvements from both old and company-created data 
inputs.  
§ Data Identifiability (anonymous versus individual 
characteristics): Understand which data is likely to be used 
as a predictor, and whether anonymization of data would 
affect the ability to create valuable output. Additionally, 
consider the federal and state statutes applicable to the type 
of data processed by the algorithm (e.g., HIPPA for health-
related data).   
§ Data Set Combination (allowed or prohibited): Understand 
whether data-set combination is likely to re-identify 
personally identifiable information regarding individual data 
subjects, and which attributes of a data set need to be 
correlated with to produce valuable output.  
§ Responsibility for gathering data in compliance with law 
(company versus outside data source): If data is gathered in 
bulk from an outside source (including from a data broker, a 
27
Wilka et al.: How Machines Learn: Where Do Companies Get Data for Machine Learn
Published by UW Law Digital Commons, 2018
 WASHINGTON JOURNAL OF LAW, TECHNOLOGY & ARTS  [VOL. 
13:3 
white-labeled incorporation of the algorithm, or an open 
source set), the outside party should bear primary 
responsibility for gathering the data in compliance with law. 
For data gathered directly from a customer, the company will 
likely bear primary responsibility for informing the 
consumer and obtaining consumer consent. For data 
gathered from the internet (via webscraping or other similar 
techniques) without the express consent of the data source, 
the attorney should analyze whether such data collection (1) 
violates law, or (2) violates online terms of service 
agreements, and the attorney and company should together 
conduct a risk-benefit analysis of such data collection.  
 
28
Washington Journal of Law, Technology & Arts, Vol. 13, Iss. 3 [2018], Art. 2
https://digitalcommons.law.uw.edu/wjlta/vol13/iss3/2
",267982567,"{'doi': None, 'oai': 'oai:digitalcommons.law.uw.edu:wjlta-1274'}",How Machines Learn: Where Do Companies Get Data for Machine Learning and What Licenses Do They Need?,"{'code': 'en', 'name': 'English'}",2018-04-01T07:00:00+00:00,UW Law Digital Commons,[],['https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?article=1274&amp;context=wjlta'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/267982567.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/267982567'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/267982567/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/267982567/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/267982567'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/267982567?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=6&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Machine learning services ingest customer data in order to provide refined, customized services. Machine learning algorithms are increasingly prominent in multiple sectors within the software-as-a-service industry including online advertising, health diagnostics, and travel. However, very little has been written on the rights a company utilizing machine learning needs to obtain in order to use customer data to improve its own products or services. Machine learning encompasses multiple types of data use and analysis, including (a) supervised machine learning algorithms, which take specific data provided in a tagged and classified format to deliver specific predictable output; and (b) unsupervised machine learning algorithms, where untagged data is processed in  order to look for patterns and correlations without a specified output. This Article introduces the reader to the types of data use involved in various machine learning models, the level of data retention normally required for each model, and the risks of using personal information or re-identifiable data in connection with machine learning. The paper also discusses the type of license a commercial provider and consumer would need to enter into for various types of machine learning software. Finally, the paper proposes best practices for ensuring adequate rights are obtained through legal agreements so that machines may self-improve and innovate","['text', 'Computer Law', 'Science and Technology Law']",disabled
,"[{'name': 'Fiebrink, Rebecca'}, {'name': 'Gillies, Marco'}]",[],2018-04-30T15:27:05+00:00,"{'name': 'Goldsmiths Research Online', 'url': 'https://api.core.ac.uk/v3/data-providers/50'}",,,10.1145/3205942,https://core.ac.uk/download/156876069.pdf,"Introduction to the Special Issue on Human-Centered
Machine Learning
REBECCA FIEBRINK, Goldsmiths, University of London, UK
MARCO GILLIES, Goldsmiths, University of London, UK
Machine learning is one of the most important and successful techniques in contemporary computer science.
Although it can be applied to myriad problems of human interest, research in machine learning is often framed
in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides
considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in
the first place, and using the outcomes of machine learning in the real world. Examining machine learning
from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine
learning workflows based on situated human working practices, and exploring the co-adaptation of humans
and intelligent systems. A human-centered understanding of machine learning in human contexts can lead
not only to more usable machine learning tools, but to new ways of understanding what machine learning is
good for and how to make it more useful. This special issue brings together nine papers that present different
ways to frame machine learning in a human context. They represent very different application areas (from
medicine to audio) and methodologies (including machine learning methods, HCI methods, and hybrids), but
they all explore the human contexts in which machine learning is used. This introduction summarizes the
papers in this issue and draws out some common themes.
CCS Concepts: • Human-centered computing; • Computing methodologies→Machine learning;
Additional Key Words and Phrases: human-centered machine learning, interactive machine learning
1 INTRODUCTION
Machine learning research often centers on impersonal algorithmic concerns, removed from human
considerations such as usability, intuition, effort, and human learning; it is also too often detached
from the variety and deep complexity of human contexts in which machine learning may be
ultimately applied. However, considerable human work is always a part of gathering training data,
tuning algorithms, and integrating machine learning into real-world systems. And human values,
goals, and social structures always underpin decisions about what should be modeled in the first
place.
It is our position that examining machine learning from a human-centered perspective includes
explicitly recognizing both human work and the human contexts in which machine learning is
used. Human-centered machine learning thus involves aligning algorithms and systems to human
goals and capabilities, creating hybrid human-machine systems capable of achieving better results
than either humans or algorithms working alone, and designing and evaluating machine learning
systems using human-centered methods. A human-centered approach to machine learning demands
making machine learning more usable and effective for a broader range of people, and designing
new systems in full recognition of the agency and complexity of human users—including both
people employing machine learning and those using or being affected by systems driven by machine
learning.
Authors’ addresses: Rebecca Fiebrink, Goldsmiths, University of London, London, SE14 6NW, UK, r.fiebrink@gold.ac.uk;
Marco Gillies, Goldsmiths, University of London, London, SE14 6NW, UK, m.gillies@gold.ac.uk.
2018. This is the author’s version of the work. It is posted here for your personal use. Not for redistribution. The definitive
Version of Record was published in ACM Transactions on Interactive Intelligent Systems.
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
2 Fiebrink and Gillies
This special issue of TiiS follows a CHI 2016 workshop on Human-Centered Machine Learning
[Gillies et al. 2016]. Our motivation for that workshop, and for this special issue, is to highlight
research that employs human-centered approaches to the conception, design, and evaluation of
machine learning systems. The papers presented in this issue illustrate how human-centered
practices can be employed to make systems that are more usable and useful, and how a deeper
understanding of human practices and contexts can ultimately lead to machine learning systems
that have greater impact and address a wider variety of human concerns.
The papers presented in the following pages describe applications of machine learning to a wide
span of human endeavors such as medical assessment, qualitative coding in the social sciences,
audio annotation, creation of movemnet-sound interactions, and product recommendation. The
research presented herein addresses a variety of human-relevant goals: from identifying and
handling ambiguity in data to improving the accuracy and usability of machine learning systems
to supporting human decision-making.
Human-centered machine learning brings together machine learning and human-computer
interaction (HCI), two fields that have very different methodologies. Machine learning research
works with pre-existing, often standardized datasets, using standard measures such as accuracy,
precision and recall (for example the paper by Smith et al. in this issue), while HCI works directly
with people, using quantitative or qualitative user studies (e.g., papers in this issue by Zeitz-Self et
al. or Kim and Pardo). Crowd computing methods as used by Chen et al., Dumitrache et al. and
Zhang et al. in this issue sit in between the two: while human-generated data underpins this work,
the data is often closely integrated into a machine learning process (e.g., used as training data).
What are we to make of the diversity of methodologies in this special issue? Should we conclude
that human-centered machine learning is not yet a mature discipline because it does not have
its own established methodology? Should we attempt to combine machine learning and HCI
methods into a common methodology? Or should we celebrate this methodological diversity?
Should human-centered machine learning remain, as Blackwell suggested of HCI [Blackwell 2015],
an “inter-discipline” that consciously draws strength and creativity from the different disciplines
that feed into it? These are just some of the questions we hope to spark with this special issue.
Below, we provide a brief summary of each paper selected for inclusion in this issue, accompanied
by our own thoughts about how each paper contributes to an understanding of what human-
centered machine learning can look like.
2 PAPERS IN THIS SPECIAL ISSUE
2.1 A Review of User Interface Design for Interactive Machine Learning
Dudley and Kristensson present an overview of prior research in one of the most important
approaches to Human-Centered Machine Learning: Interactive Machine Learning (IML), and in
particular user interface design for IML. They authors define IML as follows:
Interactive Machine Learning is an interaction paradigm in which a user or user group
iteratively trains a model by selecting, labeling and/or generating training examples to
deliver a desired function.
IML is highly relevant to any discussion of human-centered machine learning, as it places
particular importance on human interactions with the machine learning process, and much IML
research combines approaches to system design and evaluation from human-computer interaction
with machine learning techniques. This paper provides a valuable contribution by helping define
and understand what is now becoming an established research area. It synthesizes the prior research
by defining a set of elements common in most IML interfaces (sample review, feedback assignment,
model inspection and task overview); describing a generalized workflow for IML (feature selection,
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
Introduction to the Special Issue on Human-Centered Machine Learning 3
model selection, model steering, quality assessment, termination assessment and transfer); and
outlining a number of emergent solution principles for the challenges of IML (Make task goals and
constraints explicit, Support user understanding of model uncertainty and confidence, Capture
intent rather than input, Provide effective data representations, Exploit interactivity and promote
rich interactions and Engage the user). This synthesis provides guidance for future research in IML
as well as a language in which to talk about and compare IML solutions.
2.2 Using Machine Learning to SupportQualitative Coding in Social Science: Shifting
The Focus to Ambiguity
Chen et al. present work about the use of machine learning for the social sciences and begin their
paper with a broad overview of the challenges involved. Social scientists deal with data sets that
are complex, heterogeneous and often large. Machine learning has the potential to speed and
support research processes with such data. However, the authors draw our attention to a number
of mismatches between the machine learning experts’ understanding of data analysis and the
understanding of social scientists. Machine learning and social science not only have different
technical methods, but also often have different goals and philosophies. For instance, social science
often employs a theory- and hypothesis-driven approach to data, while machine learning typically
deals with data that is gathered and analyzed without reference to a theoretical framework (or
more worrying, with a theoretical framework that is entirely implicit and unstated even by the
researcher). This is one of many important issues raised by the paper, which shows us the difficulties
of applying machine learning to the processes of an established discipline and of communicating
across disciplines.
This paper looks particularly at the use of machine learning for the qualitative data analysis
practice of “coding”, which the authors describe as “a process of arranging qualitative data in a
systematic order by segregating, grouping and linking it in order to facilitate formulation of meaning
and explanation”. Chen et al. identify ambiguity as a key challenge in the coding process (it is also an
important theme of other papers in this special issue). Some of the textual data may be ambiguous
in terms of its meaning and therefore the appropriate code. This can result in human coders being
more uncertain of their code as well as in disagreement between coders. In a Mechanical Turk
study, the authors found that non-expert crowd workers were consistent with experts in their
judgment of ambiguity of data and that ambiguity was likely to result in disagreement between
both expert and non-expert coders.
2.3 Predicting User’s Confidence During Visual Decision Making
Smith et al. address a key problem raised by many papers in this special issue: humans providing
training data for machine learning systems may be acting with uncertainty, leading to training
data that is inaccurate or inconsistent. Information about human uncertainty has the potential to
improve machine learning outcomes, for instance by reducing reliance on labels that may not be
correct. However, obtaining information about uncertainty can be difficult: while it is possible to
ask people how confident they are, this can impose a large overhead of human time and effort, and
making confidence judgments can itself be a difficult task.
Smith et al. address these problems by using machine learning to estimate users’ confidence at a
visual decision making task (similar to a labeling task). Rather than explicitly asking users about
their confidence, the researchers use implicit information gathered during the task: specifically, the
user’s eye gaze patterns while performing the task. The paper presents a novel representation of
gaze data over time as a 2D image. This data is amenable to learning techniques that are commonly
used for image data, in this case convolutional neural networks. This paper demonstrates the
effectiveness of using implicit information for understanding user confidence and points to future
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
4 Fiebrink and Gillies
research that could make measures such as gaze tracking an integral part of a human-centered
machine learning workflow.
2.4 Crowdsourcing Ground Truth for Medical Relation Extraction
Dumitrache et al. study the use of crowd workers to label medical texts. The workers’ goal is to
identify relations between terms within the text; for example, the sentence “fever induces dizziness”
demonstrates the relation “causes” between the terms “fever” and “dizziness”. Relation extraction
of this type is a key data mining problem and one that, like many machine learning tasks, requires
large quantities of labeled training data. Having medical experts label the data is expensive if it is
possible at all, given these experts’ time constraints, so using crowd-sourcing to collect ground
truth annotations on which a machine learning system can be trained is an attractive alternative.
Like work by Chen et al. and Smith et al. in this special issue, a key issue in this paper is
recognizing and dealing appropriately with ambiguity in the human labeling task. Dumitrache et al.
found that many sentences in their corpus were ambiguous, and this was reflected in disagreement
between crowd workers (in some cases, in sentences for which the authors themselves could not
resolve the ambiguity). Rather than discarding instances for which crowd workers disagree or
attempting to resolve the ambiguity to a single “correct” label as most researchers do, they actively
make use of the ambiguity by weighting labels based on the agreement between crowd annotators.
The authors then compare a medical relation extraction classifier trained on the weighted, crowd-
supplied labels to a classifier trained on expert-provided labels, as well as to a fully automated
baseline method. The results show that the crowd-labeled data approach was competitive with the
results of experts and outperformed the automated approach.
2.5 Visualizing Ubiquitously Sensed Measures of Motor Ability in Multiple Sclerosis:
Reflections on communicating machine learning in practice
Morrison et al. explore the use of machine learning to aid clinical decision making, specifically in the
context of measuring human motor ability in the assessment of multiple sclerosis. Computer-based
sensing systems and machine learning have the potential to improve on human assessments of
motor ability. However, making such systems useful in clinical practice requires that algorithms
provide more information than a simple assessment score: clinicians work in a complex decision-
making landscape, in which they must integrate information about algorithmic assessment with
their own knowledge, as well as with collaborative assessment with human colleagues.
This paper describes work to understand how visualization can be used to aid in the inter-
pretability of machine learning systems for multiple sclerosis assessment. In an application of HCI
methods to machine learning system design, the authors use a series of design iterations with
clinicians to arrive at a better understanding of the challenges and user needs in this space. These
reveal that simply making the algorithmic decision-making process more transparent to users is
insufficient for supporting human decision-making. On the other hand, visualization can reveal
useful aspects of the algorithmic decision-making context, including information about data quality
and relationships within the data that are interpretable by and relevant to clinicians.
The contributions of this paper are of interest far beyond medical assessment. For instance,
the method of employing a series user-centered design workshops with domain experts can be
applied to better understand design challenges and principles for manymachine learning application
contexts. Further, the authors present a compelling argument against treating the design of machine
learning algorithms and the design of user-facing applications as distinct tasks that can be done
independently.
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
Introduction to the Special Issue on Human-Centered Machine Learning 5
2.6 A Human-in-the-loop System for Sound Event Detection and Annotation
Kim and Pardo leverage machine learning’s ability to learn from human-provided examples not
to replace a human annotator, but to speed up human annotation. They focus on the application
domain of annotating sound files to indicate the locations and durations of sounds of interest (e.g.,
spoken words, musical instruments, environmental sounds). Doing such annotation manually is
labor-intensive, yet training a machine learning algorithm to accurately perform this annotation
can require many examples (possibly more example sounds than exist in the corpus of interest),
and may still yield insufficient accuracy.
Kim and Pardo thus use an iterative human-in-the-loop approach in which a small number of
annotations provided by a person inform a nearest-neighbor-based relevance weighting, which
identifies the unlabeled regions of audio most likely to contain the sound of interest. The human
annotator adjusts the machine-generated labels on these regions if necessary, the relevance weight-
ing is updated, and the process repeats with the next set of machine-suggested annotation regions.
In the authors’ evaluation with human annotators, this approach yielded a two-fold speed-up in
completing annotation of sound files, compared to manual annotation alone.
This paper demonstrates how understanding the user’s interaction capabilities and true goal
can lead to a different formulation of the machine learning component of a system than one might
expect. If the user’s goal were to create a truly automatic annotation system, for instance, a more
conventional active learning approach (in which the system recommends the user label data the
algorithm finds ambiguous or informative) may have been appropriate. Instead, though, this system
focuses on requesting labels for the audio segments that it most confidently believes are examples
of the target sound, in order to most efficiently direct the user’s attention to these areas.
The paper’s evaluation of this approach is notable in its differentiation between the time incurred
by the requirement that the user listen to the audio recommended for labeling by the algorithm
(time which would be shortened, for instance, by a more accurate algorithm), and the extra time
incurred by human interactions with the interface (the “interaction overhead” which might be
improved, for instance, by a better user interface). By measuring the interaction overhead incurred
by people using a particular interface, the time to label a new corpus (or the time to annotate using
a different algorithm) with this interface can thus be estimated using a simulation experiment
rather than additional user studies.
2.7 Evaluation and Refinement of Clustered Search Results with the Crowd
Zhang et al. describe a hybrid human-machine approach for clustering search results. While
automatically clustering items can help people navigate and understand large datasets, clustering
algorithms do not always group items in ways that people find most coherent or useful. This is the
case for the application domain considered by Zhang et al., who focus on clustering of search results
returned for a user’s query in the Google Play Store. One approach to improving the usefulness of
these machine-generated clusters of search results is to have human experts manually refine them,
but this does not scale to large numbers of search queries. Zhang et al. show that crowd-sourcing
the task of refining clusters can produce good clusters in a more scalable way.
Their solution combines algorithmic and crowd-driven components. Multiple clustering algo-
rithms are applied in parallel to the search results, due to the observation that different algorithms
may work well for different types of searches. Crowd workers then evaluate these alternative
clusterings, refine the best, and assign good titles to the final clusters. The implementation of
crowd-driven components of the system thus entails both the decomposition of the overall task
of improving clusters into small tasks that can be assigned to individual workers, as well as the
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
6 Fiebrink and Gillies
design of user interfaces that enable workers without specialized expertise to easily and accurately
perform these tasks.
2.8 Observation-Level and Parametric Interaction for High-Dimensional Data
Analysis
A number of human-in-the-loop systems for exploratory data analysis or model-building use one
(or both) of the following interaction strategies: user manipulation of model parameters, or user
manipulation of data from which new model parameters are inferred. Zeitz-Self et al. propose that
these two strategies support different types of human tasks. They explore both strategies in the
context of a data visualization system that uses weighted multidimensional scaling to project a
dataset with multiple attributes into a two-dimensional visualization. This system allows users to
directly manipulate the attribute weights used in the distance metric underlying the projection, or
to move data points within the visualization to demonstrate an implied distance relationship (for
which weights are then inferred). A controlled user study revealed that manipulating weights and
manipulating data points supported different types of analytical activities, such as attribute-based
filtering and observation comparison, respectively.
Notably, this paper considers a type of interactive task in which the goal is not to identify the
“right” model weights for a particular dataset or application. Rather, the goal is to use exploratory
manipulation of an underlying model in conjunction with a dynamic visualization of the current
model to better understand the data. As in the paper by Kim and Pardo, then, the primary purpose
of machine learning here is not to build a reusable model or replace human judgment, but to aid in
a human task. Here, this task is understanding data. Machine learning is well placed to help with
this task, but it is notable that improved human understanding does not result from the successful
output of the machine learning process. Rather, understanding is facilitated by iterative interactions
with the data and the model. This opens up an interesting possibility that, in interactive machine
learning, human interactions and learning can be ends in themselves.
2.9 Motion-Sound Mapping through Interaction An Approach to User-Centered
Design of Auditory Feedback using Machine Learning
Françoise et al. apply machine learning to gestural interaction design. This follows a recent trend
in human computer interaction to look to new modes of interaction that are based on fuller body
movements than a traditional mouse and keyboard or touch screen. They look at movement as a
means of controlling sound synthesis, based on the insight that humans naturally interact with
sound, and particularly music, with body movement such as dance or foot tapping. Their system
allows end users to design mappings between movement and sound. Machine learning makes it
possible to define complex, non-linear mappings by demonstrating example.
Françoise et al. use machine learning as a tool for interaction design. They address the challenge
that traditional rapid prototyping approaches used in HCI, such as paper prototypes and wireframes
are not well suited to movement based interaction. These techniques focus on the visual design
of interfaces on assumption that the physical actions of the user will be relatively standard. For
movement interaction, on the other hand the focus is on people’s movements, not visual displays,
particularly on the audio based application in this paper, where a visual interface can be entirely
missing. Another challenge is that movement interaction is an embodied skill in the sense that
we know how to dance or gesture by doing, rather than explicitly in an intellectual or symbolic
form. This makes it hard to explicitly define gestures, for example in code, because know by doing,
without being able to explain how we do it. Interactive machine learning makes it possible to
prototype gestural interfaces by demonstrating movements themselves, rather than on paper or
in symbolic form. Françoise et al. have developed an approach called mapping through interaction
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
Introduction to the Special Issue on Human-Centered Machine Learning 7
in which users demonstrate examples of interactions with sound, and these examples are used to
train a machine learning model.
The authors used a mixed approach to evaluation methods. They have a qualitative study in
which users could interact with their system as part of a game. This allowed an HCI based evaluation
of user experience. This HCI approach was supplemented with an off-line, machine learning style
evaluation in which the accuracy of different machine learning algorithms was compared with a
standard gesture data set. This ability to work across different disciplinary methodologies is likely
to be an important part of Human-Centered Machine Learning research in future.
3 CONCLUSION
As we have seen, the papers in this special issue represent many different ways to bring human
considerations into the creation and use of machine learning systems. Perhaps this is the most
interesting outcome of this special issue: human-centered machine learning is not a single approach,
but a wide diversity of problems, methods, technologies and theories, all of which could, and should,
be explored by researchers for years to come. As machine learning moves out of the research lab
and into more real-world systems, the question of how to ensure that these systems are usable
and useful for people becomes increasingly urgent. We hope the papers in this special issue will
help inform readers as they contemplate where and how we might continue to make machine
learning research more human-centered, foregrounding human goals and experiences in both the
development and application of new machine learning technologies.
ACKNOWLEDGMENTS
The authors would like to thank the co-organizers and attendees of the CHI 2016 workshop on
Human-Centered Machine Learning [Gillies et al. 2016] for their initial work articulating a vision for
human-centered machine learning and building a community around this topic. We are indebted to
TiiS Editor-in-Chief Michelle Zhou and assistant Anbang Xu for their guidance in the preparation of
this special issue, and we are grateful for the hard work and thoughtful feedback of the anonymous
peer reviewers who assisted with the paper selection and revision process. Finally, we would like to
thank the authors whose hard work has made this special issue as diverse and stimulating as it is.
REFERENCES
Alan F. Blackwell. 2015. HCI As an Inter-Discipline. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on
Human Factors in Computing Systems (CHI EA ’15). ACM, New York, NY, USA, 503–516. https://doi.org/10.1145/2702613.
2732505
Marco Gillies, Rebecca Fiebrink, Atau Tanaka, Jérémie Garcia, Frédéric Bevilacqua, Alexis Heloir, Fabrizio Nunnari, Wendy
Mackay, Saleema Amershi, Bongshin Lee, Nicolas d’Alessandro, Joëlle Tilmanne, Todd Kulesza, and Baptiste Caramiaux.
2016. Human-centred machine learning. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors
in Computing Systems. ACM, 3558–3565. https://doi.org/10.1145/2851581.2856492
Pre-print for Introduction to TiiS Special Issue on Human-Centred Machine Learning
",156876069,"{'doi': '10.1145/3205942', 'oai': 'oai:eprints.gold.ac.uk:23259'}",Introduction to the Special Issue on Human-Centered Machine Learning,,2018-07-01T00:00:00+00:00,'Association for Computing Machinery (ACM)',[],['https://research.gold.ac.uk/23259/1/HCMLIntro.pdf'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/156876069.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/156876069'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/156876069/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/156876069/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/156876069'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/156876069?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=3&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine papers that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, HCI methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the papers in this issue and draws out some common themes",[],disabled
,"[{'name': 'Nafea, Ibtehal Talal'}]",[],2020-05-07T20:02:56+00:00,"{'name': 'IntechOpen', 'url': 'https://api.core.ac.uk/v3/data-providers/15104'}",,,10.5772/intechopen.72906,https://core.ac.uk/download/322434405.pdf,"Selection of our books indexed in the Book Citation Index 
in Web of Science™ Core Collection (BKCI)
Interested in publishing with us? 
Contact book.department@intechopen.com
Numbers displayed above are based on latest data collected. 
For more information visit www.intechopen.com
Open access books available
Countries delivered to Contributors from top 500 universities
International  authors and editors
Our authors are among the
most cited scientists
Downloads
We are IntechOpen,
the world’s leading publisher of
Open Access books
Built by scientists, for scientists
12.2%
122,000 135M
TOP 1%154
4,800
Chapter 9
Machine Learning in Educational Technology
Ibtehal Talal Nafea
Additional information is available at the end of the chapter
http://dx.doi.org/10.5772/intechopen.72906
© 2016 The Author(s). Licensee InTech. This chapter is distributed under the terms of the Creative Commons 
Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, 
and reproduction in any medium, provided the original work is properly cited. 
Ibtehal Talal Nafea
Additional information is available at the end of the chapter
Abstract
Machine learning is a subset of artificial intelligence (AI) that helps computers or teaching 
machines learn from all previous data and make intelligent decisions. The machine-learning 
framework entails capturing and maintaining a rich set of information and transforming it 
into a structured knowledge base for different uses in various fields. In the field of educa-
tion, teachers can save time in their non-classroom activities by adopting machine learning. 
For example, teachers can use virtual assistants who work remotely from the home for their 
students. This kind of assistance helps to enhance students’ learning experience and can 
improve progression and student achievement. Machine learning fosters personalized learn-
ing in the context of disseminating education. Advances in AI are enabling teachers to gain 
a better understanding of how their students are progressing with learning. This enables 
teachers to create customized curriculum that suits the specific needs of the learners. When 
employed in the context of education, AI can foster intelligence moderation. It is through this 
platform that the analysis of data by human tutors and moderators is made possible.
Keywords: machine learning, artificial intelligence education, virtual assistant education 
sector
1. Introduction
Currently, technology is everywhere including the education sector, where it has proven to be 
of great importance for realizing the learning outcomes for students. Education is no longer just 
the teaching of text or requiring the student to memorize manuscripts. The instructional process, 
both inside and outside of the classroom, has become an activity with measurable goals and 
results. Over time, educational techniques have turned out to be a dynamic part of the inputs 
and outputs of the learning process. Moreover, these practices have grown into a vital part that 
plays a significant role in broadening the advancement of the components of the learning system, 
upgrading the rudiments of the curriculum, and making both more effective and resourceful. 
© 2018 The Author(s). Licensee IntechOpen. This chapter is distributed under the t rms of the Crea ive
Comm ns Attribution Lic nse (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use,
distribution, and reproduction in any medium, provided the original work is properly cited.
These components are used in the process of planning, implementing, evaluating, following-up 
and developing objectives [1]. Machine learning has become a new frontier for higher educa-
tion. Being one of the strongest newer technologies, machine learning plays the main rules in 
artificial intelligent and human interaction. Machine learning is the innovative tool being used 
to combat cancer, climate change, and even terrorism [2]. It is the new infrastructure for every-
thing. Consequently, machine learning helps computers to find hidden insights without being 
programmed to do so. Moreover, machine learning works as a good predictive.
• In this chapter, machine learning technology is used as a principle of educational activi-
ties. There are different ways of using machine learning technology in education, such as in 
providing diverse learning options so a learner can discover what suits him/her best but in 
a manner where all individual variances between pupils are considered. Machine learning 
can also be used in review a lesson that was hard to understand [1]. Machine learning in ed-
ucation works in harmony with students’ needs, and at a time and place that suits them best.
• Virtual assistance plays a crucial role in education and is a good forum for machine learn-
ing use. A virtual assistant can interact in a conversation with students [1]. This interaction 
involves conversational agents who assist students by using an application or website. The 
process works is quite simple with the student needing to input text. On the other hand, the 
agents execute the task and determine the appropriate response to the input before provid-
ing an easy response that the student can easily understand.
• Both machine learning and virtual assistants are used to interpret patterns and human in-
teraction which supports deeper learning and provides users with fast and accurate data. 
This chapter proposes a new education framework that is powered by virtual assistance. 
It provides customized research for students. The suggested framework allows teachers to 
monitor their students’ progress through their learning activities at any time. This is the 
best approach to training students to enhance their experience. The framework also helps 
teachers save time that is normally spent in preparing lectures, creating exams, document 
review, document creation, and light specific research. The proposed framework facilitates 
the leveraging of the most powerful technologies in improving the quality of education for 
both student and teacher. Another advantage associated with this framework of machine 
learning and virtual assistants is that it is less prone to the errors that usually encumber 
human operations. If an error occurs, the framework allows it to easily troubleshoot the 
problem and craft the appropriate resolution of the error.
2. Related works
With the recent increase in the spread and use of technology across the world, it has become 
common for various sectors to adapt technology for use in their respective fields. This applies 
the education sector as it does to any other field. Terms such as artificial intelligence (AI), deep 
learning, and machine learning are now commonly used in education and by education pro-
fessionals. Indeed, in the educational field, AI is used in machine learning. That is, machine 
learning makes use of AI in its effort to teach machines how to look for different types of data.
Machine Learning - Advanced Techniques and Emerging Applications176
2.1. Machine learning
Currently, education and learning remains largely focused on feeding students with information 
and hoping that it is retained. Accordingly, a student’s intelligence is assessed by testing their 
ability to recall information previously taught. The problem is that this model ignores examining 
how well the students understand the information and how they apply it in real-life situations. 
This model has proven to be toxic over the years. More schools and education centers have begun 
to realize how use of machine learning can make work more efficient and easier and have started 
to adopt technology at an increasing rate. Indeed, machine learning can accommodate all kinds 
of students. In the long run, machine learning is bound to produce the following advantages:
Customized and personalized learning – Machine learning is flexible enough to cater to all students 
regardless of their learning speeds. By making use of algorithms that learn how the student 
consumes information, machine learning allows the learner to move ahead only after they have 
truly grasped the previous content. This process ensures that no student is overlooked or left 
behind. This is true even if they are the only one in class that has not yet understood the con-
tent. The machine learning system also allows teachers to individually monitor student and 
help them those areas where they are deficient. This contrasts with the traditional educational 
method, which focuses on a one-size-fits-all management where everyone in class is taught the 
same way. This type of learning can be found in the EdTech and MagicBox learning systems [3].
Analytics of content—Refers to a machine learning system where teachers instruct students 
by using machines. The machines are used to analyze the information teachers are using to 
teach and to determine whether the quality of the content meets the applicable standards. 
The machines are also used to help determine if the content taught to the students complies 
with the intellectual ability of each student. Since students are taught in accordance to their 
individual needs, their learning progress and understanding improve.
Grading—Machine learning systems are used to reduce the amount of time needed to grade 
student work. In addition, machines are used to increase the efficiency and accountability 
of the grading system. The system still allows for the larger portion of the grading to be 
 performed by teachers. However, machines aid in the analysis of student information such as 
in the detection of plagiarism or cheating.
Simplification of tedious tasks—In the traditional method of learning, teachers spend a substan-
tial amount of time in repetitive and tedious tasks, such as taking class attendance or gather-
ing of class assignments. Machines can be used to automate these tasks and reduce the time 
or need for teachers to do them. Accordingly, teachers will have more time to focus on more 
important tasks such as making sure that their students fully understand the learning material.
Students’ progress—By using machines, the teachers can monitor each student on a personal level 
and evaluate their learning progress, individually. Machines can also provide additional learning 
patterns of the students, which help teachers to determine the best ways of teaching the students.
As the above information makes clear, using machine learning in teaching brings numerous 
advantages to the table. It is therefore advisable for every school to adopt these types of  learning 
platforms, such as the EdTech revolution program. With this, learning becomes easier, more 
Machine Learning in Educational Technology
http://dx.doi.org/10.5772/intechopen.72906
177
efficient and customizable to each student’s need [4]. By employing methods relating to digital 
learning, there is the possibility of collecting a wide range of data about the behaviour of the 
learner, especially in the learning activities. The measurements collected consist of variables like 
completion time, video views, group discussion activities and test results. Measurements of this 
nature are applicable in the context of feature engineering that leans on the machine learning 
algorithms. Experts argue that the algorithms can find a correlation between the specific behav-
iour exhibited by learners, regarding their learning performance [5]. It is this outcome that is 
used to determine the overall efficiency manifested by a particular machine program.
Recommender systems are the more obvious target of machine learning usage. The experience 
of this technology is illustrated by its use on some of the more prominent software platforms 
like Amazon and LinkedIn. Recently, Twitter has begun applying this technology to their 
platform. Researchers in the education sector consider recommender systems as the most uti-
lized systems in modern times. In the context of human learning, recommender systems that 
are oriented to learning in a specific way have the capacity to assist learners properly identify 
the appropriate content [6]. In this regard, there is a guarantee of realizing the projected com-
petence development objectives as far as machine-oriented education is concerned.
The advance in AI technology has allowed machine learning in education gain a considerable 
amount of support. In fact, machine learning should be credited for making AI a possible and 
fruitful endeavor in education. In achieving this result, machine learning has combined and 
utilized the aspects of mathematical algorithms. Researchers in the vast education field have 
tried to introduce the concept of machine learning into the mainstream schooling system. The 
goal has been to use machine learning as teaching assistants that can ease the job of human 
educators [7]. This approach aids in data provision of the students’ performance, coupled with 
suggested actions geared towards making improvements to the student’s learning experience.
The use of machine learning in tools related to education technology has been more signifi-
cant in its overall applications. Experts have created a real-time platform capable of giving 
immediate feedback to learners. The same platform has harnessed the efficiency and effective-
ness of online-based tutors. In fact, the platform has been credited with almost all the success 
that takes place on the Internet. The latest platforms are so sophisticated that they are capable 
of detecting and monitoring the reaction of the student concerning the concepts being taught. 
This approach is known to reduce the misunderstanding normally experienced during the 
learning process. The ability of these platforms to give early warning to tutors enables them 
to avoid mistakes that would have otherwise been made during the learning process [8]. 
Tutoring systems based on AI is an interesting and resourceful concept, to the extent that it 
employs substantial amounts of data that is coupled to machine learning, to offer guidance 
that is personalized and supplemental to the students. The feedback system provided by the 
AI tutoring systems is critical in tracking the learners’ progress.
The adoption of machine learning technology has enhanced the concept of crowd-sourced 
tutoring. The goal of crowd-sourced tutoring is to provide assistance from private tutors, 
and in some cases, classmates who fill gaps in understanding by supplementing the content 
learned in class. Students using social networking sites for learning purposes, like Brainy, are 
enjoying the effectiveness of AI in the learning process. Most of the social networking sites 
Machine Learning - Advanced Techniques and Emerging Applications178
that are education-oriented tend to employ AI algorithms which harness their networking 
features. The algorithms also bring a personal touch to the learning process, making it more 
appealing to the learner. In addition, AI increases the level of interactivity in these platforms, 
which is helpful in fostering the learning process.
Machine learning algorithm works by having machines use software applications that assist 
the machine to determine outcomes that are accurate. By using algorithms, the machines 
can receive data, analyze it and then produce an output that is within an acceptable range. 
Machine learning algorithms are divided into two major groups: supervised algorithms and 
unsupervised algorithms. For supervised algorithms, people input information together with 
the required results into the machine. With this, the machine can learn what is desired of it 
when a similar command is inputted. For unsupervised learning, the machines are not fed 
with the outcome that one would like [9].
In the education sector, machine learning algorithms have made normal operations easier, 
faster and more efficient when compared to when they are done manually. This has proven to 
be a game changer in education sector. One of the major benefits of its adoption has been to 
help identify each student’s needs so that the teachers can differentiate between problems gen-
eral to the class and those specific to individual students. Accordingly, through machine learn-
ing, no student is overlooked or left behind. Additionally, with machine learning, students are 
also given a platform from which they can voice their grievances so that the problems do not 
escalate beyond resolution. The machines help in the grading, by monitoring the scores of the 
students in their assignments and the tests. The machines also assist teachers by organizing 
the information being taught to students. The inclusion of machine learning in education has, 
therefore, made the education system more convenient for both teachers and students alike.
AI has enabled teachers and to a larger extension, schools, design textbooks and learning exercises 
that can achieve a high degree of customization to the needs of the user. Content Technologies, 
Inc. (CTI) is one of the major players in the industry. CTI tends to specialize on deep learning 
concepts to create custom textbooks [9]. This is achieved by inputting a syllabus into the engine 
of CTI. After that, the system absorbs the content to generate new patterns. It is then the work of 
algorithms to use the knowledge gained for the purposes of creating textbook materials.
3. The architecture of the virtual assistance framework
Since students have different styles of learning, it is necessary to use a variety of assistance 
to help increase the performance level of learning. Various machine learning algorithms and 
techniques, such as decision-making algorithms and techniques, can be implemented for 
allowing the virtual assistant to communicate with the students and teachers.
There are two main parts to the virtual assistants, namely, one for students and other for 
teachers. Students can answer one or more virtual assistant’s questions. One or more spon-
sored links, related to the determined course, is then provided to the student. The sponsored 
links can be voice, audio data, displaying video or textual information. Exam training and 
test dates remainder are kinds of the virtual assistant that gets provided to learners. Also, the 
Machine Learning in Educational Technology
http://dx.doi.org/10.5772/intechopen.72906
179
proposed system helps students to manage their teamwork project. After the session with the 
system, a student is provided with the feedback about his progress.
The system is also able to design presentations for specific learners. Notably, different students 
have different learning abilities; therefore, the system is able to compute a favorable learning 
style for each student. The teacher monitors the progress of each student through feedback 
about how each student performed in the sessions. This facilitates appropriate grading. Also, 
the virtual assistant is able to point out areas of the course that need to be explored further to 
enhance learning by providing additional reference materials to a topic. Also, the teacher is 
able to identify which students need extra help using the feedback provided by the system.
The proposed architecture is a reliable virtual assistant website that not only helps teachers 
and students to do their tasks in a shorter time but also allows them to coordinate their work.
4. Technical implementation
The underlying technical implementation of the virtual assistant system starts with creating 
use cases for the product. The identification of virtual assistants and the underlying technol-
ogy are required for moving ahead with the implementation of the proposed website [10]. The 
 following technical specifications have been identified for building the virtual assistant website:
Software used:
• BitVoicer: Speech recognition.
• Python 2.7: Coding language
• Eclipse, Geany or your preferred interface for coding on Python Virtual COM Port.
To facilitate interaction between the virtual assistant website and the user, a software known 
as Wit.ai is installed. Wit.ai offers a perfect combination of voice recognition, and subsequent 
machine learning in the context of developers. The software offers services that concentrate on 
converting verbal commands into text. Moreover, Wit.ai has the capability of understanding 
the commands that are said. The most sophisticated forms of Wit.ai can be programmed to 
understand commands whose prior understanding was scant or non-existent. This is crucial 
in the educational context since learners tend to understand at varying paces. The extensive 
capability of Wit.ai software to improve the interactivity of virtual assistant website can be 
verified by the fact that it has been incorporated by a number of notable social media net-
works, such as Facebook [10].
Clarifai is another service that can be added to the virtual assistant website to improve its 
interactivity. Clarifai is a service geared towards AI, and it possesses the ability to decode 
contents that is in an image and video format. Another strength associated with Clarifai is that 
it possesses a deep learning engine that improves with its usage [10]. The tool is of paramount 
importance when there is a need to make improvements in the AI prototype and grant it the 
capability of seeing and recognizing objects.
Machine Learning - Advanced Techniques and Emerging Applications180
The virtual assistant experience with the users has been remarkable. All students who pro-
vided feedback regarding their interactions reported positive experiences. Fundamentally, the 
issue of the ease of interactivity, friendly user interfaces and responsiveness were reviewed. 
The first student reported that the system has a friendly user interface that is not complex, thus 
allowing a user to navigate through different sections of the system. The student added that 
the system was highly responsive in terms of answering questions. He recounted that, in the 
traditional class setting, he was afraid to ask questions in front of the other student. However, 
the virtual assistant offered personalized interaction where he could ask any questions, clari-
fications and point out his areas of weakness. The second student who experimented with the 
software also found it quite useful. He emphasized that he liked the fact that he was able to get 
immediate feedback on his questions. This was a vast improvement over the traditional way 
of waiting to talk to the teacher after class, when the teachable moment has already expired. 
Some instructors are always in a rush after finishing their classes. As such, they are unable to 
allocate ample time to explain specific concepts taught in class to the student. Therefore, the 
student misses out on these concepts that might cause low academic performance. In other 
cases, teachers recommend students with clarifications to get the assistance of their class-
mates. This hampers full understanding as one needs to develop a rapport with their fellow 
student to enhance learning, and others become intimidated. However, the virtual assistant 
allocates enough time and is able to answer all questions, providing detailed explanations. 
The third student said that she found it was an effective supplement to one teacher’s exten-
sive use of multiple choice exams. According to the student, while such exams might tell her 
whether or not she knows the answer to a question, they do not help her understand the logic 
underlying the answer. The virtual assistance was helpful in achieving that understanding. 
The fourth student also reported satisfaction with the system. Firstly, the student confessed 
to being a slow learner. This had really affected how she grasped concepts. Most of the times, 
she felt left behind in classwork and had no one to consult as she was shy about her condition. 
However, the system helped her to learn at her pace and recommended interactive learning 
model that allowed her ask for clarification after every 10 minutes of the learning session. She 
was enthusiastic to note that this has helped her understand most of the concepts taught in 
class and generally improve her grades. Lastly, a teacher who had made use of the assistance 
said it allowed her more time to figure out what her students actually understood and where 
they were having difficulties. By so doing, it helped her know which areas needed much atten-
tion to enhance understanding. She recounted that teaching a class of 30 students can some-
times be difficult to know who understood well, who needed extra attention on a specific topic 
and what learning model suited a specific group of students. The virtual assistant, according 
to the teacher, answers these questions. The system is able to compile interactive activities to 
address specific learning outcomes to indent whether the students understood the topic.
5. Conclusion
Machine learning with AI has opened incredible possibilities in various fields. This is espe-
cially the case in terms of the education sector and education-related fields. This means that 
future learning environments are likely to be highly personalized, with the ability to help 
Machine Learning in Educational Technology
http://dx.doi.org/10.5772/intechopen.72906
181
learners realize their utmost potential in the most fulfilling way. There will be a steady adop-
tion of machine learning in various areas of concern for educational technology. In the initial 
stages, its impact will not be clearly apparent or significant to the end user. Despite this, teach-
ers have started to see how tasks can be simplified and more effectively completed through 
the employment and application of machine learning technologies. The advances made in 
adopting machine learning into education sector have significantly saved teachers’ time in 
both the classroom and non-classroom-related activities. Stakeholders have welcomed this 
unprecedented benefit, as it makes learning easier and more appealing.
The future work on machine learning, especially in the education context, shall witness the 
development of more sophisticated AI tools. There are multiple prospects for designing com-
plex chatbots that will improve the sophistication of virtual assistants. This development shall 
foster more human interactions that will replace emails and text messages. Already, plans 
are underway for developing online virtual assistants named “Amy” or “Andrew” at x.ai 
to schedule meetings with both tutors and learners. AI coupled with machine learning that 
incorporates deep learning and natural language processing is projected to go a level higher 
by incorporating more sophisticated systems laced with capabilities to adapt, learn and pre-
dict systems with utmost autonomy. The future works on these systems shall incorporate a 
combination of advanced algorithms and embedded massive data sets.
Author details
Ibtehal Talal Nafea
Address all correspondence to: inafea@taibahu.edu.sa
College of Computer Science and Engineering (CCSE), Taibah University, Medina, Kingdom 
of Saudi Arabia
References
[1] Lv Z, Li X. Virtual reality assistant technology for learning primary geography. In 
International Conference on Web-Based Learning. Springer International Publishing. 
ISO 690. 2015 November. pp. 31-40. DOI: 10.1007/978-3-319-32865-2_4
[2] Tomei LA. Learning Tools and Teaching Approaches through ICT Advancements. 
Hershey, PA: Information Science Reference; 2013
[3] Mulwa C, Lawless S, Sharp M, Arnedillo-Sanchez I, Wade V. Adaptive educational hyper-
media systems in technology enhanced learning: A literature review. In Proceedings of 
the 2010 ACM Conference on Information Technology Education. ACM. ISO 690. 2010 
October. pp. 73-84
Machine Learning - Advanced Techniques and Emerging Applications182
[4] Bhat AH, Patra S, Jena D. Machine learning approach for intrusion detection on cloud 
virtual machines. International Journal of Application or Innovation in Engineering & 
Management (IJAIEM). 2013;2(6):56-66
[5] Lafond D, Proulx R, Morris A, Ross W, Bergeron-Guyard A, Ulieru M. HCI dilemmas for 
context-aware support in intelligence analysis. Dalhousie Medical Journal. 2014
[6] Lisetti C, Amini R, Yasavur U. Now all together: Overview of virtual health assis-
tants emulating face-to-face health interview experience. KI – Künstliche Intelligenz. 
2015;29(2):161-172. DOI: 10.1007/s13218-015-0357-0
[7] Bell B.  Supporting educational software design with knowledge-rich tools. In Authoring 
Tools for Advanced Technology Learning Environments. Springer Netherlands. 2003. 
pp. 341-375
[8] Haynes M, Anagnostopoulou K. Supporting educational software design with knowl-
edge-rich tools. In Authoring Tools for Advanced Technology Learning Environments. 
Springer Netherlands. 2003. pp. 341-375
[9] Brinson JR. Learning outcome achievement in non-traditional (virtual and remote) ver-
sus traditional (hands-on) laboratories: A review of the empirical research. Computers 
& Education. 2015;87:218-237. DOI: 10.1016/j.compedu.2015.07.003
[10] Padró L, Stanilovsky E. Towards wider multilinguality. In: Proceedings of the 8th 
International Conference on Language Resources and Evaluation. 2012
Machine Learning in Educational Technology
http://dx.doi.org/10.5772/intechopen.72906
183

",322434405,"{'doi': '10.5772/intechopen.72906', 'oai': 'oai:intechopen.com:58546'}",Machine Learning in Educational Technology,"{'code': 'en', 'name': 'English'}",2018-09-19T00:00:00+00:00,'IntechOpen',[],['http://www.intechopen.com/chapter/pdf-download/58546'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/322434405.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/322434405'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/322434405/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/322434405/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/322434405'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/322434405?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=9&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Machine learning is a subset of artificial intelligence (AI) that helps computers or teaching machines learn from all previous data and make intelligent decisions. The machine-learning framework entails capturing and maintaining a rich set of information and transforming it into a structured knowledge base for different uses in various fields. In the field of education, teachers can save time in their non-classroom activities by adopting machine learning. For example, teachers can use virtual assistants who work remotely from the home for their students. This kind of assistance helps to enhance students’ learning experience and can improve progression and student achievement. Machine learning fosters personalized learning in the context of disseminating education. Advances in AI are enabling teachers to gain a better understanding of how their students are progressing with learning. This enables teachers to create customized curriculum that suits the specific needs of the learners. When employed in the context of education, AI can foster intelligence moderation. It is through this platform that the analysis of data by human tutors and moderators is made possible","['Chapter, Part Of Book', 'Machine Learning - Advanced Techniques and Emerging Applications']",disabled
,"[{'name': 'Bayoude, Kenza'}, {'name': 'Ouassit, Youssef'}, {'name': 'Ardchir, Soufiane'}, {'name': 'Azouazi, Mohamed'}]",[''],2019-10-17T19:44:24+00:00,"{'name': 'Periodicals of Engineering and Natural Sciences (PEN - International University of Sarajevo)', 'url': 'https://api.core.ac.uk/v3/data-providers/13199'}",,,10.21533/pen.v6i2.526,https://core.ac.uk/download/229562277.pdf,"Periodicals of Engineering and Natural Sciences  ISSN 2303-4521 
Vol. 6, No. 2, December 2018, pp.373-379 
Available online at: http://pen.ius.edu.ba 
  
 373 
How Machine Learning Potentials are transforming the Practice of 
Digital Marketing: State of the Art 
 
 
Kenza Bayoude
1
, Youssef Ouassit
2
, Soufiane Ardchir
3
, Mohamed Azouazi
4 
1,2,3,4 Departement of mathematics and computer science, Faculty of Sciences, Hassan II University, Casablanca, Morocco 
 
 
Article Info  ABSTRACT  
 
 
Received Jun 25, 2018 
 
 
Today, the digital marketing is constantly evolving, new tools are regularly 
introduced with the new consumer habits and the multiplication of data, often 
forcing marketers to delve into too much data that may not even give them the 
overview they need to make business decisions that have an impact.  
After the revolution of machine learning technology in other real world 
application, machine learning is changing the digital marketing landscape, 
84% of marketing organizations are implementing or expanding their use of 
machine learning in 2018[1].It becomes easier to predict and analyze 
consumer behavior with great accuracy. 
In our work we will start by establish an art of state on the main and most 
used machine learning potentials in digital marketing strategies and we show 
how machine learning tools can be used at large scale for marketing purposes 
by analyzing extremely large sets of data. The way that ML is integrated in 
digital marketing practices helps them better understand the target consumers 
and optimize their interactions with them. 
Keyword: 
Machine Learning 
Digital Marketing 
Predictive Analysis  
Customer Behavior 
 
 
Corresponding Author: 
Kenza Bayoude,  
 
PhD Student Departement of mathematics and computer science,  
Hassan II University,Casablanca,Morocco  
Email: kenza.bayoude@gmail.com 
 
1. Introduction 
It is In recent years, the marketing field has been impacted by advanced technologies; it is changing rapidly 
with advancements being made in machine learning coupled with increases in the availability and quantity of 
data, more companies are looking in to adapt the concept of machine learning in their marketing strategies to 
make their operations more effective and smarter. According to QuanticMind, 97% of executives estimate that 
the future of marketing rely on how digital marketers use machine learning potentials [2]. 
Increasingly, the marketing world is using these resources to understand data, and learn more about 
customers, and streamline operations marketing. Marketers leverage the ability of machine learning to connect 
data points to gain insight into their customers and process critical data to predict the future. 
Our research is a descriptive study, it aims to present the various dimensions attached to the concept of 
machine learning in marketing to improve the marketing strategies, and we explain how machine learning 
tools are incorporating in the digital marketing strategies. 
Through this paper, we propose a state of the art of the transformative ways in which machine learning 
technology is changing the world of digital marketing, we begin with basic concepts on machine learning then 
we present its role in business world. The last part is reserved to explore how the power of machine learning 
are utilizing to revolution a digital marketing and discusses the necessity of using this strategy for the 
marketing of product and services. 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
374 
 
2. Machine learning: Concepts 
2.1.  Machine learning: Definition 
Machine learning is a branch of study that allows computers the ability to learn and improve themselves by 
learning from data experience without basing on explicit programming, relied on the idea that computer 
systems can identify models and make decisions with minimal human intervention [3]. They can self-rulingly 
change. They gain from past counts to make strong, repeatable decisions and results. It's a science that isn't 
new but instead one that has expanded new power. While many machine learning computations have been 
around for a long time, the ability to normally apply complex numerical [15]. 
Within the field of predictive analytics, machine learning is a method of data analysis that improves and 
automates analytical model building. It’s used to develop complex predictive algorithms and to generate more 
precise mathematical models that analyze larger amounts of increasingly current and historical data to make 
predictions about future events. The adoption of data intensive machine learning methods can be found 
throughout technology, science and ecommerce, which leads to evidence-based decision making in many 
domains, including marketing and other sectors [4]. 
2.2.  Machine learning: The Process 
Machine Learning consists of series of steps to be followed. 
 
 
Figure1.Machine learning process. 
 
 
2.2.1. Data collection 
The first step and most importantly is to gather relevant data according to the problem statement. 
 
2.2.2. Data Pre-processing 
In this process, the raw data is collected and you analyze the data to find a way to transform it into useful data. 
 
2.2.3. Model Training 
This training model requires providing a Machine learning algorithm with training data to be studied. 
 
2.2.4. Model Testing 
It is important to be able to evaluate the performance of the model and compare different algorithms to 
estimate the properties of model. 
Because of the process of training and testing, data is often divided into two different subsets, training datasets 
and testing datasets. The main reason for this is to ensure that the model can generalize enough to make good 
predictions on new examples [5]. 
 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
375 
2.2.5. Improving the performance 
The performance of the model can be improved both on the training and testing data sets by trying several 
machine learning algorithms and using the one that does the best. 
 
3. Role of Machine learning in Business 
Machine Learning is a topic of considerable interest in digital world and industry discussions and the 
advertising world these days. 
Many companies use machine learning algorithms to highlight hidden knowledge in consumer data 
and to improve business, provide for better customer experience, and contribute to an operational 
efficiency like a speed, cost savings, and greater precision. 
A variety of technological developments in the marketing sector have contributed to generate the 
infrastructure and data sets and the business organization gains a suitable solution to develop 
business processes, improve company performance, increase profits, investigate competitors’ motive 
and activities and obtain additional satisfied customers[16].  
The proliferation of electronic trading platforms has been accompanied by an augmentation in the 
disponibility of high quality market data in structured formats. In some countries, such as the United 
States, market regulators allow to use social media for public announcements [6].  
In addition to making digitized data available for machine learning, the computerization of markets 
has made it feasible for machine learning algorithms to interact directly with markets, putting in real-
time complex purchase and sell orders relied on sophisticated decision-making, in many cases with a 
little human intervention.Fig.2. 
 
 
Figure 2. Demand and supply factors of marketing adoption of machine learning. 
 
4. The relationship between Machine learning and Digital Marketing 
Most companies are taking advantage of Machine Learning techniques because of having the capacity to 
analyze a large number of data sets and give understandable analysis that help marketers to specialize in other 
field to optimize the marketing strategies. 
Machine Learning will define how digital marketing will be conducted now and in the future. The following 
are the ways that has transformed Machine Learning potentials in changing the world of digital marketing. 
 
4.1. Predicting the behavior of customers: 
 
Using Propensity models and predictive analytics, marketers can identify potential customers who are more 
likely to respond to their offers by correlating customer characteristics with their anticipated behaviors. 
 
4.4.1. Predictive analytics 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
376 
 
 Predictive analysis is the analysis of available historical and current data and the use of machine learning 
techniques to create forecasts of future behaviors, preferences and needs. It aims to predict future trends, 
particularly in the marketing sector [7]. 
Predictive analysis allows marketers to extract information from data, that users leave when they interact 
online, and uses it to predict purchasing trends and user behavior models in order to develop automated 
systems and customer profiles to target certain markets.  
This tool analyzes data in large volumes and helps reveal the most impactful insights. This works through the 
following steps [8]: 
Identify the objectives that analyze data extracted from several sources, and that require cleaning and 
preparation to determine the pattern or model that suits our needs. 
Creation of models and their validation. The models are validated based on the goals set. 
Application of the model results to take advantages of it and guide to making the correct business decisions 
and continually refining the model for better results. 
For businesses today, the key to optimize marketing campaigns is being able to anticipate customer behavior. 
Predictive models generated by machine learning can be trained to the prospect list and make it possible to 
predict the ideal customer profiles based on relevant data from the web and certain criteria defined by the 
sales team as ""qualified buyers."" In this way, they can a predict lot of aspects, such as which clients are more 
vulnerable to making more than one purchase. This, in addition to contributing to raise the sales, means saving 
considerable time and resources. This is the essence of the machine learning workflow, which stores the 
relationships and applies them to make the prediction [9].  
The model is trained with new data as it becomes available, which improves the reliability of future 
predictions.Fig.3. 
 
 
Figure 3. The components and processes of a machine learning model to predict a potential customers. 
 
4.4.2. Propensity Modeling 
 
A propensity model is statistical method to make accurate predictions about customer behavior; the creation of 
the model based on the data history powered the machine learning algorithm. Common models include 
predictive lifetime value; likelihood of engagement; propensity of unsubscribe; propensity of conversion; 
propensity of purchasing; and propensity of churn [10]. The simple diagram below shows the steps of this 
process: Fig.4 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
377 
 
Figure 4. The process of propensity modeling. 
 
4.2. Predictive Marketing 
Social media plays an important role in collecting more personal information about the potential 
clients, which makes it easier for marketers to have focused campaigns. With each click every time 
a user browses the internet, new data is generated and compiled for predictive analysis [12]. This 
data is valuable for marketers to optimize information and provide the most appropriate 
information. 
4.3. Chatbots 
Most businesses already know and use live chatbots. In short, Chatbots are driven programs that 
interact with users in natural language environments. This makes this website unique, valuable, or 
interesting and different from the others. 
Most digital marketers see chatbots as a tool to provide customized customer service by gathering 
data and personal information. However, chatbots also can guide users through the customer's 
journey to sales [13]. 
From a digital marketing perspective, chatbots provide the possibility to engage with an audience 
that is targeted at a personal level. 
4.4. Content Marketing 
Nowadays, companies are embarking on content personalization and consumers are increasingly 
interested in interacting with them on an individual basis [14]. Thanks to machine learning tools, 
companies can now personalize customer experiences even if their customer profiles are very 
specific.  They use also the power of Social Networks; it plays a communication media in real 
time for the user’s interaction. They are used to share all the experiences and their personal valid 
opinions on various topics like news, politics, celebrities, sports, events and products.It exhibits 
high potential indigital marketing for integral growth [17]. 
Machine learning helps companies create a personalized content based on the consumer's profile, 
their expectations, behavior, buying habits and interests, companies can implement specific tools 
on their site such as algorithms that analyze data and chatbots. It is also possible to direct the 
Internet user to specific content thanks to personalized recommendation systems. 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
378 
5. Conclusion 
Technological advancements have always helped businesses by creating new opportunities for reaching 
customers. One of greatest technologies of our time is machine learning. It creates new opportunities for 
storytelling and marketing which is change how people interact with information, technology, brands and 
services. 
In this article we presented the state of the art of how machine learning techniques can improve the digital 
marketing practices, our next article we will in more detail about how applied propensity modeling to open 
new customer opportunities by gathering the data from social media. 
 
References 
 
[1] “Impact of Machine Learning on Marketing”,https://satoristudio.net/machine-learning-in-digital-
marketing, 2019 
[2] “How to Apply Machine Learning to Your Digital Marketing Strategy”, Available on: 
https://digitalmarketinginstitute.com/en-ie/blog/14-03-2018-how-to-apply-machine-learning-to-your-digital-
marketing-strategy,2019 
[3] Alescodata “machine learning for direct mail marketing”, 2018 
http://www.alescodata.com/ebook/Alesco_Machine_Learning_Ebook.pdf 
[4] M. I. Jordan1, T. M. Mitchell2, “Machine learning: Trends, perspectives, and prospects”, Science,pp 1-7, 
2015 
[5] Pål Sundsøy1, Johannes Bjelland1, Asif M. Iqbal1, Alex “Sandy” Pentland2, and Yves-Alexandre de 
Montjoy, “Big Data-Driven Marketing: How Machine Learning Outperforms Marketers’ Gut-
Feeling”,Springer, 2014 
[6] “Artificial intelligence and machine learning in financial services”, Financial Stability Board, 2017 
[7] Georgios Paliouras, Vangelis Kakaletsis,”Machine learning and its application”,pp 295-299, 
Springer,2001 
[8]T.Thiraviyam, “Artificial intelligence marketing”, 
https://www.researchgate.net/publication/328580914_Artificial_intelligence_Marketing,2018 
[9] Sahar F. Sabbeh,”Machine-Learning Techniques for Customer Retention: A Comparative Study”, 
International Journal of Advanced Computer Science and Applications (IJACSA), 2018 
[10] Dr. Amol Murgai,""Transforming Digital Marketing with Artificial Intelligence”, International Journal of 
Latest Technology in Engineering, Management & Applied Science (IJLTEMAS), 2018. 
[11] Jim Sterne,”Artificial Intelligence for Marketing: Practical Applications”, 2017 
[12] Xiao Luo,Revanth Nadanasabapathy,A. Nur Zincir-Heywood,Keith Gallant,Janith Peduruge”Predictive 
Analysis on Tracking Emails for Targeted Marketing”,Springer,2015 
[13] Simon Kingsnorth, “Digital Marketing Strategy: an integrated approach to online marketing”, Google 
Scholar, 2016 
[14] Louis Columbus”10 Ways Machine Learning Is Revolutionizing Marketing”, Forbes, 2018 
 
[15] Yogesh Hole,""Service marketing and quality strategies"",Periodicals of Engineering and Natural 
Sciences,Vol 6,No 1, 2018 
 
 PEN Vol. 6, No. 2, December 2018, pp.373- 379 
379 
[16] Srikanth Bethu,""Data Science: Identifying influencers in Social Networks"", Periodicals of Engineering 
and Natural Sciences, Vol 6, No 1, 2018 
 
[17] Aruna R Flarence,""Importance of Supervised Learning in Prediction Analysis"", Periodicals of 
Engineering and Natural Sciences, Vol 6, No 1, 2018 
 
 
 
 
 
 
 
 
 
 
 
",229562277,"{'doi': '10.21533/pen.v6i2.526', 'oai': 'oai:ojs.pen.ius.edu.ba:article/526'}",How machine learning potentials are transforming the practice of digital marketing: State of the art,,2018-10-24T00:00:00+00:00,'International University of Sarajevo',[],['http://pen.ius.edu.ba/index.php/pen/article/download/526/308'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/229562277.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/229562277'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/229562277/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/229562277/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/229562277'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/229562277?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=10&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Today, the digital marketing is constantly evolving, new tools are regularly introduced with the new consumer habits and the multiplication of data, often forcing marketers to delve into too much data that may not even give them the overview they need to make business decisions that have an impact. 
After the revolution of machine learning technology in other real world application, machine learning is changing the digital marketing landscape, 84% of marketing organizations are implementing or expanding their use of machine learning in 2018[1].It becomes easier to predict and analyze consumer behavior with great accuracy.
In our work we will start by establish an art of state on the main and most used machine learning potentials in digital marketing strategies and we show how machine learning tools can be used at large scale for marketing purposes by analyzing extremely large sets of data. The way that ML is integrated in digital marketing practices helps them better understand the target consumers and optimize their interactions with them","['info:eu-repo/semantics/article', 'info:eu-repo/semantics/publishedVersion']",disabled
,"[{'name': 'Kamp, Michael'}, {'name': 'Boley, Mario'}, {'name': 'Missura, Olana'}, {'name': 'Gärtner, Thomas'}]",[],2019-02-06T06:44:14+00:00,"{'name': 'arXiv.org e-Print Archive', 'url': 'https://api.core.ac.uk/v3/data-providers/144'}",,research,,http://arxiv.org/abs/1810.03530,"Effective Parallelisation for Machine Learning
Michael Kamp
University of Bonn
and Fraunhofer IAIS
kamp@cs.uni-bonn.de
Mario Boley
Max Planck Institute for Informatics
and Saarland University
mboley@mpi-inf.mpg.de
Olana Missura
Google Inc.
olanam@google.com
Thomas Ga¨rtner
University of Nottingham
thomas.gaertner@nottingham.ac.uk
Abstract
We present a novel parallelisation scheme that simplifies the adaptation of learn-
ing algorithms to growing amounts of data as well as growing needs for accurate
and confident predictions in critical applications. In contrast to other paralleli-
sation techniques, it can be applied to a broad class of learning algorithms with-
out further mathematical derivations and without writing dedicated code, while
at the same time maintaining theoretical performance guarantees. Moreover, our
parallelisation scheme is able to reduce the runtime of many learning algorithms
to polylogarithmic time on quasi-polynomially many processing units. This is a
significant step towards a general answer to an open question on the efficient par-
allelisation of machine learning algorithms in the sense of Nick’s Class (NC). The
cost of this parallelisation is in the form of a larger sample complexity. Our empir-
ical study confirms the potential of our parallelisation scheme with fixed numbers
of processors and instances in realistic application scenarios.
1 Introduction
This paper contributes a novel and provably effective parallelisation scheme for a broad class of
learning algorithms. The significance of this result is to allow the confident application of machine
learning algorithms with growing amounts of data. In critical application scenarios, i.e., when errors
have almost prohibitively high cost, this confidence is essential [27, 36]. To this end, we consider the
parallelisation of an algorithm to be effective if it achieves the same confidence and error bounds as
the sequential execution of that algorithm in much shorter time. Indeed, our parallelisation scheme
can reduce the runtime of learning algorithms from polynomial to polylogarithmic. For that, it
consumes more data and is executed on a quasi-polynomial number of processing units.
To formally describe and analyse our parallelisation scheme, we consider the regularised risk min-
imisation setting. For a fixed but unknown joint probability distribution D over an input space X
and an output space Y , a dataset D ⊆ X ×Y of size N ∈ N drawn iid from D, a convex hypothesis
space F of functions f : X → Y , a loss function ` : F × X × Y → R that is convex in F , and a
convex regularisation term Ω: F → R, regularised risk minimisation algorithms solve
L(D) = argmin
f∈F
∑
(x,y)∈D
` (f, x, y) + Ω(f) . (1)
The aim of this approach is to obtain a hypothesis f ∈ F with small regret
Q (f) = E [` (f, x, y)]− argmin
f ′∈F
E [` (f ′, x, y)] . (2)
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
ar
X
iv
:1
81
0.
03
53
0v
1 
 [c
s.L
G]
  8
 O
ct 
20
18
Regularised risk minimisation algorithms are typically designed to be consistent and efficient. They
are consistent if there is a function N0 : R+ ×R+ → R+ such that for all ε > 0, ∆ ∈ (0, 1], N ∈ N
with N ≥ N0(ε,∆), and training data D ∼ DN , the probability of generating an ε-bad hypothesis
is no greater than ∆, i.e.,
P (Q (L(D)) > ε) ≤ ∆ . (3)
They are efficient if the sample complexity N0(ε,∆) is polynomial in 1/ε, log 1/∆ and the runtime
complexity TL is polynomial in the sample complexity. This paper considers the parallelisation of
such consistent and efficient learning algorithms, e.g., support vector machines, regularised least
squares regression, and logistic regression. We additionally assume that data is abundant and that
F can be parametrised in a fixed, finite dimensional Euclidean space Rd such that the convexity of
the regularised risk minimisation problem (Equation 1) is preserved. In other cases, (non-linear)
low-dimensional embeddings [2, 28] can preprocess the data to facilitate parallel learning with our
scheme. With slight abuse of notation, we identify the hypothesis space with its parametrisation.
The main theoretical contribution of this paper is to show that algorithms satisfying the above con-
ditions can be parallelised effectively. We consider a parallelisation to be effective if the (ε,∆)-
guarantees (Equation 3) are achieved in time polylogarithmic in N0(ε,∆). The cost for achieving
this reduction in runtime comes in the form of an increased data size and in the number of processing
units used. For the parallelisation scheme presented in this paper, we are able to bound this cost by
a quasi-polynomial in 1/ε and log 1/∆. The main practical contribution of this paper is an effective
parallelisation scheme that treats the underlying learning algorithm as a black-box, i.e., it can be
parallelised without further mathematical derivations and without writing dedicated code.
Similar to averaging-based parallelisations [32, 45, 46], we apply the underlying learning algorithm
in parallel to random subsets of the data. Each resulting hypothesis is assigned to a leaf of an
aggregation tree which is then traversed bottom-up. Each inner node computes a new hypothesis
that is a Radon point [30] of its children’s hypotheses. In contrast to aggregation by averaging, the
Radon point increases the confidence in the aggregate doubly-exponentially with the height of the
aggregation tree. We describe our parallelisation scheme, the Radon machine, in detail in Section 2.
Comparing the Radon machine to the underlying learning algorithm which is applied to a dataset of
the size necessary to achieve the same confidence, we are able to show a reduction in runtime from
polynomial to polylogarithmic in Section 3.
The empirical evaluation of the Radon machine in Section 4 confirms its potential in practical
settings. Given the same amount of data as the underlying learning algorithm, the Radon ma-
chine achieves a substantial reduction of computation time in realistic applications. Using 150 pro-
cessors, the Radon machine is between 80 and around 700-times faster than the underlying learning
algorithm on a single processing unit. Compared with parallel learning algorithms from Spark’s
MLlib, it achieves hypotheses of similar quality, while requiring only 15− 85% of their runtime.
Parallel computing [18] and its limitations [13] have been studied for a long time in theoretical com-
puter science [7]. Parallelising polynomial time algorithms ranges from being ‘embarrassingly’ [26]
easy to being believed to be impossible. For the class of decision problems that are the hardest in P,
i.e., for P-complete problems, it is believed that there is no efficient parallel algorithm in the sense
of Nick’s Class (NC [9]): efficient parallel algorithms in this sense are those that can be executed
in polylogarithmic time on a polynomial number of processing units. Our paper thus contributes to
Algorithm 1 Radon Machine
Input: learning algorithm L, dataset D ⊆ X × Y , Radon number r ∈ N, and parameter h ∈ N
Output: hypothesis f ∈ F
1: divide D into rh iid subsets Di of roughly equal size
2: run L in parallel to obtain fi = L(Di)
3: S ← {f1, . . . , frh}
4: for i = h− 1, . . . , 1 do
5: partition S into iid subsets S1, . . . , Sri of size r each
6: calculate Radon points r(S1), . . . , r(Sri) in parallel # see Definition 1 and Appendix C.1
7: S ← {r(S1), . . . , r(Sri)}
8: end for
9: return r(S)
2
understanding the extent to which efficient parallelisation of polynomial time learning algorithms is
possible. This connection and other approaches to parallel learning are discussed in Section 5.
2 From Radon Points to Radon Machines
The Radon machine, described in Algorithm 1, first executes the underlying (base) learning algo-
rithm on random subsets of the data to quickly achieve weak hypotheses and then iteratively aggre-
gates them to stronger ones. Both the generation of weak hypotheses and the aggregation can be
executed in parallel. To aggregate hypotheses, we follow along the lines of the iterated Radon point
algorithm which was originally devised to approximate the centre point, i.e., a point of largest Tukey
depth [38], of a finite set of points [8]. The Radon point [30] of a set of points is defined as follows:
Definition 1. A Radon partition of a set S ⊂ F is a pair A,B ⊂ S such that A ∩ B = ∅ but
〈A〉 ∩ 〈B〉 6= ∅, where 〈·〉 denotes the convex hull. The Radon number of a space F is the smallest
r ∈ N such that for all S ⊂ F with |S| ≥ r there is a Radon partition; or ∞ if no S ⊂ F with
Radon partition exists. A Radon point of a set S with Radon partition A,B is any r ∈ 〈A〉 ∩ 〈B〉.
We now present the Radon machine (Algorithm 1), which is able to effectively parallelise consistent
and efficient learning algorithms. Input to this parallelisation scheme is a learning algorithm L on
a hypothesis space F , a dataset D ⊆ X × Y , the Radon number r ∈ N of the hypothesis space
F , and a parameter h ∈ N. It divides the dataset into rh subsets D1, . . . , Drh (line 1) and runs the
algorithm L on each subset in parallel (line 2). Then, the set of hypotheses (line 3) is iteratively
aggregated to form better sets of hypotheses (line 4-8). For that the set is partitioned into subsets of
size r (line 5) and the Radon point of each subset is calculated in parallel (line 6). The final step of
each iteration is to replace the set of hypotheses by the set of Radon points (line 7).
The scheme requires a hypothesis space with a valid notion of convexity and finite Radon number.
While other notions of convexity are possible [16, 33], in this paper we restrict our consideration to
Euclidean spaces with the usual notion of convexity. Radon’s theorem [30] states that the Euclidean
space Rd has Radon number r = d + 2. Radon points can then be obtained by solving a system
of linear equations of size r × r (to be fully self-contained we state the system of linear equations
explicitly in Appendix C.1). The next proposition gives a guarantee on the quality of Radon points:
Proposition 2. Given a probability measure P over a hypothesis space F with finite Radon number
r, let F denote a random variable with distribution P . Furthermore, let r be the random variable
obtained by computing the Radon point of r random points drawn according to P r. Then it holds
for the expected regret Q and all ε ∈ R that
P (Q (r) > ε) ≤ (rP (Q (F ) > ε))2 .
The proof of Proposition 2 is provided in Section 7. Note that this proof also shows the robustness
of the Radon point compared to the average: if only one of r points is ε-bad, the Radon point is
still ε-good, while the average may or may not be; indeed, in a linear space with any set of ε-good
hypotheses and any ε′ ≥ ε, we can always find a single ε′-bad hypothesis such that the average of
all these hypotheses is ε′-bad.
A direct consequence of Proposition 2 is a bound on the probability that the output of the Radon
machine with parameter h is bad:
Theorem 3. Given a probability measure P over a hypothesis space F with finite Radon number r,
let F denote a random variable with distribution P . Denote by r1 the random variable obtained by
computing the Radon point of r random points drawn iid according to P and by P1 its distribution.
For any h ∈ N, let rh denote the Radon point of r random points drawn iid from Ph−1 and by Ph its
distribution. Then for any convex function Q : F → R and all ε ∈ R it holds that
P (Q(rh) > ε) ≤ (rP (Q(F ) > ε))2
h
.
The proof of Theorem 3 is also provided in Section 7. For the Radon machine with parameter
h, Theorem 3 shows that the probability of obtaining an ε-bad hypothesis is doubly exponentially
reduced: with a bound δ on this probability for the base learning algorithm, the bound ∆ on this
probability for the Radon machine is
∆ = (rδ)
2h
. (4)
In the next section we compare the Radon machine to its base learning algorithm which is applied
to a dataset of the size necessary to achieve the same ε and ∆.
3
3 Sample and Runtime Complexity
In this section we first derive the sample and runtime complexity of the Radon machine R from
the sample and runtime complexity of the base learning algorithm L. We then relate the runtime
complexity of the Radon machine to an application of the base learning algorithm which achieves
the same (ε,∆)-guarantee. For that, we consider consistent and efficient base learning algorithms
with a sample complexity of the form NL0 (ε, δ) = (αε + βε ld 1/δ)
k, for some1 αε, βε ∈ R, and
k ∈ N. From now on, we also assume that δ ≤ 1/2r for the base learning algorithm.
The Radon machine creates rh base hypotheses and, with ∆ as in Equation 4, has sample complexity
NR0 (ε,∆) = r
hNL0 (ε, δ) = r
h ·
(
αε + βε ld
1
δ
)k
. (5)
Theorem 3 then implies that the Radon machine with base learning algorithm L is consistent: with
N ≥ NR0 (ε,∆) samples it achieves an (ε,∆)-guarantee.
To achieve the same guarantee as the Radon machine, the application of the base learning algorithm
L itself (sequentially) would require M ≥ NL0 (ε,∆) samples, where
NL0 (ε,∆) = N
L
0
(
ε, (rδ)2
h
)
=
(
αε + 2
h · βε ld 1
rδ
)k
. (6)
For base learning algorithms L with runtime TL(n) polynomial in the data size n ∈ N, i.e.,
TL(n) ∈ O (nκ) with κ ∈ N, we now determine the runtime TR,h(N) of the Radon machine with
h iterations and c = rh processing units on N ∈ N samples. In this case all base learning algo-
rithms can be executed in parallel. In practical applications fewer physical processors can be used
to simulate rh processing units—we discuss this case in Section 5.
The runtime of the Radon machine can be decomposed into the runtime of the base learning al-
gorithm and the runtime for the aggregation. The base learning algorithm requires n ≥ NL0 (ε, δ)
samples and can be executed on rh processors in parallel in time TL(n). The Radon point in each of
the h iterations can then be calculated in parallel in time r3 (see Appendix C.1). Thus, the runtime
of the Radon machine with N = rhn samples is
TR,h(N) = TL (n) + hr3 . (7)
In contrast, the runtime of the base learning algorithm for achieving the same guarantee is
TL(M) with M ≥ NL0 (ε,∆). Ignoring logarithmic and constant terms, NL0 (ε,∆) behaves as
2hNL0 (ε, δ). To obtain polylogarithmic runtime of R compared to TL(M), we choose the parame-
ter h ≈ ldM − ld ldM such that n ≈ M/2h = ldM . Thus, the runtime of the Radon machine is in
O (ldκM + r3 ldM). This result is formally summarised in Theorem 4.
Theorem 4. The Radon machine with a consistent and efficient regularised risk minimisation al-
gorithm on a hypothesis space with finite Radon number has polylogarithmic runtime on quasi-
polynomially many processing units if the Radon number can be upper bounded by a function poly-
logarithmic in the sample complexity of the efficient regularised risk minimisation algorithm.
The theorem is proven in Appendix A.1 and relates to Nick’s Class [1]: A decision problem can
be solved efficiently in parallel in the sense of Nick’s Class, if it can be decided by an algorithm
in polylogarithmic time on polynomially many processors (assuming, e.g., PRAM model). For the
class of decision problems that are the hardest in P , i.e., for P -complete problems, it is believed
that there is no efficient parallel algorithm for solving them in this sense. Theorem 4 provides a
step towards finding efficient parallelisations of regularised risk minimisers and towards answering
the open question: is consistent regularised risk minimisation possible in polylogarithmic time on
polynomially many processors. A similar question, for the case of learning half spaces, has been
called a fundamental open problem by Long and Servedio [21] who gave an algorithms which runs
on polynomially many processors in time that depends polylogarithmically on the sample size but
is inversely proportional to a parameter of the learning problem. While Nick’s Class as a notion of
efficiency has been criticised [17], it is the only notion of efficiency that forms a proper complexity
class in the sense of Blum [4]. To overcome the weakness of using only this notion, Kruskal et al.
[17] suggested to consider also the inefficiency of simulating the parallel algorithm on a single
processing unit. We discuss the inefficiency and the speed-up in Appendix A.2.
1We derive αε, βε for hypothesis spaces with finite VC [41] and Rademacher [3] complexity in App. C.2.
4
4 Empirical Evaluation
This empirical study compares the Radon machine to state-of-the-art parallel machine learning al-
gorithms from the Spark machine learning library [25], as well as the natural baseline of averaging
hypotheses instead of calculating their Radon point (averaging-at-the-end, Avg). We use base learn-
ing algorithms from WEKA [44] and scikit-learn [29]. We compare the Radon machine to the base
learning algorithms on moderately sized datasets, due to scalability limitations of the base learners,
and reserve larger datasets for the comparison with parallel learners. The experiments are executed
on a Spark cluster (5 worker nodes, 25 processors per node)2. All results are obtained using 10-fold
cross validation. We apply the Radon machine with parameter h = 1 and the maximal parame-
ter h such that each instance of the base learning algorithm is executed on a subset of size at least
100 (denoted h = max). Averaging-at-the-end executes the base learning algorithm on the same
number of subsets rh as the Radon machine with that parameter and is denoted in the Figures by
stating the parameter h as for the Radon machine. All other parameters of the learning algorithms
are optimised on an independent split of the datasets. See Appendix B for additional details.
What is the speed-up of our scheme in practice? In Figure 1(a), we compare the Radon machine to
its base learners on moderately sized datasets (details on the datasets are provided in Appendix B).
2The source code implementation in Spark can be found in the bitbucket repository
https://bitbucket.org/Michael_Kamp/radonmachine.
102
103
104
105
106
tra
in
in
g 
tim
e 
(lo
g-
sc
al
e)
WekaSGD
WekaLogReg
LinearSVC
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=1)[LinearSVC]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
PRM(h=max)[LinearSVC]
co
dr
na
St
ag
ge
r1
SE
A_
50
po
ke
r
cli
ck
_p
re
d
SU
SY
0.0
0.2
0.4
0.6
0.8
1.0
AU
C
(a)
102
103
104
105
Avg(h=1)[WekaSGD]
Avg(h=max)[WekaSGD]
Avg(h=1)[WekaLogReg]
Avg(h=max)[WekaLogReg]
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
20
_n
ew
s
SU
SY
HI
GG
S
wi
ki
da
ta
CA
SP
9
0.0
0.2
0.4
0.6
0.8
1.0
(b)
102
103
104
105
SparkLogRegwSGD
SparkSVMwSGD
SparkLogRegwLBFGS
SparkLogReg
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
20
_n
ew
s
SU
SY
HI
GG
S
wi
ki
da
ta
CA
SP
9
0.0
0.2
0.4
0.6
0.8
1.0
(c)
Figure 1: (a) Runtime (log-scale) and AUC of base learners and their parallelisation using the Radon
machine (PRM) for 6 datasets with N ∈ [488 565, 5 000 000], d ∈ [3, 18]. Each point represents the
average runtime (upper part) and AUC (lower part) over 10 folds of a learner—or its parallelisation—
on one datasets. (b) Runtime and AUC of the Radon machine compared to the averaging-at-the-end
baseline (Avg) on 5 datasets with N ∈ [5 000 000, 32 000 000], d ∈ [18, 2 331]. (c) Runtime
and AUC of several Spark machine learning library algorithms and the Radon machine using base
learners that are comparable to the Spark algorithms on the same datasets as in Figure 1(b).
5
po
ker
Sta
gg
er1
SE
A_5
0
SU
SY
clic
k_p
red
cod
rna
101
102
103
sp
ee
du
p
WekaSGD
WekaLogReg
LinearSVC
Figure 2: Speed-up (log-scale) of the Radon
machine over its base learners per dataset from
the same experiment as in Figure 1(a).
106 107
dataset size
103
104
105
106
ru
nt
im
e
1.57
1.17
central
Radon
Figure 3: Dependence of the runtime on the
dataset size for of the Radon machine com-
pared to its base learners.
103 104
training time
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
AU
C 20_newsgroups
SUSY
HIGGS
wikidata
CASP9
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
Avg(h=max)[WekaSGD]
Avg(h=max)[WekaLogReg]
SparkSVMwSGD
SparkLogRegwLBFGS
Figure 4: Representation of the results in Fig-
ure 1(b) and 1(c) in terms of the trade-off between
runtime and AUC for the Radon machine (PRM)
and averaging-at-the-end (Avg), both with param-
eter h = max, and parallel machine learning
algorithms in Spark. The dashed lines connect
the Radon machine to averaging-at-the-end with
the same base learning algorithm and a compara-
ble Spark machine learning algorithm.
There, the Radon machine is between 80 and
around 700-times faster than the base learner
using 150 processors. The speed-up is de-
tailed in Figure 2. On the SUSY dataset
(with 5 000 000 instances and 18 features),
the Radon machine on 150 processors with
h = 3 is 721 times faster than its base learn-
ing algorithms. At the same time, their predic-
tive performances, measured by the area under
the ROC curve (AUC) on an independent test
dataset, are comparable.
How does the scheme compare to averaging-
at-the-end? In Figure 1(b) we compare the
runtime and AUC of the parallelisation scheme
against the averaging-at-the-end baseline (Avg).
In terms of the AUC, the Radon machine out-
performs the averaging-at-the-end baseline on
all datasets by at least 10%. The runtimes can
hardly be distinguished in that figure. A small
difference can however be noted in Figure 4
which is discussed in more details in the next
paragraph. Since averaging is less computa-
tionally expensive than calculating the Radon
point, the runtimes of the averaging-at-the-end
baselines are slightly lower than the ones of
the Radon machine. However, compared to the
computational complexity of executing the base
learner, this advantage becomes negligible.
How does our scheme compare to state-of-the-art Spark machine learning algorithms? We
compare the Radon machine to various Spark machine learning algorithms on 5 large datasets. The
results in Figure 1(c) indicate that the proposed parallelisation scheme with h = max has a substan-
tially smaller runtime than the Spark algorithms on all datasets. On the SUSY and HIGGS dataset,
the Radon machine is one order of magnitude faster than the Spark implementations—here the com-
paratively small number of features allows for a high level of parallelism. On the CASP9 dataset,
the Radon machine is 15% faster than the fastest Spark algorithm. The performance in terms of AUC
of the Radon machine is similar to the Spark algorithms. In particular, when using WekaLogReg
with h = max, the Radon machine outperforms the Spark algorithms in terms of AUC and runtime
on the datasets SUSY, wikidata, and CASP9. Details are given in the Appendix B. A summarizing
comparison of the parallel approaches in terms of their trade-off between runtime and predictive
performance is depicted in Figure 4. Here, results are shown for the Radon machine and averaging-
at-the-end with parameter h = max and for the two Spark algorithms most similar to the base
6
learning algorithms. Note that it is unclear what caused the consistently weak performance of all
algorithms on wikidata. Nonetheless, the results show that on all datasets the Radon machine has
comparable predictive performance to the Spark algorithms and substantially higher predictive per-
formance than averaging-at-the-end. At the same time, the Radon machine has a runtime comparable
to averaging-at-the-end on all datasets and both are substantially faster than the Spark algorithms.
How does the runtime depend on the dataset size in a real-world system? The runtime of
the Radon machine can be distinguished into its learning phase and its aggregation phase. While
the learning phase fully benefits from parallelisation, this comes at the cost of additional runtime for
the aggregation phase. The time for aggregating the hypotheses does not depend on the number of
instances in the dataset but for a fixed parameter h it only depends on the dimension of the hypothesis
space and that parameter. In Figure 3 we compare the runtimes of all base learning algorithms per
dataset size to the Radon machines. Results indicate that, while the runtimes of the base learning
algorithms depends on the dataset size with an average exponent of 1.57, the runtime of the Radon
machine depends on the dataset size with an exponent of only 1.17.
How generally applicable is the scheme? As an indication of the general applicability in practice,
we also consider regression and multi-class classification. For regression, we apply the scheme to
the Scikit-learn implementation of regularised least squares regression [29]. On the dataset YearPre-
dictionMSD, regularised least squares regression achieves an RMSE of 12.57, whereas the Radon
machine achieved an RMSE of 13.64. At the same time, the Radon machine is 197-times faster. We
also compare the Radon machine on a multi-class prediction problem using conditional maximum
entropy models. For multi-class classification, we use the implementation described in Mcdonald
et al. [23], who propose to use averaging-at-the-end for distributed training. We compare the Radon
machine to averaging-at-the-end with conditional maximum entropy models on two large multi-
class datasets (drift and spoken-arabic-digit). On average, our scheme performs better with only
slightly longer runtime. The minimal difference in runtime can be explained—similar to the results
in Figure 1(b)—by the smaller complexity of calculating the average instead of the Radon point.
5 Discussion and Future Work
In the experiments we considered datasets where the number of dimensions is much smaller than
the number of instances. What about high-dimensional models? The basic version of the paral-
lelisation scheme presented in this paper cannot directly be applied to cases in which the size of the
dataset is not at least a multiple of the Radon number of the hypothesis space. For various types of
data such as text, this might cause concerns. However, random projections [15] or low-rank approx-
imations [2, 28] can alleviate this problem and are already frequently employed in machine learning.
An alternative might be to combine our parallelisation scheme with block coordinate descent [37].
In this case, the scheme can be applied iteratively to subsets of the features.
In the experiments we considered only linear models. What about non-linear models? Learning
non-linear models causes similar problems to learning high-dimensional ones. In non-parametric
methods like kernel methods, for instance, the dimensionality of the optimisation problem is equal
to the number of instances, thus prohibiting the application of our parallelisation scheme. How-
ever, similar low-rank approximation techniques as described above can be applied with non-linear
kernels [11]. Furthermore, methods for speeding up the learning process for non-linear models ex-
plicitly approximate an embedding in which then a linear model can be learned [31]. Using explicitly
constructed feature spaces, Radon machines can directly be applied to non-linear models.
We have theoretically analysed our parallelisation scheme for the case that there are enough process-
ing units available to find each weak hypothesis on a separate processing units. What if there are
not rh, but only c < rh processing units? The parallelisation scheme can quite naturally be “de-
parallelised” and partially executed in sequence. For the runtime this implies an additional factor of
max{1, rh/c}. Thus, the Radon machine can be applied with any number of processing units.
The scheme improves ∆ doubly exponentially in its parameter h but for that it requires the weak hy-
potheses to already achieve δ ≤ 1/2r. Is the scheme only applicable in high-confidence domains?
Many application scenarios require high-confidence error bounds, e.g., in the medical domain [27]
or in intrusion detection [36]. In practice our scheme achieves similar predictive quality much faster
than its base learner.
7
Besides runtime, communication plays an essential role in parallel learning. What is the commu-
nication complexity of the scheme? As for all aggregation at the end strategies, the overall amount
of communication is low compared to periodically communicating schemes. For the parallel aggre-
gation of hypotheses, the scheme requires O(rh+1) messages (which can be sent in parallel) each
containing a single hypothesis of size O(r). Our scheme is ideally suited for inherently distributed
data and might even mitigate privacy concerns.
In a lot of applications data is available in the form of potentially infinite data streams. Can the
scheme be applied to distributed data streams? For each data stream, a hypotheses could be
maintained using an online learning algorithm and periodically aggregated using the Radon machine,
similar to the federated learning approach proposed by McMahan et al. [24].
In this paper, we investigated the parallelisation of machine learning algorithms. Is the Radon
machine more generally applicable? The parallelisation scheme could be applied to more general
randomized convex optimization algorithms with unknown and random target functions. We will
investigate its applicability for learning in non-Euclidean, abstract convexity spaces.
6 Conclusion and Related Work
In this paper we provided a step towards answering an open problem: Is parallel machine learn-
ing possible in polylogarithmic time using a polynomial number of processors only? This question
has been posed for half-spaces by Long and Servedio [21] and called “a fundamental open prob-
lem about the abilities and limitations of efficient parallel learning algorithms”. It relates machine
learning to Nick’s Class of parallelisable decision problems and its variants [13]. Early theoretical
treatments of parallel learning with respect to NC considered probably approximately correct (PAC)
[5, 39] concept learning. Vitter and Lin [42] introduced the notion of NC-learnable for concept
classes for which there is an algorithm that outputs a probably approximately correct hypothesis in
polylogarithmic time using a polynomial number of processors. In this setting, they proved positive
and negative learnability results for a number of concept classes that were previously known to be
PAC-learnable in polynomial time. More recently, the special case of learning half spaces in par-
allel was considered by Long and Servedio [21] who gave an algorithm for this case that runs on
polynomially many processors in time that depends polylogarithmically on the size of the instances
but is inversely proportional to a parameter of the learning problem. Our paper complements these
theoretical treatments of parallel machine learning and provides a provably effective parallelisation
scheme for a broad class of regularised risk minimisation algorithms.
Some parallelisation schemes also train learning algorithms on small chunks of data and average the
found hypotheses. While this approach has advantages [12, 32], current error bounds do not allow
a derivation of polylogarithmic runtime [20, 35, 45] and it has been doubted to have any benefit
over learning on a single chunk [34]. Another popular class of parallel learning algorithms is based
on stochastic gradient descent, targeting expected risk minimisation directly [34, and references
therein]. The best so far known algorithm in this class [34] is the distributed mini-batch algorithm
[10]. This algorithm still runs for a number of rounds inversely proportional to the desired opti-
misation error, hence not in polylogarithmic time. A more traditional approach is to minimise the
empirical risk, i.e., an empirical sample-based approximation of the expected risk, using any, deter-
ministic or randomised, optimisation algorithm. This approach relies on generalisation guarantees
relating the expected and empirical risk minimisation as well as a guarantee on the optimisation error
introduced by the optimisation algorithm. The approach is readily parallelisable by employing avail-
able parallel optimisation algorithms [e.g., 6]. It is worth noting that these algorithms solve a harder
than necessary optimisation problem and often come with prohibitively high communication cost in
distributed settings [34]. Recent results improve over these [22] but cannot achieve polylogarithmic
time as the number of iterations depends linearly on the number of processors.
Apart from its theoretical advantages, the Radon machine also has several practical benefits. In
particular, it is a black-box parallelisation scheme in the sense that it is applicable to a wide range
of machine learning algorithms and it does not depend on the implementation of these algorithms.
It speeds up learning while achieving a similar hypothesis quality as the base learner. Our empirical
evaluation indicates that in practice the Radon machine achieves either a substantial speed-up or a
higher predictive performance than other parallel machine learning algorithms.
8
7 Proof of Proposition 2 and Theorem 3
In order to prove Proposition 2 and consecutively Theorem 3, we first investigate some properties of
Radon points and convex functions. We proof these properties for the more general case of quasi-
convex functions. Since every convex function is also quasi-convex, the results hold for convex
functions as well. A quasi-convex function is defined as follows.
Definition 5. A function Q : F → R is called quasi-convex if all its sublevel sets are convex, i.e.,
∀θ ∈ R : {f ∈ F | Q (f) < θ} is convex.
First we give a different characterisation of quasi-convex functions.
Proposition 6. A function Q : F → R is quasi-convex if and only if for all S ⊆ F and all s′ ∈ 〈S〉
there exists an s ∈ S with Q (s) ≥ Q (s′).
Proof.
(⇒) Suppose this direction does not hold. Then there is a quasi-convex functionQ, a set S ⊆ F ,
and an s′ ∈ 〈S〉 such that for all s ∈ S it holds that Q (s) < Q (s′) (therefore s′ /∈ S). Let
C = {c ∈ F | Q (c) < Q (s′)}. As S ⊆ C = 〈C〉 we also have that 〈S〉 ⊆ 〈C〉 which
contradicts 〈S〉 3 s′ /∈ C.
(⇐) Suppose this direction does not hold. Then there exists an ε such that
S = {s ∈ F | Q (s) < ε} is not convex and therefore there is an s′ ∈ 〈S〉 \ S. By as-
sumption ∃s ∈ S : Q (s) ≥ Q (s′). Hence Q (s′) < ε and we have a contradiction since
this would imply s′ ∈ S.
The next proposition concerns the value of any convex function at a Radon point.
Proposition 7. For every set S with Radon point r and every quasi-convex function Q it holds that
|{s ∈ S | Q (s) ≥ Q (r)}| ≥ 2.
Proof. We show a slightly stronger result: Take any family of pairwise disjoint sets Ai with⋂
i〈Ai〉 6= ∅ and r ∈
⋂
i〈Ai〉. From proposition 6 follows directly the existence of an ai ∈ Ai
such that Q (ai) ≥ Q (r). The desired result follows then from ai 6= aj ⇐ i 6= j.
We are now ready to proof Proposition 2 and Theorem 3 (which we re-state here for convenience).
Theorem 3. Given a probability measure P over a hypothesis space F with finite Radon number r,
let F denote a random variable with distribution P . Denote by r1 the random variable obtained by
computing the Radon point of r random points drawn iid according to P and by P1 its distribution.
For any h ∈ N, let rh denote the Radon point of r random points drawn iid from Ph−1 and by Ph its
distribution. Then for any convex function Q : F → R and all ε ∈ R it holds that
P (Q(rh) > ε) ≤ (rP (Q(F ) > ε))2
h
.
Proof of Proposition 2 and Theorem 3. By proposition 7, for any Radon point r of a set S there must
be two points a, b ∈ S with Q (a) ,Q (b) ≥ Q (r). Henceforth, the probability of Q (r) > ε is less
than or equal to the probability of the pair a, b havingQ (a) ,Q (b) > ε. Proposition 2 follows by an
application of the union bound on all pairs from S. Repeated application of the proposition proves
Theorem 3.
Acknowledgements
Part of this work was conducted while Mario Boley, Olana Missura, and Thomas Ga¨rtner were at the
University of Bonn and partially funded by the German Science Foundation (DFG, under ref. GA
1615/1-1 and GA 1615/2-1). The authors would like to thank Dino Oglic, Graham Hutton, Roderick
MacKenzie, and Stefan Wrobel for valuable discussions and comments.
9
A Theory
A.1 Proof of Theorem 4
In the following, Theorem 4 is proven we which re-state here for convenience.
Theorem 4. The Radon machine with a consistent and efficient regularised risk minimisation al-
gorithm on a hypothesis space with finite Radon number has polylogarithmic runtime on quasi-
polynomially many processing units if the Radon number can be upper bounded by a function poly-
logarithmic in the sample complexity of the efficient regularised risk minimisation algorithm.y
Proof. We assume the base learning algorithm L to be a consistent and efficient regularised risk
minimisation algorithm on a hypothesis space with finite Radon number. Let r ∈ N be the Radon
number of the hypothesis space and
NL0 (ε, δ) =
(
αε + βε ld
1
δ
)k
be its sample complexity with αε, βε ≥ 0. In the following, we want to compare the runtime of
the Radon machine for achieving an (ε,∆)-guarantee to the runtime of the application of the base
learning algorithm for achieving the same (ε,∆)-guarantee.
To achieve an (ε,∆)-guarantee, the Radon machine with parameter h ∈ N requires N = nrh
examples (i.e., with rh processing units), where n denotes the size of the data subset available
to each parallel instance of the base learning algorithm. Since ∆ = (rδ)2
h
, each base learning
algorithm needs to achieve an (ε, δ)-guarantee and thus requires at least
n =
⌈
NL0 (ε, δ)
⌉ ≤ (αε + βε ld 1
δ
)k
+ 1 (8)
examples. The application of the base learning algorithm requires at least (cf. Equation 6)
M =
⌈
NL0 (ε,∆)
⌉
=
⌈(
αε + 2
h · βε ld 1
rδ
)k⌉
=
⌈(
αε + 2
h
(
βε ld
1
δ
− βε ld r
))k⌉
. (9)
Solving Equation 8 for βε ld 1/δ yields
βε ld
1
δ
≤ (n− 1) 1k − αε .
By inserting this into Equation 9 we obtain
M ≥
⌈(
αε + 2
h
(
(n− 1) 1k − αε − βε ld r
))k⌉
∈ O (2h (n− ld r)) . (10)
In the following, we show that for the choice of
h =
⌈
1
k
(ldM − ld ldM)
⌉
, (11)
the runtime of the Radon machine is polylogarithmic in M , i.e., polylogarithmic in the number
of examples the base learning algorithm requires to achieve the same (ε,∆)-guarantee. For that,
the Radon machine requires quasi-polynomially many processors in M . Note that the Radon ma-
chine processes N ≥ M many samples to achieve that (ε,∆)-guarantee, which is more than the
base learning algorithm requires by a factor in O (rh/2hk).
Thus, we need to express the runtime of the Radon machine, that is,
TR,h(N) = TL
(
N
rh
)
+ r3 logr r
h = TL (n) + r3 logr r
h ,
in terms of M instead of N . First, we express n in terms of M , by solving Equation 10 for n which
yields
n ≤
((
αε
(
1− 1
2h
)
+ βε ld r +
1
2h
M
1
k
)k
+ 1
)
∈ O
(
logk2 r +
1
2hk
M
)
. (12)
10
Since L is efficient, TL(n) ∈ O(nκ) and thus the runtime of the Radon machine in terms of M ,
denoted TMR , is
TMR = TL (n) + r
3 logr r
h ∈ O
((
logk2 r +
1
2hk
M
)κ
+ r3 ld rh
)
.
Inserting h as in Equation 11 yields(
logk2 r +
1
2hk
M
)κ
+ r3 ld
M
ldM
=
(
logk2 r +
M
2k
1
k ld
M
ldM
)κ
+ r3 ld
M
ldM
=
(
logk2 r +
M
M
ldM
)κ
+ r3 ld
M
ldM
=
(
logk2 r + ldM
)κ
+ r3 ld
M
ldM
.
This shows that
TMR ∈ O
(
ldκM + ldkκ r + r3 ldM
)
.
Thus, the runtime of the Radon machine to achieve an (ε,∆)-guarantee in terms of M (i.e.,
the number of samples required by the base learning algorithm to achieve that guarantee) is in
O
(
ldκM + ldkκ r + r3 ldM
)
and therefore polylogarithmic in M .
We now determine the number of processing units c = rh in terms of M . For that, observe that h as
in Equation 11 can be expressed as
h =
⌈
1
k
(ldM − ld ldM)
⌉
=
⌈
1
k
(
ld
M
ldM
)⌉
=
⌈
ld r
k
logr
M
ldM
⌉
.
With this the number of processing units is
c = rh ∈ O (M ld r) .
As mentioned in Section 3, for the Radon machine to achieve an (ε,∆)-guarantee each instance
of its base learning algorithm has to achieve δ ≤ 1/2r. Thus, the sample size with respect to M
has to be large enough so that each base learner achieves this minimum δ. Similar to the proof of
Theorem 4, we can express this minimum sample size in terms of M : The base learning algorithm
achieves δ ≤ 1/2r forM ≥ 2kβε(αε+1). This can be shown by first observing that Equation 9 implies
that for each instance of the base learning algorithm to achieve δ ≤ 1/2r it is required that
M ≥
(
αε + 2
h · βε ld 1
r 12r
)k
=
(
αε + 2
hβε
)k
=
(
αε +
(
M
ldM
) 1
k
βε
)k
. (13)
This holds for M ≥ 2kβε(αε+1) ≥ 2kβε , since
M ≥︸︷︷︸
ldM≥βkε (αε+1)k
M
ldM
βkε (αε + 1)
k
≥︸︷︷︸
( MldM )
1
k βε≥1
( M
ldM
) 1
k
βε
 αε(
M
ldM
) 1
k βε
+ 1
k = (αε + ( M
ldM
) 1
k
βε
)k
.
After having proven that the Radon machine has polylogarithmic runtime on quasi-polynomially
many processors, in the following section we analyse the speed-up over the base learning algorithm
which achieves the same (ε,∆)-guarantee.
11
A.2 Analysis of the Speed-Up of the Radon machine
In this section, we analyse the speed-up of the Radon machine over the execution of the base learning
algorithm when both achieve the same (ε,∆)-guarantee, as well as its inefficiency [17] and its data
inefficiency, i.e., how much more data the Radon machine requires compared to the base learning
algorithm which achieves the same (ε,∆)-guarantee. For that, recall that the sample complexity of
the base learning algorithm for a given ε > 0, 0 < ∆ < 1 is
NL0 (ε,∆) =
(
αε + βε log2
1
∆
)k
.
We assume that αε ∈ Θ(ε−1) and βε ∈ Θ(ε−1) (see for example Lemma 11 and Lemma 12).
Following the notion of Hanneke [14] the sample complexity can then be expressed as
NL0 (ε,∆) ∈ Θ
((
1
ε
+
1
ε
ld
1
∆
)k)
= Θ
((
1
ε
ld
1
∆
)k)
. (14)
Similar to Kruskal et al. [17], we assume the base algorithm to have a runtime polynomial in N , i.e.,
TL ∈ Θ (Nκ) = Θ
((
1
ε
ld
1
∆
)kκ)
. (15)
The Radon machine runs L in parallel on c processors to obtain rh weak hypotheses with (ε, δ)-
guarantee. It then combines the obtained solutions h times—level-wise in parallel—calculating the
Radon point (which takes time r3). In this paper we assume the number of available processors to
be abundant and thus set c = rh. With this, the runtime of the Radon machine is
TR ∈ Θ
((
1
ε
ld
1
δ
)kκ
+ hr3
)
. (16)
We now provide an analysis on the speed-up for c = rh and arbitrary h ∈ N.
Proposition 8. Given a polynomial time consistent regularised risk minimisation algorithm L using
a hypothesis space with finite Radon number r ∈ N and runtime as in Equation 15, the Radon
machine run with parameter h ∈ N on rh processors. Then, the ratio of the runtime of the base
learner over the runtime of the Radon machine, denoted the speed-up [17]
TL
TR
,
is in
Θ
 2hkκ
1 + hr
3
( 1ε ld
1
δ )
kκ
 .
Proof. In order to achieve an (ε,∆)-guarantee, the Radon machine runs rh parallel instances of the
the base learning algorithm on n = dNL0 (ε, δ)e examples with δ ≤ 1/2r so that ∆ = (rδ)2
h
. To
achieve the same (ε,∆)-guarantee, the base learning algorithm requires
M =
⌈
NL0 (ε,∆)
⌉
=
⌈(
2h · 1
ε
ld
1
rδ
)k⌉
∈ Θ
((
2h
1
ε
ld
1
rδ
)k)
= Θ
((
2h
1
ε
ld
1
δ
)k)
many examples. The last step follows from the fact that, since δ ≤ 1/2r, we have 1/rδ ≥ 2r/r ≥ r
and thus
ld
1
rδ
≤ ld 1
rδ
+ ld r = ld
1
δ
≤ 2 ld 1
rδ
⇒ ld 1
δ
∈ Θ
(
ld
1
rδ
)
⇔ ld 1
rδ
∈ Θ
(
ld
1
δ
)
,
12
To achieve the (ε,∆)-guarantee, the base learning algorithm has a runtime of
TL ∈ Θ (Mκ) = Θ
((
2h
1
ε
ld
1
δ
)kκ)
.
Using TR from Equation 16, we get that
TR
TL
∈Θ
((
1
ε ld
1
δ
)kκ
+ hr3(
2h 1ε ld
1
δ
)kκ
)
= Θ
(
1
2hkκ
(
1 +
hr3(
1
ε ld
1
δ
)kκ
))
The speed-up then is
TL
TR
∈ Θ
 2hkκ
1 + hr
3
( 1ε ld
1
δ )
kκ
 .
Note that the runtime of the Radon machine for the case that 1 ≤ c ≤ rh is given by
TR ∈ Θ
(
rh
c
((
1
ε
ld
1
δ
)kκ)
+ r3
h∑
i=1
⌈
ri
c
⌉)
.
In this case, the speed-up is lower by a factor of rh/c.
In the following, we analyse the inefficiency [17] of the Radon machine, i.e., the ratio between the
total number of operations executed by all processors and the work of the base learning algorithm.
Proposition 9. The Radon machine with a consistent and efficient regularised risk minimisation
algorithm on a hypothesis space with finite Radon number has quasi-polynomial inefficiency if the
Radon number is upper bounded by a function polylogarithmic in the sample complexity of the
efficient regularised risk minimisation algorithm.
Proof. Let L be a consistent and efficient regularised risk minimisation algorithm on a hypothesis
space with finite Radon number r ∈ N. Since L is efficient, its runtime TL(M) is in O(Mκ). From
the proof of Theorem 4 follows that, when choosing h =
⌈
1
k (ldM − ld ldM)
⌉
the Radon ma-
chine has a runtime of TR(M) ∈ O
(
ldκM + ldkκ r + r3 ldM
)
using c ∈ O (M ld r) processing
units. The inefficiency of the Radon machine then is
c · TR(M)
TL(M)
∈ O
M ld r
(
ldκM + ldkκ r + r3 ldM
)
Mκ
 ∈ O (M (ld r)−κ ldκM) = O (M ld r) .
Thus, the inefficiency of the Radon machine is quasi-polynomially bounded or, for short, it has
quasi-polynomial inefficiency.
In order to achieve the same (ε,∆)-guarantee as the base learning algorithm, the Radon machine re-
quires more data. In the following, we analyse the data inefficiency NR(ε,∆)/NL(ε,∆), i.e., the ratio
of the data required by the Radon machine over the data required by the base learning algorithm.
Proposition 10. The Radon machine with a consistent and efficient regularised risk minimisation
algorithm L with sample complexity NL(ε,∆) on a hypothesis space with finite Radon number
r ∈ N has a data inefficiency in
Θ
((
M
ldM
) ld r
k
)
,
where M = dNL(ε,∆)e.
13
Proof. We assume the sample complexity can be expressed as in Equation 14. For ∆ = (rδ)2
h
we
have that
NR(ε,∆) =rhNL(ε, δ) ∈ Θ
(
rh
(
1
ε
ld
1
δ
)k)
=Θ
(
rh
(
1
2h
1
ε
ld
1
∆
)k)
= Θ
(
rh
2hk
(
1
ε
ld
1
∆
)k)
=Θ
(
rh
2hk
NL(ε,∆)
)
= Θ
(
rh
2hk
M
)
.
Thus, the data inefficiency is in
Θ
(
rh
2hk
)
.
Choosing h = dk−1(ldM − ld ldM)e as in the proof of Theorem 4, this is in
Θ
(
r
1
k ld r logr
M
ldM
2k
1
k ld
M
ldM
)
= Θ
( MldM ) ld rk
M
ldM
 = Θ(( M
ldM
) ld r
k
)
B Experiments
This section provides additional details on the experiments conducted. All experiments are per-
formed on a Spark cluster with a master node, 5 worker nodes, 25 processors and 64GB of RAM
per node. The Radon machine is applied with parameter h = 1 and with the maximal h for a given
dataset: Recall, that the number of iterations h is limited by the dataset size (i.e., number of in-
stances) and the Radon number of the hypothesis space, since the dataset is partitioned into rh parts
of size n. Thus, given a data set of size N , the maximal h is given by
hmax =
⌊
logr
N
nmin
⌋
,
where nmin denotes the minimum size of the local subset of data that each instance of the base learner
is executed on. The experiments have been carried out with nmin = 100. As discussed in Section 5,
if rh is larger than the actual number of processing units, some instances of the base learner are
executed sequentially.
As base learning algorithms we use the WEKA [44] implementation of Stochastic Gradient Descent
(WekaSGD), and Logistic Regression (WekaLogReg), as well as a the Scikit-learn [29] implementa-
tion of the linear support vector machine (LinearSVM) with pyspark. The paralellisation of a base
learner using the Radon machine is denoted PRM(h=?)[<base learner>].
We compare the Radon machine to the natural baseline of aggregating hypotheses by calculating
their average, denoted averaging-at-the-end (Avg(h=?)[<base learner>]). Given a parameter h ∈
N, averaging-at-the-end executes the base learning algorithm on rh subsets of the data, i.e., on the
same number of subsets as the Radon machine. Accordingly, the runtime for obtaining the set of
hypotheses is similar, but the time for aggregating the models is shorter, since averaging is less
computationally expensive than calculating the Radon point.
We also compare the Radon machine to parallel machine learning algorithms from the
Spark machine learning library (MLlib): SparkMLLibLogisticRegressionWithLBFGS (Spark-
LogRegwLBFGS), SparkMLLibLogisticRegressionWithSGD (SparkLogRegwSGD), SparkMLLib-
SVMWithSGD (SparkSVMwSGD), and SparkMLLogisticRegression (SparkLogReg).
The properties of the datasets used in the empirical evaluation are presented in Table 1. Datasets have
been acquired from OpenML [40], the UCI machine learning repository [19], and Big Data compe-
tition of the ECDBL’14 workshop3. Experiments on moderately sized datasets—on which we com-
pare the Radon machine to the base learning algorithms executed on the entire dataset are conducted
3Big Data Competition 2014: http://cruncher.ncl.ac.uk/bdcomp/
14
Name Instances Dimensions Output
click prediction 1 496 391 11 Y = {−1, 1}
poker 1 025 010 10 Y = {−1, 1}
SUSY 5 000 000 18 Y = {−1, 1}
Stagger1 1 000 000 9 Y = {−1, 1}
HIGGS 11 000 000 28 Y = {−1, 1}
SEA 50 1 000 000 3 Y = {−1, 1}
codrna 488 565 8 Y = {−1, 1}
CASP9 31 993 555 631 Y = {−1, 1}
wikidata 19 254 100 2331 Y = {−1, 1}
20 newsgroups 399 940 1002 Y = {−1, 1}
YearPredictionMSD 515 345 90 Y ⊆ R
drift 13 991 90 Y = {1, . . . , 89}
spoken-arabic-digit 263 256 15 Y = {1, . . . , 10}
Table 1: Description of the datasets used in our experiments.
on the datasets click prediction, poker, SUSY, Stagger1, SEA 50, and codrna. The comparison
of Radon machine and Spark MLlib learners is executed on the datasets CASP9, HIGGS, wikidata,
20 newsgroups, and SUSY. The regression experiment is conducted using the YearPredictionMSD
dataset, multiclass-prediction experiments using the drift, and spoken-arabic-digit datasets.
In the following, we provide more details on the experiments presented in Figures 1(a), 1(b), and
1(c) in Section 4. In particular, we analyse the trade-off between training time and AUC per dataset.
Figure 5 shows the trade-off between training time and AUC for base learning algorithms and their
parallelisation using the Radon machine. It confirms that the training time for the Radon machine is
orders of magnitude smaller than the base learning algorithms on all datasets. Moreover, the training
time is substantially smaller for the Radon machine with maximal height (h = max), compared to
the parameter h = 1. In terms of AUC, the performance of the parallelisation is comparable to the
base learner for WekaLogReg and LinearSVC on all datasets. For the base learner WekaSGD, the
predictive performance of the parallelisation with the Radon machine is comparable on all datasets
but codrna. There, the Radon machine with parameter h = 1 has substantially lower AUC, while
the parallelisation with h = max has substantially higher AUC than the base learning algorithm
executed on the entire dataset.
The comparison of the Radon machine to the averaging-at-the-end baseline in Figure 6 confirms the
findings of Section 4, i.e., the Radon machine achieves a substantially higher AUC with only slightly
higher runtime. Comparing the Radon machine to the Spark MLlib learning algorithms in Figure 7
indicates that the Radon machine is always favourable in terms of training time. However, in terms
of AUC the results are mixed: For the base learner WekaLogReg, its parallelisation is always among
the best in terms of AUC. The parallelisation of WekaSGD, however, has worse performance than
the Spark learners on 2 out of 5 datasets. It also confirms that for the datasets SUSY and HIGGS,
the runtime of the Radon machine with h = 1 is substantially larger than for h = max. Thus, for
the best performance in terms of runtime and AUC, the height should be maximal.
In order to investigate the results depicted in Figure 7 more closely, we provide the training times
and AUCs in detail in Table 2. As mentioned above, the Radon machine using WekaLogReg as base
learner has better runtime than all Spark algorithms. At the same time, this version of the Radon
machine outperforms the Spark algorithms in terms of AUC on all datasets but 20 newsgroups—
there it is 2.2% worse than the best Spark algorithm. In particular, on the largest dataset in the
15
103 104
0.65
0.70
0.75
0.80
0.85
0.90
0.95
AU
C
codrna
103 104 105
0.975
0.980
0.985
0.990
0.995
1.000
1.005
Stagger1
103 104
0.82
0.83
0.84
0.85
SEA_50
103 104
training time (log-scale)
0.46
0.48
0.50
0.52
AU
C
poker
103 104 105
training time (log-scale)
0.5
0.6
0.7
0.8
0.9
1.0
click_prediction
103 104 105 106
training time (log-scale)
0.70
0.72
0.74
0.76
0.78
SUSY
WekaSGD
WekaLogReg
LinearSVC
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=1)[LinearSVC]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
PRM(h=max)[LinearSVC]
Figure 5: AUC vs. training time for base learning algorithms and their parallelisation with the Radon
machine per dataset from the same experiment as in Figure 1(a).
experiments—the CASP9 dataset with 32 million instances and 631 features—the Radon machine is
15% faster and 2.6% better in terms of AUC than the best Spark algorithm.
Note that for HIGGS and SUSY, the Radon machine with h = 1 is an order of magnitude slower than
with h = max as well as the Spark algorithms. This follows from the low degree of parallelisation,
since for h = 1 only 20 (for SUSY), respectively 30 (for HIGGS) hypotheses have to be generated.
Thus, only 20, or 30 of the 150 available processors are used in parallel. At the same time, the
amount of data each processor has to process is orders of magnitude larger than for h = max.
For the above experiments we assume that the data is already distributed over the nodes in the cluster
so that it can directly be processed by the Radon machine. When loading data in Spark, this data is
distributed over the worker nodes in subsetsbut not necessarily in rh subsets. In Spark, distributed
Dataset Runtime
SparkLogReg
wSGD
SparkSVM
wSGD
PRM(h=1)
[WekaSGD]
PRM(h=max)
[WekaSGD]
SparkLogReg
wLBFGS
SparkLogReg PRM(h=1)
[WekaLogReg]
PRM(h=max)
[WekaLogReg]
20 newsgroups 317.7 256.2 163.4 162.5 282.9 208.5 152.8 155.4
SUSY 7 439.5 5 961.8 27 781.6 1 363.7 6 526.3 4 516.8 26 299.6 1259.7
HIGGS 19 815.1 16 071.9 61 429.5 2 029.7 17 617.4 12 783.6 56 394.2 1876.2
wikidata 40 645.8 32 288.5 13 575.7 13 677.3 36 060.1 23 702.0 13 039.5 12845.5
CASP9 75 782.4 59 864.7 49 711.5 50 430.6 67 367.3 55 523.5 47 085.1 47070.1
AUC
20 newsgroups 0.6098 0.6075 0.4893 0.5063 0.63 0.6165 0.601 0.6226
SUSY 0.7454 0.7585 0.7134 0.7033 0.76 0.7652 0.7697 0.7814
HIGGS 0.5506 0.631 0.5753 0.5717 0.6257 0.6181 0.6237 0.6256
wikidata 0.1505 0.1004 0.0494 0.1002 0.1983 0.1489 0.1615 0.1974
CASP9 0.6181 0.6037 0.641 0.6514 0.6579 0.6454 0.6464 0.6622
Table 2: Runtime and AUC of Spark machine learning library algorithms and the Radon machine us-
ing WekaSGD and WekaLogReg as base learning algorithms. The results, reported for each dataset,
are the average over all folds in a 10-fold cross-validation. These results correspond to the ones
presented in Figure 1(c) in Section 4.
16
2 × 102
0.40
0.45
0.50
0.55
0.60
AU
C
20_newsgroups
103 104
0.625
0.650
0.675
0.700
0.725
0.750
0.775
SUSY
104
training time (log-scale)
0.50
0.52
0.54
0.56
0.58
0.60
0.62
HIGGS
104
training time (log-scale)
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
AU
C
wikidata
5 × 104
training time (log-scale)
0.56
0.58
0.60
0.62
0.64
0.66
CASP9
Avg(h=1)[WekaSGD]
Avg(h=1)[WekaLogReg]
Avg(h=max)[WekaSGD]
Avg(h=max)[WekaLogReg]
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
Figure 6: AUC vs. training time for the parallelisation of base learning algorithms using the
averaging-at-the-end baseline (Avg) and the Radon machine per dataset from the same experiment
as in Figure 1(b).
data is organised in partitions, where each partition corresponds to the subset of data available to
one instance of the base learning algorithm. In order to apply the Radon machine to a dataset within
the Spark framework, the data needs to re-distributed and partitioned into rh partitions which is
achieved by a method called repartition. In the experiments in Section 4, we assume that the data is
already partitioned to make a fair comparison to the Spark learning algorithms which do not require
repartitioning. Figure 8(a) illustrates the time required for repartitioning a dataset in contrast to
the runtime of the Radon machine. Repartitioning in Spark always includes a complete shuffling
of the data, requiring communication to redistribute the dataset. This is rather inefficient in our
context. Nonetheless, the time required for repartitioning is small compared to the overall runtime—
in the worst case it takes 14% of the runtime of the Radon machine. Still, taking into account the
time for repartitioning the data shrinks the runtime advantage of the proposed scheme over the
Spark algorithms. Figure 8(b) shows the runtimes of the Spark algorithms compared to the Radon
machine—similar to Figure 1(c) in Section 4—but with the time required for repartitioning the data
added to the runtime of the Radon machines. The Radon machine with h = max remains superior
to the Spark algorithms in terms of runtime.
C Practical Aspects
C.1 Radon Point Construction
In the following, a simple construction is given for a system of linear equations with which a Radon
point of a set can be determined. In his main theorem, Radon [30] gives the following construction
of a Radon point for a set S = {s1, ..., sr} ⊆ Rd. Find a non-zero solution λ ∈ R|S| for the
17
2 × 102 3 × 102
0.50
0.52
0.54
0.56
0.58
0.60
0.62
AU
C
20_newsgroups
104
0.70
0.72
0.74
0.76
0.78
SUSY
104
training time (log-scale)
0.54
0.56
0.58
0.60
0.62
HIGGS
2 × 104 3 × 1044 × 104
training time (log-scale)
0.050
0.075
0.100
0.125
0.150
0.175
0.200
AU
C
wikidata
5 × 104 6 × 104 7 × 1048 × 104
training time (log-scale)
0.61
0.62
0.63
0.64
0.65
0.66
CASP9
SparkLogRegwSGD
SparkSVMwSGD
SparkLogRegwLBFGS
SparkLogReg
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
Figure 7: AUC vs. training time for Spark learners and parallelisations of comparable base learning
algorithms with the Radon machine per dataset from the same experiment as in Figure 1(c).
following linear equations.
r∑
i=1
λisi = (0, . . . , 0) ,
r∑
i=1
λi = 0
Such a solution exists, since |S| > d+1 implies that S is linearly dependent. Then, let I, J be index
sets such that for all i ∈ I : λi ≥ 0 and for all j ∈ J : λj < 0. Then a Radon point is defined by
r(λ) =
∑
i∈I
λi
Λ
si =
∑
j∈J
λj
Λ
sj ,
where Λ =
∑
i∈I λi = −
∑
j∈J λj . Any solution to this linear system of equations is a Radon point.
The equation system can be solved in time r3. By setting the first element of λ to one, we obtain a
unique solution of the system of linear equations. Using this solution λ, we define the Radon point
of a set S as r(S) = r(λ) in order to resolve ambiguity.
C.2 Consistency Results for Empirical Risk Minimisation
In this section we provide some technical results on the consistency of empirical risk minimisation
algorithms.
Lemma 11. For consistent empirical risk minimisers with a hypothesis class of finite Vapnik-
Chervonenkis (VC) dimension the sample size required to achieve an (ε,∆)-guarantee is given
by N(∆) = (αε + βε log2 1/∆)
k with αε = 4 ln 21/ε2, βε = 4/ε2 log2 e and k = 2.
Proof. For consistent empirical risk minimisers with finite VC-dimension, the confidence 1−∆ for
a given N and ε is ∆ = 2N (F , N) exp(−Nε2/4) [43], where the shattering coefficient N (F , N)
18
po
ke
r
St
ag
ge
r1
HI
GG
S
wi
ki
da
ta
SU
SY
SE
A_
50
CA
SP
9
cli
ck
_p
re
d
co
dr
na
103
104
105
tim
e 
(lo
g-
sc
al
e)
PRM[WekaSGD]
PRM[WekaLogReg]
PRM[LinearSVC]
Data Partitioning
(a)
20
_n
ew
s
SU
SY
HI
GG
S
wi
ki
da
ta
CA
SP
9
102
103
104
105
tra
in
in
g 
tim
e 
(lo
g-
sc
al
e)
SparkLogRegwSGD
SparkSVMwSGD
SparkLogRegwLBFGS
SparkLogReg
PRM(h=1)[WekaSGD]
PRM(h=1)[WekaLogReg]
PRM(h=max)[WekaSGD]
PRM(h=max)[WekaLogReg]
(b)
Figure 8: (a) Runtime of the Radon machine together with the time required for repartitioning the
data to fit the parallelisation scheme. (b) Runtime and AUC of several Spark machine learning
library algorithms and the Radon machine including the time required for repartitioning the data
before training.
is a polynomial in N for finite VC-dimension. Solving for N yields that the algorithm run with
N ≥ 1
ε2
(
ln 2 + 4
1
log2(e)
log2
1
δ
)
achieves a confidence larger or equal to the desired 1−∆.
Lemma 12. For consistent empirical risk minimisers with a hypothesis class of finite Rademacher
complexity the sample size required to achieve an (ε,∆)-guarantee is given by N(∆) = (αε +
βε log2 1/∆)
k with αε = 0, βε = 1/2(ε+ 2ρ)2 and k = 1, where ρ denotes the Rademacher complex-
ity.
Proof. For consistent empirical risk minimisers with a hypothesis class of finite Rademacher com-
plexity ρ, a given ∆ and N the error bound is given by ε = 2ρ +
√
log2 1/δ/2N [43]. Solving for N
yields the above result.
19
References
[1] Sanjeev Arora and Boaz Barak. Computational complexity: A modern approach. Cambridge
University Press, 2009.
[2] Maria Florina Balcan, Yingyu Liang, Le Song, David Woodruff, and Bo Xie. Communication
efficient distributed kernel principal component analysis. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 725–
734, 2016.
[3] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3:463–482, 2003.
[4] Manuel Blum. A machine-independent theory of the complexity of recursive functions. Jour-
nal of the ACM (JACM), 14(2):322–336, 1967.
[5] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability
and the Vapnik-Chervonenkis dimension. Journal of the ACM (JACM), 36(4):929–965, 1989.
[6] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed op-
timization and statistical learning via the alternating direction method of multipliers. Founda-
tions and Trends® in Machine Learning, 3(1):1–122, 2011.
[7] Ashok K. Chandra and Larry J. Stockmeyer. Alternation. In 17th Annual Symposium on
Foundations of Computer Science, pages 98–108, 1976.
[8] Kenneth L. Clarkson, David Eppstein, Gary L. Miller, Carl Sturtivant, and Shang-Hua Teng.
Approximating center points with iterative Radon points. International Journal of Computa-
tional Geometry & Applications, 6(3):357–377, 1996.
[9] Stephen A. Cook. Deterministic CFL’s are accepted simultaneously in polynomial time and log
squared space. In Proceedings of the eleventh annual ACM symposium on Theory of computing,
pages 338–345, 1979.
[10] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online
prediction using mini-batches. Journal of Machine Learning Research, 13(1):165–202, 2012.
[11] Shai Fine and Katya Scheinberg. Efficient svm training using low-rank kernel representations.
Journal of Machine Learning Research, 2:243–264, 2002.
[12] Yoav Freund, Yishay Mansour, and Robert E. Schapire. Why averaging classifiers can protect
against overfitting. In Proceedings of the 8th International Workshop on Artificial Intelligence
and Statistics, 2001.
[13] Raymond Greenlaw, H. James Hoover, and Walter L. Ruzzo. Limits to parallel computation:
P-completeness theory. Oxford University Press, Inc., 1995.
[14] Steve Hanneke. The optimal sample complexity of PAC learning. Journal of Machine Learning
Research, 17(38):1–15, 2016.
[15] William B. Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert
space. Contemporary mathematics, 26(189-206):1, 1984.
[16] David Kay and Eugene W. Womble. Axiomatic convexity theory and relationships between
the Carathe´odory, Helly, and Radon numbers. Pacific Journal of Mathematics, 38(2):471–485,
1971.
[17] Clyde P. Kruskal, Larry Rudolph, and Marc Snir. A complexity theory of efficient parallel
algorithms. Theoretical Computer Science, 71(1):95–132, 1990.
[18] Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. Introduction to parallel
computing: design and analysis of algorithms. Benjamin-Cummings Publishing Co., Inc.,
1994.
20
[19] Moshe Lichman. UCI machine learning repository, 2013. URL http://archive.ics.
uci.edu/ml.
[20] Shao-Bo Lin, Xin Guo, and Ding-Xuan Zhou. Distributed learning with regularized least
squares. Journal of Machine Learning Research, 18(92):1–31, 2017. URL http://jmlr.
org/papers/v18/15-586.html.
[21] Philip M. Long and Rocco A. Servedio. Algorithms and hardness results for parallel large
margin learning. Journal of Machine Learning Research, 14:3105–3128, 2013.
[22] Chenxin Ma, Jakub Konecˇny´, Martin Jaggi, Virginia Smith, Michael I. Jordan, Peter Richta´rik,
and Martin Taka´cˇ. Distributed optimization with arbitrary local solvers. Optimization Methods
and Software, 32(4):813–848, 2017.
[23] Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S. Mann. Effi-
cient large-scale distributed training of conditional maximum entropy models. In Advances in
Neural Information Processing Systems, pages 1231–1239, 2009.
[24] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar-
cas. Communication-efficient learning of deep networks from decentralized data. In Artificial
Intelligence and Statistics, pages 1273–1282, 2017.
[25] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies
Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin,
Michael J. Franklin, Reza Zadeh, Matei Zaharia, and Ameet Talwalkar. Mllib: Machine learn-
ing in apache spark. Journal of Machine Learning Research, 17(34):1–7, 2016.
[26] Cleve Moler. Matrix computation on distributed memory multiprocessors. Hypercube Multi-
processors, 86(181-195):31, 1986.
[27] Ilia Nouretdinov, Sergi G. Costafreda, Alexander Gammerman, Alexey Chervonenkis,
Vladimir Vovk, Vladimir Vapnik, and Cynthia H.Y. Fu. Machine learning classification with
confidence: application of transductive conformal predictors to MRI-based diagnostic and
prognostic markers in depression. Neuroimage, 56(2):809–813, 2011.
[28] Dino Oglic and Thomas Ga¨rtner. Nystro¨m method with kernel k-means++ samples as land-
marks. In Proceedings of the 34th International Conference on Machine Learning, pages
2652–2660, 06–11 Aug 2017.
[29] Fabian Pedregosa, Gae¨l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, RRon Weiss, Vincent Dubourg, Jake
Vanderplas, AAlexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and
E´douard Duchesnay Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
[30] Johann Radon. Mengen konvexer Ko¨rper, die einen gemeinsamen Punkt enthalten. Mathema-
tische Annalen, 83(1):113–115, 1921.
[31] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Ad-
vances in Neural Information Processing Systems, pages 1177–1184, 2007.
[32] Jonathan D. Rosenblatt and Boaz Nadler. On the optimality of averaging in distributed statis-
tical learning. Information and Inference, 5(4):379–404, 2016.
[33] Alexander M. Rubinov. Abstract convexity and global optimization, volume 44. Springer
Science & Business Media, 2013.
[34] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Proceed-
ings of the 52nd Annual Allerton Conference on Communication, Control, and Computing,
pages 850–857, 2014.
[35] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization
using an approximate newton-type method. In International conference on machine learning,
pages 1000–1008, 2014.
21
[36] Robin Sommer and Vern Paxson. Outside the closed world: On using machine learning for
network intrusion detection. In Symposium on Security and Privacy, pages 305–316, 2010.
[37] Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright. Optimization for machine learning.
MIT Press, 2012.
[38] John W Tukey. Mathematics and the picturing of data. In Proceedings of the International
Congress of Mathematicians, volume 2, pages 523–531, 1975.
[39] Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142,
1984.
[40] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked
science in machine learning. SIGKDD Explorations, 15(2):49–60, 2013.
[41] Vladimir N. Vapnik and Alexey Y. Chervonenkis. On the uniform convergence of relative
frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):
264–280, 1971.
[42] Jeffrey S. Vitter and Jyh-Han Lin. Learning in parallel. Information and Computation, 96(2):
179–202, 1992.
[43] Ulrike Von Luxburg and Bernhard Scho¨lkopf. Statistical learning theory: models, concepts,
and results. In Inductive Logic, volume 10 of Handbook of the History of Logic, pages 651–
706. Elsevier, 2011.
[44] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. Data Mining: Practical
machine learning tools and techniques. Elsevier, 2017.
[45] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Communication-efficient algorithms
for statistical optimization. Journal of Machine Learning Research, 14(1):3321–3363, 2013.
[46] Martin Zinkevich, Markus Weimer, Alexander J. Smola, and Lihong Li. Parallelized stochastic
gradient descent. In Advances in Neural Information Processing Systems, pages 2595–2603,
2010.
22
",186274913,"{'doi': None, 'oai': 'oai:arXiv.org:1810.03530'}",Effective Parallelisation for Machine Learning,"{'code': 'en', 'name': 'English'}",2018-10-08T00:00:00+00:00,,[],['http://arxiv.org/abs/1810.03530'],,2018,"[{'type': 'download', 'url': 'http://arxiv.org/abs/1810.03530'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/186274913'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/186274913?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=8&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","We present a novel parallelisation scheme that simplifies the adaptation of
learning algorithms to growing amounts of data as well as growing needs for
accurate and confident predictions in critical applications. In contrast to
other parallelisation techniques, it can be applied to a broad class of
learning algorithms without further mathematical derivations and without
writing dedicated code, while at the same time maintaining theoretical
performance guarantees. Moreover, our parallelisation scheme is able to reduce
the runtime of many learning algorithms to polylogarithmic time on
quasi-polynomially many processing units. This is a significant step towards a
general answer to an open question on the efficient parallelisation of machine
learning algorithms in the sense of Nick's Class (NC). The cost of this
parallelisation is in the form of a larger sample complexity. Our empirical
study confirms the potential of our parallelisation scheme with fixed numbers
of processors and instances in realistic application scenarios.Comment: Advances in Neural Information Processing Systems, 201","['text', 'Computer Science - Machine Learning', 'Computer Science - Artificial Intelligence', 'Computer Science - Distributed, Parallel, and Cluster Computing', 'Statistics - Machine Learning']",disabled
,"[{'name': 'Kuwajima, Hiroshi'}, {'name': 'Yasuoka, Hirotoshi'}, {'name': 'Nakae, Toshihiro'}]",[],2019-02-06T06:49:49+00:00,"{'name': 'arXiv.org e-Print Archive', 'url': 'https://api.core.ac.uk/v3/data-providers/144'}",,research,,http://arxiv.org/abs/1812.03057,"Open Problems in Engineering andQuality Assurance
of Safety Critical Machine Learning Systems
Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae
DENSO CORPORATION
Kariya, Aichi
{HIROSHI_KUWAJIMA,HIROTOSHI_YASUOKA,TOSHIHIRO_NAKAE}@denso.co.jp
ABSTRACT
Fatal accidents are a major issue hindering the wide accep-
tance of safety-critical systems using machine-learning and
deep-learning models, such as automated-driving vehicles.
Quality assurance frameworks are required for suchmachine-
learning systems, but there are no widely accepted and es-
tablished quality-assurance concepts and techniques. At the
same time, open problems and the relevant technical fields
are not organized. To establish standard quality assurance
frameworks, it is necessary to visualize and organize these
open problems in an interdisciplinary way, so that the ex-
perts from many different technical fields may discuss these
problems in depth and develop solutions. In the present study,
we identify, classify, and explore the open problems in qual-
ity assurance of safety-critical machine-learning systems,
and their relevant corresponding industry and technological
trends, using automated-driving vehicles as an example. Our
results show that addressing these open problems requires
incorporating knowledge from several different technologi-
cal and industrial fields, including the automobile industry,
statistics, software engineering, and machine learning.
1 INTRODUCTION
The recent development in the machine learning techniques
such as deep neural networks has led to the widespread ap-
plication of systems that assign advanced environmental per-
ception and decision-making to computer logics learned from
big data, instead of manually built rule-based logics [4]. Avail-
able big data and affordable high performance computing
such as deep learning framework on off-the-shelf GPU [24]
made highly complex machine learning techniques practical.
Machine-learning models are becoming indispensable com-
ponents even in systems that require safety-critical environ-
mental perception and decision-making, such as automated-
driving systems [1]. For human society to accept safety-
critical machine-learning systems however, it is important
to develop common quality-assurance (QA) frameworks for
managing the risks of usingmachine-learningmodels [3]. QA
has an impact on the social receptivity, because it can be one
of the approaches to deliver safety and security. In fact, re-
cent accidents caused during the use of several experimental
automated vehicles have revealed QA frameworks as impera-
tive for addressing this upcoming social issue [2]. However,
although the automated-driving technologies are being ac-
tively developed and proposed on a daily basis, QA concept
and technologies to ensure safety and security have not
been systematized yet. Therefore, in this study, we organize
the review open QA problems on safety-critical machine-
learning systems using in-vehicle automated-driving sys-
tems equipped with machine-learning models as an example.
The contributions of this study are:
• Proposed points of view to clarify open QA problems
on safety-critical machine-learning systems;
• Proposed possible open QA problems on safety-critical
machine-learning systems along with the POV;
• Shown the recent trends addressing the open prob-
lems;
• Shown ideas to address some of the open problems;
• Shown the relevant industrial fields and the next re-
search directions to address the open problems.
2 QUALITY ASSURANCE OF IN-VEHICLE
AUTOMATED-DRIVING SYSTEMS
An automated-driving vehicle is a vehicle that operates with-
out human input. Automated driving is not built as a stand-
alone system in a vehicle. To realize automated driving,
a system consisting of clouds, roadside devices (fog), and
automated-driving vehicles (edge) [6] are necessary that
creates and updates high-precision digital maps [5], and co-
operates with peripheral vehicles. An in-vehicle automated-
driving system installed in a vehicle is composed of multiple
subsystems for perception, planning, and control, and it re-
alizes automated-driving operations in cooperation with the
clouds and roadside units. Each perception, planning, and
control subsystem may contain machine-learning models,
as necessary. Supervised-learning models [7] and decision-
reinforcement learning models [8] can be used for perception
and planning, and conventional control theory can be used
for control, respectively. On the other hand, in order to per-
form QA on a system, it is necessary to predefine quality
standards and a QA process in advance, and strictly follow
them at development time. In this study, we identify open
1
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems Kuwajima et al.
problems related to the three levels: the in-vehicle automated-
driving system, the subsystems, and the machine learning
model. Thus, we use an automated-driving system as an ex-
ample of a safety-critical machine-learning system. Then, we
investigate the open QA problems in terms of the three steps
of the development process: design (requirements and speci-
fications), verification (including validation), and operation
(including maintenance). The three levels and three steps
are shown in Fig. 1. Open problems that can be addressed
using conventional methods are beyond the scope of this
research study and are therefore excluded. Notably, many of
the open problems considered in this paper do not occur only
in automated-driving systems, but in safety-critical systems
in general.
3 OPEN PROBLEMS OF IN-VEHICLE
AUTOMATED-DRIVING SYSTEMS
In-vehicle automated-driving systems are automated-driving
systems that are installed in vehicles. In the three layers of
the system operating on an automated vehicle: the in-vehicle
automated-driving system, the subsystems, and the machine
learning model, only the former is exposed to the physi-
cal environment. Therefore, it is necessary to consider data
on the physical environment more than theoretical assump-
tions when designing and verifying an in-vehicle automated-
driving system. The open problems in this paragraph are to
capture the operational physical environments of automated-
driving systems, and address the intractableness of field op-
eration testing (FOT).
Requirement: QA requires limitations, orwarranty scopes.
Therefore, a machine-learning system must be implemented
in a predefined environment. In the automobile industry, this
is called the operational design domain (ODD), and it essen-
tially corresponds to the driving conditions with which the
system is compatible [10]. Efficiently collecting driving sce-
narios on actual roads from past field operational tests and
customer data remains a huge task and an open challenge in
QA, but it is necessary for defining assumed environments
and verifying the developed QA system against them. In
addition, the evaluation criteria (such as test coverage) must
be defined for the collected scenarios, in order to measure
whether they are sufficient to ensure safety and security.
Verification: The simplest approach to guaranteeing the
quality of an in-vehicle automated driving system is through
verification against actual data. Accumulating a large num-
ber of safe automated-driving trips, as long distance as with
human drivers, will effectively demonstrate that automated-
driving systems are as safe as human drivers. However, ob-
taining statistically significant results would require conduct-
ing FOT by driving for hundreds of thousands to hundreds
of billions of miles FOT [11]. In order to verify the system
within a realistic time-frame, it is necessary to accelerate
the experiment by simulation, reproducing the corner-case
scenarios on test courses with a short mileage, i.e., scenarios
with extremely low probability of occurrence and difficult to
statistically obtain (rare) by FOT on the actual road.
3.1 Trend
The German PEGASUS project is a joint initiative of vehicle
manufacturers (OEM), suppliers, tool vendors, certification
organizations, and research institutes, which aims to define
standard QA methods for automated-driving systems [12].
The purpose of this project is to clarify the expected per-
formance level and evaluation criteria of automated-driving
systems through scenario-based verification. The scope of
the project includes standard test procedures, continuous
and flexible tool chains, integration of tests into the develop-
ment processes, cross-company test methods, requirements
definition methods, driving scenarios, a common database
of scenarios, as well as creating safety statements based on
them. Each scenario is defined by three levels of abstraction:
the functional scenario (natural language description), the
logical scenario (parameterized model), and the concrete sce-
nario (model with specified parameters). First, in the concept
phase, natural languages are used to describe the functional
scenario without ambiguous terms, in a way that can be un-
derstood by human experts. Next, in the system development
phase, we define the parameters and their distributions, and
describe the logical scenario in a specified data format. In
the test & verification phase, specific parameters are sam-
pled from the parameter distributions, and the scenario is
detailed into a concrete scenario, so that it can to be executed
(simulated) with test tools. Scenarios are collected from test
drives and the market to demonstrate that systems are equal
to or better than human drivers. Regular scenarios are con-
tinuously tested by simulation, and those critical are tested
through artificially configured environments on test courses.
The PEGASUS project is an excellent example of continuous
definition of requirements for (in-vehicle) automated-driving
systems and their verification. The open problems in QA for
the in-vehicle automated-driving system level require sta-
tistics and simulation techniques to develop solutions for
scenario collection and verification, respectively. Thereafter,
it is expected that the automobile industry (especially vehicle
manufacturers) will select the appropriate solutions and lead
the standardization process.
4 OPEN PROBLEMS OF SUBSYSTEMS
Subsystems run on top of the in-vehicle automated-driving
system to provide special functionalities. The open problems
in this paragraph are to design subsystems which include
machine learning models, by considering and applying the
2
Open Problems in Engineering and Quality Assurance
of Safety Critical Machine Learning Systems
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems
In-Vehicle Automated 
Driving System
Subsystem
Machine Learning
Model
Simulating 
Driving 
Scenarios
Validating 
Architecture 
Design
Coverage 
Verification and 
Visualization of 
ML Models
Continuous
Collection of 
Driving 
Scenarios
Handling New 
Use Cases w/o 
Retraining
Consistency 
Checking 
Operation Data 
and Test Data
(No Change)
Redundantt 
Architecture to 
Mitigate 
Uncertainty
Specifying 
Reproducible 
Training 
Process
Collection 
Process of 
Driving 
Scenarios
Describing 
Statistical 
Properties
Describing Test 
Data Property, 
Model Quality
Verification Operation
Specification
Requirement
Design
Specification
Requirement
Specification
Requirement
Figure 1: Development Process of Safety-Critical ML Systems (Example: In-Vehicle Automated-Driving Systems)
characteristics of machine learning, such as uncertainty and
""Change Anything Change Everything"" (CACE) [15] .
Requirement: Subsystems with machine-learning mod-
els have uncertainty derived from the machine-learning mod-
els. Therefore, it is necessary to develop methods for describ-
ing this uncertainty as a statistical measure.
Design: Thus, QA must be conducted independently for
each subsystem including those incorporatingmachine-learning
models. As a result, QA of subsystems that do not usemachine-
learning models can be performed through conventional
methods, with minimal change in the development process.
Subsystems with different functionalities, such as percep-
tion and planning, may require different QA approaches. For
example, the same QA approach cannot be applied to per-
ception with a Convolutional Neural Network [7] and to
planning with a Deep Q-Network [8]. In general, a machine-
learningmodel cannot achieve 100% accuracy on test data [13],
and as accuracy approaches 100%, further performance im-
provement becomes difficult. Therefore, optimizing machine-
learning models are not the only way to improve subsystem
performance. In fact, a subsystem should be subdivided and
verified based on its development process, combining the con-
ventional deductive approach with the inductive approach
that is unique to machine learning [14]. Rigorous breakdown
of subsystem requirements into machine-learning model re-
quirements is essential for safety-critical machine-learning
systems. In this process, safety analysis methods and pro-
cesses are important, such as: black box verification; encapsu-
lation of machine-learning models by rule-based safeguard-
ing; redundant & diverse architecture that mitigates and
absorbs the uncertainty of machine learning models; system
modification patterns to cope with additional requirements
without modification of the machine-learning model, in or-
der to avoid the principle of CACE as much as possible.
Verification: Moreover, in order to prevent reworking
after training the machine-learning models, which is a heavy
3
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems Kuwajima et al.
heuristic process, it is necessary to verify the validity of the
redundant architecture. Assuming that a machine learning
model behaves probabilistically based on the predetermined
uncertainty, we would like to know if the uncertainty of the
entire system is absorbed or mitigated, before training the
model.
4.1 Trend
System-level technologies STAMP/STPA [16] and GSN [17]
have received attention in recent years. They are highly
promising, but they will need an extension that can express
the uncertainty related to the machine-learning models. In
addition, SOTIF, which is a safety standard/process concern-
ing performance limits of normal functions, is also focused
on securing functions with uncertainty even under normal
conditions [18]. We consider that the most relevant technical
area for QA at the subsystems level is software engineering.
Subsystems are the intermediate layer between the deduc-
tive (manual) development of systems and the inductive
(data driven) development of machine-learning models. The
safety-critical properties of systems should be managed at
the subsystem level and handed to the machine-learning
models.
5 OPEN PROBLEMS OF MACHINE
LEARNING MODELS
A machine-learning model is acquired by executing a train-
ing algorithm (training) with a model structure and training
data sets as inputs. A machine-learning model before deter-
mining the parameters by training is called a model structure
in this paper. Trained machine models are evaluated using
test datasets [19]. The open problems in this paragraph are to
deductively design the requirements of the machine learning
models and their test data, as the final step manual engineer-
ing in machine-learning systems.
Requirement: In machine learning, we consider that the
roles of training data and test data are different. While the
training data is used for improving the performance of the
machine-learning model, the test data should be used to ac-
curately reflect the environmental conditions in operation.
For example, when all the data obtained at the time of de-
velopment are randomly divided, some of them are used as
training data and the others are test data. In this way, the
training data and the test data are approximately equally
distributed, but the relationship to the operational data is
unknown. Particularly in a safety-critical machine-learning
system, it is necessary to design the test data distribution
considering the operational environment, in which the sys-
tem will actually be operated in the future, and to collect test
data based on the designed data distribution. In this way, by
accepting an a-priori viewpoint of the test data distribution,
we can deductively define the assumed environment, and
inductively collect data based on the requirements of the
overlaid subsystem. Moreover, by assuming the distribution
of the test data, we can now discuss their distribution, and
define the operation domain with concrete design informa-
tion.
Even in the current development of in-vehicle automated-
driving systems, the test data would be collected assuming
the operational environment, in order to make the distri-
bution at the time of operation and the distribution of the
test data as consistent as possible. However, the methods of
describing the assumed environment of machine-learning
models are not organized. In particular, we need methods to
describe the properties of the test data. Furthermore, since
the required performance may change depending on the
assumed environment, it is necessary to express the associ-
ation between the assumed environment and the required
performance. Test data should have attributes such as time
and weather, and their distributions, which are based on
the assumed environment. Each condition of the test data
distribution should have a confusion matrix which machine
learning models will have them as desired value (Table 1).
In Table 1, the confusion matrix (accuracy) is shown as a
requirement of the machine-learning model, but there are
other quality characteristics of machine-learningmodels that
should be investigated. In order to use machine-learning
models in safety-critical systems, it is necessary to define
and evaluate models by the performance indicators from
various perspectives besides accuracy. If we refer to the soft-
ware quality model [20], then accuracy (confusion matrix),
resource behavior (required memory), time behavior (exe-
cution time), fault tolerance (robustness against Adversarial
Examples [22], hereinafter referred to as AE), analyzabil-
ity (ease of internal visualization or interpretability), and
testability (simple model structure advantageous for formal
verification) appear to be relevant perspective (performance
indicator) to machine-learning models.
Design: A machine-learning model is automatically ob-
tained by training the model structure using the training
data. Thus, specifications cannot be designed a priori. This
limitation is essential, because high-performance machine-
learning models are realized by learning high-dimensional
parameters from data that engineers cannotmanually specify.
However, in the development of a safety-critical machine-
learning system, at a minimum, it is necessary to record
the model structure, the training data, and the training sys-
tem (including hyper parameters, initial parameters, random
number seeds, etc.) to secure the reproducibility of the train-
ing process. In this way, specification is achieved indirectly.
Operational data tend to change with time, creating a ma-
jor problem for the operation and maintenance of machine-
learning models [21]. In this way, the deviation between test
4
Open Problems in Engineering and Quality Assurance
of Safety Critical Machine Learning Systems
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems
Table 1: top - sample environment (data distribution), bottom - sample requirements (confusion matrix)
time
day night
weather fine 40% † 30%rain 20% 10% ‡
fine × day † prediction
pedestrian vehicle
actual pedestrian 90% 10%vehicle 20% 80%
rain × night ‡ prediction
pedestrian vehicle
actual pedestrian 85% 15%vehicle 15% 85%
† e.g., when fine day there are many pedestrians, therefore precision on pedestrian is prioritized.
‡ e.g., when rainy night there are many vehicles, therefore precision on vehicle is prioritized.
data used during development and operation data becomes
large as time elapses from completion of development. It is
important to check the consistency between the operational
data and the test data (originally assumed environment), and
to make the machine-learning models follow the operational
data in a continuous maintenance process.
Verification: Some quality characteristics cannot be eval-
uated with test data. For example, when evaluating robust-
ness against AE as fault tolerance, it is necessary to artificially
generate perturbations around the original test data points.
We can generate AE near each individual test data point
and quantify robustness by maximizing the AE to which the
model can give correct answers. Besides robustness, there are
also demands for measurement methods of various quality
characteristics as listed above.
5.1 Trend
Increasing stability against disturbance, that is, enhancing
robustness is a key to QA. AE occurs when the image recog-
nition model incorrectly recognizes slight noise that cannot
be recognized by humans with high confidence [22]. AE is
known to have model-independent versatility and is an is-
sue that threatens the safety of automated-driving systems
depending on image-recognition technology. Study on AE is
important not only for defending intentional attacks such as
false traffic signs, but also for understanding the formation of
decision boundaries of machine learning model and realizing
highly robust recognition functions.
Inference processes of advanced machine learning models
such as neural networks (NN) are considered black boxes.
In particular, safety-critical systems should exhibit inter-
pretability and transparency. In this context, a black box
refers to a situation, where, although feature activations can
be observed physically, the actual phenomenon cannot be
understood. Regarding interpretability, NN visualization [23]
shows great promise. In NN visualization, one method in-
tentionally performs occlusion on input data, and specifies
the region where the inference result changes drastically
as a region of interest; another method back-propagates ac-
tivation values from the influencer nodes at the later fea-
ture extraction process to identify the region of interest. NN
visualization may have use in cases of debugging during
training, and validation of the training result (understand-
ing the internal behavior of the trained NN). Therefore, one
open problem is obtaining concrete use cases to address the
interpretability and transparency of safety-critical machine-
learning systems, by using NN visualization, interpretation,
transparency methods.
In the previous paragraphs, we presented the current
trends in machine-learning technology related to QA of
machine-learning models. Furthermore, even in the field
of theoretical computer science, toward a verification of
machine-learning model, automatic coverage verification
based on formal verification technology is becoming possi-
ble (Table 2). The technical areas necessary for QA at the
machine-learning model level are machine learning and the-
oretical computer science.
6 CONCLUSION
With the development of rapid technology in recent years,
machine learning has been used in various systems. In order
to use machine learning in a safety-critical system such as
an automated-driving system, it is necessary to demonstrate
the safety and security of the system to society through
QA. In this paper, taking automated driving as an exam-
ple, we presented the open problems and corresponding
research/industry trends from the viewpoint of design, ver-
ification, and operation for in-vehicle automated-driving
systems, subsystems, and machine-learning models. As a
result, we found that the automobile industry (standardiza-
tion), statistics, software engineering, machine learning, and
5
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems Kuwajima et al.
Table 2: Trend on Automatic Coverage Verification for Machine-Learning Models
 	
  
	
	
 
	
		
 	
	 		

				 	 	

	
	
	 


 	
		

					
			  		
 	
		
!"" 	##	
					
		$%&

		 	 	
! ""
	
	
# $
%&&
'( 		
)			
)	""!		*	
	+$%,
		' 
	 
		(  	
 )

	
&&	
	&&
*
+
*	&&
- 					
*#				$%,
				 	'
	
	 	! 


%&&
'(			""#./		0	

				*#	
			$%,
""	#	 	,'

			 
-,'	
*
		
*

* 	  #	
+		!	
 			1	
 #		+$%,
$%	
%	 &''	(	 &&
			
 
* 	  		
				
	 	)		
""12$%,
	

 	






	 	

	
	 

 	



 	
	 	
&&
-*
			 
3 				
	""!			
	$%4





  
	

!





  
	
	
""
#$%
  
&'() 	

 *
			.  	""	
					

$%,
.			
+			
+	

*
%		

*
theoretical computer science are the relevant industrial fields
and the next research directions to investigate for solving
the open problems of in-vehicle automated-driving systems,
subsystems, and machine-learning models, respectively.
In order to enable QA of safety-critical machine-learning
systems, research on elemental technology is also necessary,
but even if each company carries out its own QA based on
its own concepts, the results cannot be widely accepted by
human society. Experts from each specialized field as men-
tioned in this paper should gather together, create technically
reasonable drafts, proposing QA processes, and form con-
sensus (standardization) overviewing the worldwide trends
of safety-critical machine-learning systems. To this end, the
present study can form a basis for future discussion and in-
terdisciplinary research, with the aim to create a draft QA
process.
REFERENCES
[1] Li Erran Li, Anca Dragan, Juan Carlos Niebles, Silvio Savarese: 2017
NIPS Workshop on Machine Learning for Intelligent Transportation
Systems, NIPS, 2017.
[2] Report of Traffic Collision Involving an Autonomous Vehicle (OL 316),
State of California DMV, 2018.
[3] Koopman, P. and Wagner, M.: Challenges in Autonomous Vehicle
Testing and Validation, SAE Int. J. Trans. Safety, 2016.
[4] Sarah Bird, Dan Crankshaw, Garth Gibson, Joseph Gonzalez, Aparna
Lakshmiratan, Li Erran Li, Christopher Re, Siddhartha Sen: Workshop
on ML Systems at NIPS 2017, NIPS, 2017.
[5] Philippe Gicquel: Autonomous VEHICLES need Reliable Dynamic Map
Data, Geospatial World, 2015.
[6] Helder Antunes: OpenFog Consortium: Growing Strong at the Inter-
section of Fog and IoT, Cisco Blogs, 2017.
[7] Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton: ImageNet Classi-
fication with Deep Convolutional Neural Networks, NIPS, 2012.
[8] Volodymyr Mnih, Koray Kavukcuogl, David Silver, Alex Graves, Ioan-
nis Antonoglou, Daan Wierstra, Martin Riedmiller: Playing Atari with
Deep Reinforcement Learning, NIPS, 2013.
[9] Stuart Bennett: Development of the PID Controller, IEEE Control
Systems, 1993.
[10] Automated Driving Systems: A Vision for Safety, NHTSA, 2017.
[11] Nidhi Kalra, Susan M. Paddock: Driving to Safety How Many Miles of
Driving Would It Take to Demonstrate Autonomous Vehicle Reliabil-
ity?, RAND Corporation, 2016.
[12] PEGASUS âĂŤ Effectively ensuring automated driving, VDA Technical
Congress, 2017.
6
Open Problems in Engineering and Quality Assurance
of Safety Critical Machine Learning Systems
Published at ICML Workshop for
Deep Learning for Safety-Critical in Engineering Systems
[13] Mike Loukides: The machine learning paradox, Ideas âĂŤ O’Reilly
Media, 2017.
[14] H. Madala: Comparison of Inductive Versus Deductive Learning Net-
works, Complex Systems, 1991.
[15] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young: Machine Learning:
The High-Interest Credit Card of Technical Debt, Google AI, 2014.
[16] Nancy Leveson: A Systems Approach to Risk Management Through
Leading Safety Indicators, Journal of Reliability Engineering and Sys-
tem Safety, 2015.
[17] Tim Kelly and Rob Weaver: The Goal Structuring Notation âĂŤ A
Safety Argument Notation, DSN Workshop on Assurance Cases, 2004.
[18] Wilhard Wendorff: Quantitative SOTIF Analysis for highly automated
Driving Systems, Safetronic, 2017.
[19] Kevin Murphy: Machine learning: a probabilistic perspective, The MIT
Press, 2012.
[20] ISO/IEC 9126 Software engineering âĂŤ Product quality âĂŤ Part 1
Quality model, 2001.
[21] Alexey Tsymbal: The problem of concept drift: Definitions and related
work, Technical Report, Department of Computer Science, Trinity
College, 2004.
[22] Alexey Kurakin, Ian Goodfellow, Samy Bengio: Adversarial Attacks
and Defences, NIPS 2017 Competition Track, Kaggle, 2017.
[23] Felix Grun, Christian Rupprecht, Nassir Navab, Federico Tombari:
A Taxonomy and Library for Visualizing Learned Features in Con-
volutional Neural Networks, ICML Visualization for Deep Learning
Workshop, 2016.
[24] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan
Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell: Caffe: Con-
volutional Architecture for Fast Feature Embedding, arXiv preprint
arXiv:1408.5093, 2014.
7
",186300827,"{'doi': None, 'oai': 'oai:arXiv.org:1812.03057'}","Open Problems in Engineering and Quality Assurance of Safety Critical
  Machine Learning Systems","{'code': 'en', 'name': 'English'}",2018-12-07T00:00:00+00:00,,[],['http://arxiv.org/abs/1812.03057'],,2018,"[{'type': 'download', 'url': 'http://arxiv.org/abs/1812.03057'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/186300827'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/186300827?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=7&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Fatal accidents are a major issue hindering the wide acceptance of
safety-critical systems using machine-learning and deep-learning models, such
as automated-driving vehicles. Quality assurance frameworks are required for
such machine learning systems, but there are no widely accepted and established
quality-assurance concepts and techniques. At the same time, open problems and
the relevant technical fields are not organized. To establish standard quality
assurance frameworks, it is necessary to visualize and organize these open
problems in an interdisciplinary way, so that the experts from many different
technical fields may discuss these problems in depth and develop solutions. In
the present study, we identify, classify, and explore the open problems in
quality assurance of safety-critical machine-learning systems, and their
relevant corresponding industry and technological trends, using
automated-driving vehicles as an example. Our results show that addressing
these open problems requires incorporating knowledge from several different
technological and industrial fields, including the automobile industry,
statistics, software engineering, and machine learning.Comment: DISE1: Joint Workshop on Deep (or Machine) Learning for
  Safety-Critical Applications in Engineerin","['text', 'Computer Science - Computers and Society', 'Computer Science - Machine Learning', 'Statistics - Machine Learning']",disabled
,"[{'name': 'Reijonen, Joel'}]","['Vaasan yliopisto', 'fi=Tekniikan ja innovaatiojohtamisen yksikkö|en=School of Technology and Innovations|']",2019-10-29T18:46:37+00:00,"{'name': 'Osuva', 'url': 'https://api.core.ac.uk/v3/data-providers/5594'}",,,,https://core.ac.uk/download/233002133.pdf,"UNIVERSITY OF VAASA         
THE SCHOOL OF TECHNOLOGY AND INNOVATIONS 
AUTOMATION AND COMPUTER SCIENCE 
 
 
 
 
Joel Reijonen 
MASTER’S THESIS 
Decentralized Machine Learning for Autonomous Ships in Distributed Cloud En-
vironment 
 
 
Master’s thesis for the degree of Master of Science in Technology submitted for inspec-
tion, Vaasa, 1 November 2018. 
 
 
Thesis supervisor Prof. Mohammed Elmusrati 
Thesis instructors D.Sc. Miika Komu 
 M.Sc. Miljenko Opsenica 
 2 
PREFACE 
“Decentralized Machine Learning for Autonomous Ships in Distributed Cloud Environ-
ment” has been an educational project where I had an opportunity to gain expertise espe-
cially in cloud computing, machine learning and autonomous ships from experts that are 
working in Nomadiclab, Ericsson Research Finland. During this project, my team and I 
filed a couple of invention disclosures regarding the topic of this thesis. 
I would like to sincerely thank my supervisor Professor Mohammed Elmusrati, instructor 
D.Sc. (Tech.) Miika Komu and instructor M.Sc. (Tech.) Miljenko Opsenica for excellent 
guidance, support and feedback. In addition, I would also like to address my gratitude to  
Jan Melén, Jimmy Kjällman and Jani-Pekka Kainulainen for their supportive feedback 
and assistance. 
Finally, I would like to thank my family and my girlfriend Nikolina for their altruistic and 
continuous support during on both this project and studies. 
Jorvas, 1.11.2018 
Joel Reijonen 
 3 
TABLE OF CONTENTS 
PREFACE 2 
TABLE OF CONTENTS 3 
ABBREVATIONS 7 
ABSTRACT 8 
TIIVISTELMÄ 9 
1 INTRODUCTION 10 
1.1 Objective of the Thesis 11 
1.2 Structure of the Thesis 12 
2 FOUNDATIONS 13 
2.1 Machine Learning 13 
2.1.1 Supervised Learning 14 
2.1.2 Unsupervised Learning 16 
2.1.3 Other Learning Methods 17 
2.2 Distributed Cloud Computing 19 
2.2.1 Cloud Computing 19 
2.2.2 Distributed Cloud Environment 20 
2.3 Microservices 22 
2.3.1 Introduction to Microservices 22 
2.3.2 Container Technologies 24 
2.4 Orchestration 26 
2.4.1 Kubernetes 27 
 4 
2.4.2 Container Orchestration 28 
2.5 Data Preparation 29 
2.5.1 Noise Removal 29 
2.5.2 Redundancy Removal 30 
2.5.3 Imputation and Excluding Methods 30 
2.6 Summary 30 
3 PLATFORM REQUIREMENTS 32 
3.1 Use case: Autonomous Ships 32 
3.1.1 Connectivity and Communication 33 
3.1.2 Sensor Fusion 33 
3.1.3 Optimization of Engine Performance 34 
3.2 Machine Learning Agent 35 
3.2.1 Interoperability Requirements 35 
3.2.2 Orchestration Requirements 36 
3.2.3 Reusability Requirements 37 
3.2.4 Performance Requirements 37 
3.3 Data Preparation Module Requirements 38 
3.4 Summary 38 
4 DESIGN PROCEDURES 40 
4.1 Architectural Design of the Machine Learning Agent 40 
4.1.1 Functional Design 40 
4.1.2 Deployment Design 41 
4.2 Machine Learning Design 43 
4.2.1 Supervised Learning Design 43 
4.2.2 Conditional Learning Design 44 
 5 
4.2.3 Decentralized Learning Design 45 
4.2.4 Fitness Evaluation Design 46 
4.2.5 Deduction Design 47 
4.3 Data Preparation Module Design 47 
4.3.1 Architecture 48 
4.3.2 Redundancy Removal Design 49 
4.3.3 Missing Data Handling Design 49 
4.3.4 Noise Removal Design 49 
4.4 Selection of Mathematical Methods 50 
4.4.1 Regression 50 
4.4.2 Least Squares Estimation 52 
4.4.3 Root Analysis of the Derived Function 53 
4.5 Summary 54 
5 IMPLEMENTATION PROCESS 56 
5.1 Implementation of the Data Preparation Module 56 
5.1.1 Duplicate Removal 57 
5.1.2 Listwise Deletion 57 
5.1.3 Moving Average Filter 57 
5.2 Implementation of Machine Learning 58 
5.2.1 Supervised Learning: Regression 59 
5.2.2 Conditional Learning 60 
5.2.3 Decentralized Learning 61 
5.3 Implementation of Deduction Logic 63 
5.3.1 Fitness Evaluation and Model Selection 63 
5.3.2 Global Optimum and Efficiency Aggregation 64 
 6 
5.4 Virtual Implementation 66 
5.4.1 Container Implementation 67 
5.4.2 Deployment with Kubernetes 68 
5.5 Testbed 69 
5.5.1 Autonomous Ship Prototype 70 
5.5.2 Functionality of the Prototype 71 
5.5.3 Distributed Cloud Environment 71 
5.6 Summary 71 
6 ANALYSIS AND EVALUATION 73 
6.1 Evaluation of Functionality 73 
6.1.1 Evaluation of Data Preparation Module 73 
6.1.2 Evaluation of Machine Learning 75 
6.1.3 Evaluation of Decentralized Learning 76 
6.2 Evaluation of Optimized Performance 78 
6.2.1 Evaluation of State-Specific Machine Learning: Sailing 79 
6.2.2 Evaluation of State-Specific Machine Learning: Docking 82 
6.3 Evaluation of Virtualized Agent 84 
6.3.1 Evaluation of Container Interoperability 84 
6.3.2 Evaluation of Orchestration 86 
6.4 Summary 87 
7 CONCLUSION AND DISCUSSION 89 
REFERENCES 92 
8 APPENDIX 96 
A. Alternative Solution with TensorFlow 96 
 7 
ABBREVATIONS 
Agent 
API 
 Machine learning agent 
Application Programmable Interface 
FIR  Finite Impulse Response 
HTTP  Hypertext Transfer Protocol 
IIR  Infinite Impulse Response 
IPC  Inter-Process Communication 
IT  Information Technology 
ML 
NoSQL 
 Machine Learning 
Non Structured Query Language 
OS  Operating System 
RO  Recursive Optimization 
RPI  Raspberry Pi 
VPN  Virtual Private Network 
XML  Extensive Markup Language 
XPS  Extruded Polystyrene 
YAML  YAML Ain’t Markup Language 
 
 
 8 
UNIVERSITY OF VAASA  
The School of Technology and Innovations 
Author:  Joel Reijonen 
Topic of the Thesis:  Decentralized Machine Learning for Autonomous 
Ships in Distributed Cloud Environment 
Supervisor:  Prof. Mohammed Elmusrati 
Instructor:  D.Sc Miika Komu 
 M.Sc Miljenko Opsenica 
Degree:  Master of Science in Technology 
Major of Subject:  Automation and computer science 
Year of Entering the University:  2014  
Year of Completing the Thesis:  2018  Pages: 95 
ABSTRACT 
Machine learning is a concept where a computing machine is capable to improve its own 
performance through experience or training. Machine learning has been adopted as an 
optimization solution in broad field of information technology (IT) industry. In addition, 
the availability of data has become more and more easier since the effective data storage 
and telecommunication technologies such as new generation cloud computing are devel-
oping. Cloud computing refers to a network-centric paradigm which provides additional 
computational resources and a scalable data storage. Even though the utilization of cloud 
computing enables improved performance of machine learning, cloud computing in-
creases the overall complexity of the system as well. 
In this thesis, we develop a machine learning agent which is an independent software 
application that is responsible for the implementation and integration of decentralized 
machine learning in a distributed cloud environment. Decentralization of machine learn-
ing enables parallel machine learning between multiple machine learning agents that are 
deployed in multiple clouds. In addition to the development of machine learning agent, 
we develop a data preparation module which ensures that the data is clean and complete. 
We develop the machine learning agent and the data preparation module to support con-
tainer implementation by taking advantage in Docker container platform. Containeriza-
tion of the applications facilitates portability in multi-cloud deployments and enables ef-
ficient orchestration by utilizing Kubernetes. In this thesis, we do not utilize existing ma-
chine learning frameworks but rather we implement machine learning by applying known 
mathematical methods. 
We have divided the development of the software applications in three phases: require-
ment specification, design and implementation. In requirement specification, we describe 
the essential features that are required to be included. Based on the requirements, we de-
sign the applications to fulfill expectations and respectively we utilize the design to guide 
the implementation. In the final chapter of this thesis, we evaluate functionality, ability 
to enhance performance and virtualized implementation of the applications. 
KEYWORDS: Decentralized machine learning, distributed cloud computing, data 
preparation, containerization, orchestration 
 9 
VAASAN YLIOPISTO 
Tekniikan ja innovaatiojohtamisen yksikkö 
Tekijä: Joel Reijonen 
Diplomityön nimi: Hajautettu koneoppiminen autonomisille laivoille 
hajautetussa pilviympäristössä 
Valvojan nimi: Prof. Mohammed Elmusrati 
Ohjaajan nimi: TkT Miika Komu 
 DI Miljenko Opsenica 
Tutkinto: Diplomi-insinööri 
Oppiaine: Automaatio ja tietotekniikka 
Opintojen aloitusvuosi: 2014  
Diplomityön valmistumisvuosi: 2018 Sivumäärä: 95 
TIIVISTELMÄ 
Koneoppiminen tarkoittaa käsitettä, jossa tietokone kykenee parantamaan koneen suori-
tuskykyä kokemusten tai opetuksen kautta. Koneoppimista hyödynnetään laajalti infor-
maatioteknologian teollisuuden optimointiratkaisuissa. Tämän lisäksi datan saatavuu-
desta on tullut entistä helpompaa datan tallennus- ja tietoliikenneteknologioiden, kuten 
uuden sukupolven pilvilaskennan, kehittyessä. Pilvilaskenta viittaa tietoverkkoihin pe-
rustuvaan paradigmaan, joka tarjoaa sekä laskennallisia lisäresursseja, että skaalautuvaa 
datan tallennustilaa. Vaikka pilvilaskennan hyödyntäminen parantaa koneoppimisen suo-
rituskykyä, se lisää myös järjestelmän yleistä kompleksisuutta. 
Tässä diplomityössä kehitetään koneoppimista suorittava agentti, joka on itsenäinen oh-
jelmisto. Agentti vastaa hajautetun koneoppimisen toimeenpanemisesta ja integraatiosta 
hajautetussa pilviympäristössä. Hajautettu koneoppiminen mahdollistaa useiden agent-
tien rinnakkaisen koneoppimisen useissa pilviympäristöissä. Agentin lisäksi kehitämme 
datan valmistelumoduulin, joka takaa, että koneoppimisessa käytetty data on puhdasta ja 
eheää. 
Agentti ja datan valmistelumoduuli kehitetään siten, että ne tukevat kontitettua käyttöön-
ottoa hyödyntäen Docker-konttialustaa. Sovellusten käyttöönotto konteissa edistää niiden 
siirrettävyyttä yhdistetyissä pilviympäristöissä ja mahdollistaa tehokkaan orkestroinnin 
Kuberneteksen avulla. Tässä diplomityössä ei hyödynnetä valmiiksi luotuja koneoppimi-
seen käytettäviä viitekehyksiä, vaan toteutetaan koneoppimista soveltaen tunnettuja ma-
temaattisia menetelmiä. 
Diplomityössä sovellusten kehittäminen on jaettu kolmeen vaiheeseen: vaatimusmäärit-
tely, suunnittelu ja toteutus. Vaatimusmäärittelyssä määritetään sovellusten välttämättö-
mät ominaisuudet, jotka tulisi sisällyttää suunnittelussa ja toteutuksessa. Vaatimusmää-
rittelyjen pohjalta suunnitellaan sovellukset siten, että ne vastaavat vaatimuksia ja vastaa-
vasti hyödynnetään suunnitelmaa toteutuksessa. Lopuksi arvioidaan sovellusten toimin-
nallinen, suorituskykyä parantava vaikutus ja virtualisoitu toteutus. 
AVAINSANAT: Hajautettu koneoppiminen, hajautettu pilviympäristö, datan valmis-
telu, konttiteknologiat, orkestrointi 
 10 
1 INTRODUCTION 
The popularity of machine learning applications has increased over the past years in the 
field of information technology (IT) due to increasing amounts of available data (Smola 
& Bishwanathan 2008: 3). Machine learning is a concept where the computing machine 
is able to extract additional information from the data that is fed into the system. A ma-
chine utilizes the extracted information to learn and derive a reasonable result which can 
be used, e.g., in predictions, conclusions or decision-making operations. In most cases, 
the increased amount of data produces more precise results. For this reason, the data stor-
age scalability and  access together with cleaned data are fundamental requirements for 
machine learning. 
Industrial trend is towards to having increasing number of devices that can be connected 
to the Internet1. Multiple connected devices increase the necessity of faster connectivity, 
larger data storage capability and higher amount of computational resources. To meet 
these expectations, the concept of cloud computing is one of the solutions that has gained 
reputation among the IT industry. Cloud computing enables network accessible and scal-
able deployment of data storage which grant additional computational resources (CPU, 
GPU, RAM, etc). 
Cloud-based environments provide a potential response for resource demanding machine 
learning tasks. Clouds take advantage in virtualization of the mounted hardware which 
enables high scalability in the resources (Sosinsky 2011:3-4.). Scalable resources enable 
efficient management of the resources since a cloud does not necessarily has to reserve 
resources for operations that are not running continuously. Respectively, the cloud scales 
out the resources for the operations that have increase in demand. Furthermore, in order 
to locally scale resources, the cloud can be interconnected with other clouds and this way 
expand the overall amount of computational resources. An environment that consists of 
multiple connected clouds is known as distributed cloud environment. 
                                                 
1 See for further information from Ericsson mobility visualizer: https://www.ericsson.com/en/mobility-
report/mobility-visualizer?f=1&ft=1&r=2,3,4,5,6,7,8,9&t=8&s=1,2,3&u=1&y=2017,2023&c=1 
 11 
In this thesis, we design and implement decentralized machine learning to optimize the 
performance of autonomous ships. Autonomous ships are self-acting ships that are com-
posed of the usage of sensor fusion, control algorithms, communication and connectivity 
(Rolls-Royce 2016). We utilize sensor fusion to provide reliable data for machine learn-
ing which continuously strives to improve the efficiency of control algorithms. Moreover, 
we also take advantage in communications and connectivity when we decentralize ma-
chine learning between local and non-local clouds. 
In this thesis, the distributed cloud environment consists of interconnected clouds in au-
tonomous ships, harbors and data centers. Multiple connected clouds facilitate efficient 
computational load balancing for decentralized machine learning that, on the other hand, 
enables parallelism in learning. 
1.1 Objective of the Thesis 
In this thesis, we utilize parallel machine learning in order to harness computational re-
sources from multiple clouds. Consequently, multi-cloud environment allows us to utilize 
even constrained resources for machine learning. 
We implement and integrate data preparation module and decentralized machine learning 
agent in a distributed cloud environment. Data preparation module is a software applica-
tion that guarantees the quality of the data that is used in machine learning. The quality 
of the data is a key factor when reliable learning results are desired. In this thesis, we 
utilize only the data that has been processed by the data preparation module. 
Respectively, decentralized machine learning agent is responsible for operating machine 
learning related operations in various cloud-based environments. The agent is an inde-
pendent software application which strives to improve overall performance of the system 
(Russel & Norvig 1995: 7). In the design and implementation of the agent, we consider 
“As a Service” -principles together with microservice architecture oriented development. 
 12 
1.2 Structure of the Thesis 
This thesis consists of seven chapters. Chapter 2 introduces relevant background theories 
and technologies that support the objective of this thesis. Chapter 3 defines the required 
features for data preparation module and machine learning agent, and presents an use case 
which sets certain requirements in design. Chapter 4 describes functional and deployment 
architecture of the components. In chapter 5, we describe the implementation of the com-
ponents and use case specific testbed. Chapter 6 consists of an analysis and an evaluation 
that describes the performance of the implemented components. Finally, we discuss about 
the results and conclude this thesis in chapter 7. 
 13 
2 FOUNDATIONS 
In this chapter, we review the essential technology involving machine learning, distrib-
uted cloud computing and orchestration. First, we introduce the concepts of machine 
learning and how they can be utilized to foster the improved overall performance of the 
system. Secondly, we review the features of the cloud-based environment to enlighten the 
opportunities and challenges that they include. We deploy, manage and run applications 
as microservices in clouds where the environment specific advantages are utilized. Mi-
croservices, as an alternative software development concept, are reviewed together with 
orchestration as an approach that supports the development and management of cloud-
based applications. Finally, we review the concepts of data preparation since clean and 
complete data support more precise machine learning. 
2.1 Machine Learning 
Machine learning is a concept which refers to the machine’s ability to improve its own 
performance independently through experiences of the past or learning from examples 
(Brink et al. 2017: 3). Applications of machine learning are especially effective when the 
solution algorithm of the model is unknown or hard to determine, and when there are 
large amounts of data that needs to be processed. The goal of the machine learning is to 
optimize the parameters of the defined model by taking advantage of past experiences or 
examples. (Alpaydin 2010: 1–3) 
Machine learning strives to extract hidden information from the data by utilizing mathe-
matical methods such as theories in probability calculus and matrix algebra. However, 
proper extraction of the information does not always guarantee improved performance 
since the learned data may be incomplete, corrupted or it might include noise. Noise is an 
unwanted anomaly in the data which is mostly caused due to inaccuracies in measure-
ments. (Alpaydin 2010: 30–31; Tan & Jiang 2013: 3) 
 14 
 
Figure 1. Machine learning techniques and some common methods.(Reconstructed        
  from Hwang & Chen 2017: 33.) 
Hwang & Chen (2017: 32) have defined the main machine learning techniques: super-
vised learning, unsupervised learning and other learning methods such as reinforcement 
learning, active learning and transfer learning (Figure 1). Machine learning techniques 
have different characteristics which should be considered in the design of the machine 
learning application. A certain technique may lead to the better results in optimization 
than using another learning technique. 
2.1.1 Supervised Learning 
Supervised learning is a technique where learning is based on training from examples 
which are provided by a supervisor. The supervisor is responsible for serving the system 
with a training set of labeled data which consists real observed input and output values. 
In supervised learning, learning utilizes the training set to learn generalized functionality 
of the system in such a way that the machine performs desired actions also in situations 
which have not been described in the training.  (Sutton & Barto 2017: 2–3) 
In supervised learning, the machine tries to fit a certain model which is based on the 
findings of the trained data. Alpaydin (2010: 9) has introduced classification and regres-
sion methods where the inputs are mapped to outputs by using supervised learning (Figure 
2). 
 15 
 
Figure 2. Classification categorizes inputs to certain classes whereas regression
  maps inputs to numerical output values.  
Classification is a procedure that determines for which output or class do the sampling of 
inputs belong to. A function that maps the inputs to a certain class is called discriminant. 
In supervised learning the learned classification rule can be used to predict the classes of 
the inputs that have not been introduced in the training. Classification can be used in 
applications such as pattern recognition and natural language processing. (Alpaydin 2010: 
5–8; Brink et al. 2017: 8) 
Regression, on the other hand, is a procedure where the inputs of the system are mapped 
on outputs that are numeric values. In the supervised learning, the goal of regression is to 
train a model which maps inputs to outputs as precisely as possible. Regression model 
can be used to approximate the output values of certain inputs that are not represented in 
the training set. Regression is used in applications such as stock-market prediction, price 
estimation and risk management. (Alpaydin 2010: 10–11; Brink et al. 2017: 8) 
The following formula defines how supervised learning can be used to solve classification 
and regression assignments (Alpaydin 2010: 9): 
y = g(x|θ) , 
 16 
where function g(.) represents the model, θ represents the parameters of the function 
and y represents a number in regression or class in classification. Supervised machine 
learning strives to optimize the parameters to fit the most satisfying model. 
2.1.2 Unsupervised Learning 
Unsupervised learning is a technique that does not utilize the observed output values and 
where the supervisor is not introduced (Alpaydin 2010: 11). Unsupervised techniques 
implement the learning operations by using the information of unlabeled data. A machine 
that utilizes unsupervised learning pursues to extract main features and structures of the 
input data and performs deductions from the findings (Brink et al. 2017: 26; Sutton & 
Barto 2017: 2). 
 
Figure 3. Clustering divides similar inputs into same clusters 
Alpaydin has defined that the goal of unsupervised learning is to find repeating behavior 
of the input data where inputs can be divided into clusters or groups (Figure 3). Infor-
mation of repeating behavior can be used to observe in which structures the similar oc-
currences of certain patterns occur more often and where not. The observation procedure 
is also known as density estimation. Concept of dividing inputs into clusters is also known 
as clustering which is one of the density estimation methods. (Alpaydin 2010: 11) 
 17 
Clustering is a procedure where inputs with similar features and attributes are allocated 
in the same cluster. Clustering has no priori output values, so the construction of clusters 
is completely based on the information extracted from the input values. Clustering utilizes 
methods such as partitioning, density-based models and model-based methods. Applica-
tions that take advantage in clustering include, e.g., image analysis, data mining and bio-
informatics. (Alpaydin 2010: 11–12; Bijuraj 2013: 169, 172) 
2.1.3 Other Learning Methods 
Other learning methods are techniques which may have similarities with supervised or 
unsupervised techniques but yet they have significant differences in their functionalities 
to be categorized differently. Other learning techniques include methods such as rein-
forcement learning, transfer learning and active learning. In this thesis, we introduce re-
inforcement learning and use it as an example how other learning techniques differ from 
supervised and unsupervised techniques. (Hwang & Chen 2017: 33–34; Sutton & Barto 
2017: 2) 
In reinforcement learning, the machine is rewarded if the actions or decisions that the 
machine has made have increased overall performance in a certain environment (Figure 
4). The machine strives to learn which actions or decisions in a certain situation guarantee 
the maximized reward. Reinforcement learning relies on the machine to discover the set 
of desired actions by itself when the initial information about possibly rewarding actions 
is not provided. Although the machine does not have preliminary information, the ma-
chine must be served with responses related to the state of the environment, and the ma-
chine should have a determined objective relating to the state of the environment. The 
more actions affect positively on the objective of the machine, the better reward machine 
receives. (Sutton & Barto 2017: 1-2; Alpaydin 2010: 447–448) 
 18 
 
Figure 4. Machine receives rewards if taken actions improve performance of the  
  machine in its environment. (Reconstructed from Sutton & Barto 2017:     
  38.) 
Reinforcement learning differs from supervised learning since in reinforcement learning 
the training set of examples is not given. In supervised learning, a supervisor provides a 
training set of examples, but reinforcement learning does not employ any supervisor. Re-
inforcement learning is a learning procedure which strives to maximize the reward, and 
it does not provide information about the actions that should be taken but, instead, it pro-
vides information on how good the action was. (Sutton & Barto 2017: 1–2; Alpaydin 
2010: 448) 
Reinforcement learning does not belong to unsupervised learning methods since rein-
forcement learning pursues to maximize rewards instead of trying to find repeating be-
havior or the structure of the input data. Sutton & Barto (2017:2) have presented the idea 
of having more than two categories of learning techniques in the following way: “We 
therefore consider reinforcement learning to be a third machine learning paradigm, along-
side supervised learning and unsupervised learning and perhaps other paradigms as 
well.”. 
 
 
 19 
2.2 Distributed Cloud Computing 
Nowadays the number of devices connected to the Internet has increased significantly 
which raises the requirements for connectivity, computing and data storage resources. 
One solution to tackle this problem is to utilize computation in a cloud. 
Cloud computing refers to network accessible and scalable deployment of data storage 
which is capable of providing additional computational power and other resources. The 
benefits of cloud computing include lower software expenses due to reduced infrastruc-
tural maintenance, extensive access, shared environment and a standardized approach that 
supports integration of multiple platforms (Sosinsky 2011: 399). In this thesis, a platform 
that performs computation in multiple joint clouds is defined as distributed cloud envi-
ronment. 
2.2.1 Cloud Computing 
The idea of centralized cloud computation and processing has raised its reputation during 
the past years due to its efficiency, scalability and accessibility. In this idea, the remote 
computation and processing of the information are handled in external data centers that 
supply network-centric computing and content management. Popularity of network-cen-
tric processing has led to the development of a paradigm called cloud computing where 
virtually shared resources are shared in a distributed network (Figure 5). (Marinescu 
2013: 1) 
 20 
 
Figure 5. Concepts of cloud computing. (Reconstructed from Marinescu 2013: 2.) 
“Cloud computing refers to applications and services that run on a distributed network 
using virtualized resources and accessed by common Internet protocols and networking 
standards.” (Sosinsky 2011:3). Cloud computing provides remotely accessible and scala-
ble resources and its popularity as a data storage solution has increased in the past years. 
Sosinsky (2011:4) has introduced how the word ‘cloud’ refers to two main concepts in 
cloud computing which are abstraction and virtualization. 
Abstraction in cloud computing means that the applications are running in unspecified 
physical systems, location of data storage is hidden from the user, and administration of 
systems is maintained by someone else. Virtualization on the other hand means that the 
cloud computing virtualizes mounted systems by pooling and shares resources in such a 
way that the resources are scalable. (Sosinsky 2011:3-4.) 
2.2.2 Distributed Cloud Environment 
Distributed cloud environment is a concept where multiple clouds are connected to each 
other. Clouds that form a distributed cloud environment can have differences in their fea-
tures and computational resources. Different types of computational operations, such as 
sensor data collection and long term storing, take place in different clouds depending on 
 21 
the architecture of the distributed cloud environment. Often the features change as the 
distance grows from the data source. 
In this thesis the architecture of distributed cloud environment consists of three different 
clouds: edge, regional and central clouds (Figure 6). 
 
Figure 6. Distributed cloud environment consists of multiple connected clouds. 
  (Reconstructed from Ericsson 2018). 
Edge cloud is regarded as a cloud environment which is close to the end-user in this thesis. 
Edge cloud utilizes paradigm called edge computing which refers to the augmentation of 
computational capability at the edge of a network (Wang et al. 2017: 290). Edge compu-
ting reduces the network bandwidth usage and decreases the latency in the edge cloud. 
Edge cloud is a more constrained environment compared to the other cloud types espe-
cially when it comes to the overall computational resources and the reduced size of the 
data storage. In this thesis, the connectivity is also considered as a constrained resource 
in the edge cloud. 
Regional cloud is a cloud environment that is bound to certain region. The concept of 
regional cloud is developed to guarantee that the cloud computational services are sup-
ported by the actors of certain area. Singh et al. (2014: 3) have defined the motivations 
behind the development of regional clouds with the following example: “An example is 
the proposal for a Europe-only cloud. Though there is often little detail surrounding the 
 22 
rhetoric – indeed, the concept is fraught with questions and complexity – it generally 
represents an attempt at greater governance and control”. In this thesis, regional clouds 
are part of distributed cloud environment where the management of the clouds is restricted 
to certain location. 
In this thesis, the central cloud is located in a centralized data center which provides scal-
able computational resources on demand. Central cloud promotes remote accessibility 
which facilitates the computational load balancing in distributed cloud environment. Cen-
tral cloud also acts as a long-term storage for the gathered data which supports analysis 
and monitoring of the devices and the cloud metrics.  
2.3 Microservices 
Cloud computing has gained a foothold in the IT industry, yet it has also declared novel 
challenges in software design. Cloud based systems are expected to improve overall reli-
ability of usage and efficiency in performance and scalability but simultaneously they 
increase the complexity of the system. Increased overall complexity has led to a develop-
ment of new design models such as microservice architectures and container technolo-
gies. (Hong & Bayley 2018: 152) 
2.3.1 Introduction to Microservices 
Traditionally software applications have been designed as monolithic applications which 
associates multiple software components into a single entity. Despite monolithic applica-
tions are quite common, they become more challenging to maintain and scale when the 
complexity of the system increases.  
Components of monolithic applications rely strongly on each other which means that the 
components have to be managed, maintained and deployed as one aggregated entity. Due 
to their ponderous maintenance and deployment, the current industry trend is towards 
 23 
microservices which are small and independent components that are responsible for han-
dling their own operations (Figure 7). (Lukša 2018: 2–4; Rodger 2018: 46) 
 
Figure 7.  Similar operations running in monolithic application and in micro-
services-based application. (Reconstructed from Lukša 2018: 3.) 
Microservices are intentionally developed as small and self-acting components which are 
relatively easy to maintain as standalone software. Rodger (2018: 35) has introduced the 
definition which states that the microservices should not have more than 100 lines of 
programmed code. Development of such small services are especially efficient when it 
comes to the debugging of the component or reconstruction of the code. 
Hong & Bayley (2018: 154) have defined the benefits of preferring microservices over 
monolithic applications in cloud-based environment: continuous software evolution, 
seamless technology integration, optimal runtime performance, horizontal scalability and 
reliability through fault tolerance. These benefits foster the development, deployment and 
management of the system due to microservices’ ability to receive individual updates and 
to scale resources individually.  
Even though deployment of microservices has their pros there are also cons that need to 
be considered. Lukša (2018:5) has explained some challenges where the deployment-re-
lated decisions become more difficult as the amount of microservices increases. Lukša 
(2018:5) has also pointed out that the challenges are also harder to overcome if the amount 
 24 
of deployment combinations increases which causes the increase in inter-dependencies 
of the components as well. 
Parallel microservices share information between each other by utilizing technique called 
inter-process communication (IPC). Frequent communication of multiple microservices 
introduce another challenge where increasing overhead reduces overall performance of 
the system by increasing latency (Hammar 2014: 5, 35). 
There are multiple options on how to overcome these challenges2 but, in this thesis, the 
deployment of microservices is handled with Kubernetes and Linux containers. Kuber-
netes is an orchestration system that supports deployment and maintenance of containers. 
Kubernetes will be reviewed more in-depth in section 2.4.1. 
2.3.2 Container Technologies 
Container technologies can support microservices in such a way that the software com-
ponents and required resources of a microservice are packed into a container image. Con-
tainers typically run a single application on top of the host operating system. The con-
tainer runtime isolates the resources (memory, file system, network, etc) of the container 
from the rest of the system.  
Figure 8 depicts how a single host can run one or multiple containers simultaneously 
where the containers share the same host operating system (OS) kernel (Hong & Bayley 
2018: 154). Shared kernel increases the processing speed of the container instructions 
since they are in the same address space. In the other hand, kernel sharing decreases the 
level of security by amplifying the crucial kernel vulnerabilities and increases the latency. 
 
                                                 
2 Challenges could be overcome also by utilizing unikernels and serverless architectures which are not 
reviewed in this thesis. See for further information about unikernels: 
https://ieeexplore.ieee.org/document/7396164/ 
and information about serverless architectures: 
https://ieeexplore.ieee.org/document/8360324/ 
 25 
 
 
s 
Figure 8. Docker container virtualization. (Reconstructed from Juniper Networks 
  2018) 
In this thesis, the development and deployment of the containerized applications are im-
plemented using Docker container platform. Docker is a platform that supports develop-
ment, deployment, packaging and execution of software applications in containers where 
application components are packed together with their execution runtime environments. 
Docker allows easy container portability for different hosts and Docker containers can be 
run on any device that is capable of supporting Docker. (Lukša 2018: 11–12) 
Figure 9 depicts three essential concepts that illustrate how Docker platform supports 
development, deployment and running of containerized software applications. Essential 
concepts are: images, registries and containers. Lukša (2018: 13) 
 
 26 
 
Figure 9. Development lifecycle of a Docker container. (Reconstructed from 
  Lukša 2018: 13.) 
Docker provides containerization tools which enable building of Docker images. Image 
is a layered entity that consists of components and environments for the software appli-
cations. Docker builds an image automatically by following the instructions that are de-
scribed in the Dockerfile. Dockerfile composes command line commands that are needed 
for building an image in a text document (Docker Guides documentation 2018). 
Developer can upload (push) and store successfully built Docker images into Docker reg-
istries which are responsible for storing images. Docker registry allows easy and shared 
access for machines where multiple hosts can download (pull) a desired image. The de-
veloper can also set the registries to be public or allow permissions for private machines 
depending on the confidentiality of the images. Lukša (2018: 13) 
Docker container platform is the most popular of the container technologies, so it also 
enables the container creation from a Docker image. Docker containers are isolated pro-
cesses that are running isolated from the host and other processes. A developer can restrict 
the resources of the Docker container in such a way that the container resource usage 
cannot exceed a certain level (Lukša 2018: 13). 
2.4 Orchestration 
Cloud-based systems consist of application components which are running on both virtu-
alized and physical hardware that can be distributed in multiple locations (Sosinsky 2011: 
46). The development, deployment and management of the application components in 
 27 
cloud environment have been problematic and it rose the need for management standards 
and cloud orchestration (Kena et al. 2017: 18862). Orchestration of cloud-based applica-
tions and components strives to automate their deployment and management. 
2.4.1 Kubernetes 
Efficient deployment, configuration and management of increasing amount of deployed 
applications in cloud environment requires usage of orchestration. Kubernetes is an open-
source system for automated deployment, scaling and management of containerized ap-
plications that are running in a cloud environment. Kubernetes is developed and intro-
duced by Google in 2014. (Lukša 2018: 2, 16, 19) 
In this thesis, we employ Kubernetes solely in the context of Docker container orchestra-
tion. Lukša (2018: 16) has pointed out that Kubernetes covers much more than Docker 
container orchestration but, on the other hand, containers are a convenient way of running 
applications in distributed cluster nodes. Container cluster in Kubernetes is a term which 
refers to composition of cluster master(s) and worker nodes. The structure of a container 
cluster is illustrated in the Figure 10. 
Figure 10. Kubernetes cluster consists of cluster master and worker nodes.  
  (Reconstructed from Lukša 2018: 18.) 
Cluster master (Control Plane) takes care of the functionality and control of the cluster. 
Cluster master consists of four types of components: Kubernetes application program-
ming interface (API) Server, Controller Manager, Scheduler and etcd which are respon-
sible for maintaining and controlling the state of the cluster (Figure 10). Applications 
however are not run by Cluster master components and that is where the role of worker 
nodes takes place. (Lukša 2018: 18–19) 
 28 
The worker nodes take care of executing and running the applications in containers. A 
single node consists of three types of components: Container runtime, Kubelet and Ku-
bernetes Service Proxy which are responsible for running, monitoring and serving the 
executed application (Figure 10). (Lukša 2018: 19) 
2.4.2 Container Orchestration 
Since, in this thesis, Kubernetes is used to orchestrate Docker-based containers, it is man-
datory to wrap runnable applications into Docker images. Figure 11 shows that, in addi-
tion to initialization of container images, the images need to be pushed (uploaded) into an 
image registry where worker nodes can access and pull (download) the images that they 
require. Cluster master manages worker nodes by following the configurations of appli-
cation description that consists of deployment-related instructions. (Lukša 2018: 19). 
 
Figure 11. Kubernetes tells worker nodes to pull container images according to 
  application description. (Reconstructed from Lukša 2018: 20.) 
Application description provides instructions to the Kubernetes API server which is re-
sponsible for communication between nodes, user and other components of the Control 
Plane. The description provides information about the required container image or images 
that constitute the components of application and possible relationships to other nodes. In 
the description, it is possible to determine more specific instructions such as how many 
replicas of an instance should be running and whether the provided services are meant to 
be used by internal or external clients. (Lukša 2018: 18–20) 
 29 
Kubernetes is an orchestration system that ensures the running of the applications as de-
clared in the description. Kubernetes automatically takes care of the deployment and 
maintenance of the application and it can be seen e.g. if the application experiences an 
unexpected error, Kubernetes restarts it on the same or another worker node. (Lukša 2018: 
20–21) 
2.5 Data Preparation 
Data preparation is essential requirement for machine learning because noisy, corrupted 
or incomplete data can lead to unwanted learning results. Data preparation in this thesis, 
mainly consists of noise removal, redundant information removal and imputation opera-
tions. In prepared data, the undesired anomalies are filtered from the data, the redundancy 
is reduced, and missing data points are derived. 
2.5.1 Noise Removal 
Appropriate noise removal should omit the noisy values from the data set in such a way 
that the anomalies are filtered out while preserving the original data pattern. Otherwise, 
data related operations would suffer from falsely derived bias if noisy values are present. 
(Tan & Jiang 2013: 3–4). 
Multiple noise removal techniques exist such as finite impulse response (FIR), infinite 
impulse response (IIR) and rolling median filters which have certain advantages in dif-
ferent scenarios (Tan & Jiang 2013: 4). The design of the noise removal procedure should 
consider the characteristics of the collected data e.g. optimal filter for continuous data 
would not necessarily be the best choice for quantized data. 
 
 
 30 
2.5.2 Redundancy Removal 
Data redundancy refers to the data values that do not provide any additional information 
(Lucky 1968: 551). Redundant data increases the amount of computation that the machine 
learning operations have to perform without having any benefit in the learning.  
Different methods exist for eliminating redundancy such as normalization, duplication 
removal and recursive optimization (RO) algorithms (Zhang et al. 2013: 106). These 
methods aim to reduce the size of the data without having negative impact on the results 
of machine learning. 
2.5.3 Imputation and Excluding Methods 
Collected data set may include missing data values due to failures in measurements. Miss-
ing data causes gaps between known data points and missing information might have 
harmful influence in machine learning. In this thesis, imputation and excluding methods 
handle the missing data values. Imputation replenishes the missing data whereas exclud-
ing methods disregard the missing data. (Alpaydin 2010: 89; Allison 2001: 5) 
Different imputation techniques exist such as interpolation and regression which provide 
approximations of the missing data values (Alpaydin 2010: 90). Respectively, different 
excluding methods exist such as pairwise deletion and listwise deletion where the missing 
data is removed in a certain way (Allison 2001:5).  
2.6 Summary 
Machine learning refers to a concept where machines can learn to improve their perfor-
mance based on the experience. Machine learning extracts hidden information from the 
data which can be used in optimization. Quality and quantity of the utilized data influ-
ences to the results of machine learning. High amount of prepared data leads to more 
satisfying learning results than less amount of unprepared data. 
 31 
Cloud computing supports network accessible usage of scalable resources. Applications, 
in cloud environment, are running in unspecified physical systems and clouds pool re-
sources by virtualizing. An environment that composes multiple connected clouds is 
called as distributed cloud environment. 
Microservices are small and self-acting software components which can be deployed ef-
ficiently in a cloud-based environment. Microservices handle their own processes, and 
they share information between each other by utilizing inter-process communication. 
Container technologies can support containerization of microservices where the software 
components and their execution runtime environments are bundled into containers. In 
container development, Kubernetes facilitates deployment and management for increased 
number of containers. Kubernetes, as an orchestration system, automates deployment, 
scaling and management of containers. 
 32 
3 PLATFORM REQUIREMENTS 
In this chapter, we specify requirements for the machine learning based optimization 
which supports decentralized functionality. The requirements are based on a real use case 
in a distributed cloud environment that discloses opportunities and challenges for ma-
chine learning. Coupled with machine learning requirements, we introduce the require-
ments for data preparation module since the usage of raw data would be unfavorable in 
the learning operations. We describe the requirements on high level, and they are used to 
guide the technical design of the machine learning software. 
3.1 Use case: Autonomous Ships 
Our use case is an example of a real-world application where we employ and integrate 
decentralized machine learning in a distributed cloud environment. In this thesis, the use 
case involves deployment of autonomous ships that utilize edge computing in independ-
ent control and in decision-making procedure. Autonomous ships are miniature proto-
types that demonstrate the full functionality of technical implementations which could be 
deployed on devices in production. 
Autonomous ships act independently in such a way that human interactions are not needed 
in sailing. Ships cruise from one harbor to other by calculating an optimal route and avoid-
ing possible obstacles in the water such as other ships or underwater rocks. Ships control 
the movement and the power usage autonomously by following instructions of control 
algorithms that utilize machine learning for optimization.  
Ships perform autonomous operations in a cloud-based environment. The ships take ad-
vantage of on board edge computing where device related and computationally light 
weight operations are executed with low latency. We conduct more demanding operations 
in other clouds that have higher computational capacity. Deployment of autonomous 
ships introduces use case specific requirements for the design and implementation of the 
machine learning application.  
 33 
3.1.1 Connectivity and Communication 
The edge cloud of an autonomous ship has a constrained connectivity if the edge cloud is 
not connected to other clouds. The edge cloud, located within a feasible range of a harbor 
area, establishes a virtual private network connection (VPN) into the regional cloud of 
the harbor. Consistently, the edge cloud disconnects from a regional cloud when the ship 
sails outside of the harbor area. 
The term VPN refers to a software that remotely connects a computer to private network 
across a public network. Thus, a VPN gives the illusion to the user of the computer as if 
it were directly connected to the private network. VPN consists of virtual connections 
which are provisional connections that have no physical instances. Connectivity in VPN 
is based on the packets that are routed over multiple machines on the public network. 
(Scott et al.1999: 2) 
Autonomous ships should minimize data transmission between the local edge cloud and 
remote cloud(s) when the ship is sailing. In sailing, the ship relies solely on narrow band 
satellite communications. The ships should, instead, transfer the collected data in the har-
bors where the edge cloud is able to connect to the distributed cloud environment. Com-
putationally heavy operations, such as machine learning, should be performed in a central 
cloud because the edge cloud needs to guarantee availability of the resources for the use 
of more essential procedures. However, the edge cloud may perform machine learning 
locally if the learning task is computationally light or if there are sufficient amount of 
available resources. 
3.1.2 Sensor Fusion 
Autonomous ships monitor their performance constantly with sensors and the ships col-
lect monitored data for further processing. Reliability of machine learning and analysis 
of the performance of the ship is highly dependent on the amount of collected data. Per-
formance of machine learning and analytic procedures improves generally when the 
 34 
volume of collected data increases because a low amount of data may not introduce 
enough instances of possible events. 
A control unit in the ship manages the autonomous sailing of the ship and additionally 
the control unit is responsible for data collection. Control unit collects data from the sen-
sors of the ship. Sensors measure physical quantities of the ship such as velocity, accel-
eration and power consumption. In our use case, a ship includes multiple quantities that 
are measured and the measurements are performed three times per second. 
The control unit stores measured data in a database which is located in the edge cloud. 
The control unit replicates the content of the database to a central cloud when the ship 
reaches a harbor area. Database replication, to the central cloud,  clears space in the edge 
cloud. 
3.1.3 Optimization of Engine Performance 
We utilize the collected data to optimize the power usage of the ship’s engine. The opti-
mization aims to improve the performance of the engine by finding a model or an algo-
rithm that describes the behavior of the data as precisely as possible. Efficient usage of 
the engine power minimizes ship’s energy consumption and extends the potential sailing 
time. 
The control unit of a ship has a state machine with different states which introduce state-
specific objectives for optimization. In our case, we have two basic states, traveling and 
docking states, for which the ship alternates the objective of its performance. A very ge-
neric optimization is challenging to be define with a single algorithm even in our simple 
use case of two states, let alone in a more complex scenario involving a real ship. For 
these reasons, we take advantage of machine learning to optimize the overall performance 
of an autonomous ship. 
 
 35 
3.2 Machine Learning Agent 
In this thesis, a machine learning agent is a software application that is responsible for 
performing machine learning related tasks and interacting with other agents that are de-
ployed in different environments. The agent composes of machine learning, evaluation, 
deduction and decentralization operations which should be designed in a generic way that 
supports possible deployment in multiple use cases. Thus, the agent should follow as a 
service principle where the application is available to be used and configurable by an end-
user, but software updates and maintenance are managed by a developer. The primary 
objective of the agent is to conduct accurate inferences that rely on the findings from the 
collected data. 
A machine learning agent can divide and distribute its workload to other agents in order 
to maximize performance. An agent needs to support interoperability in different envi-
ronments since it should be able to operate in a decentralized way in multiple clouds. 
3.2.1 Interoperability Requirements 
In this thesis, the computational environment consists of multiple connected clouds, i.e., 
distributed cloud environment, where the computational resources can vary between the 
cloud types. Figure 12 illustrates how machine learning agents should adapt and adjust 
their performance in different cloud-based environments. 
Figure 12. Machine learning agents operate in different cloud-based environments 
 36 
Functionality and behaviour of the agent relies on the deployment environment since the 
agent should not reserve computational resources from other, more essential operations. 
For instance, when free available resources are very limited in a certain environment, the 
agent should not run computationally heavy machine learning operations simultaneously 
since it would critically reduce the performance of the system. 
3.2.2 Orchestration Requirements 
We require machine learning agents to be interoperable and act independently in the en-
vironment where they are deployed. Deployment and management of multiple independ-
ent agents become more laborious as the number of deployed agents increases which is a 
similar challenge when deploying multiple microservices. Proper management and de-
ployment of agents requires orchestration. 
Orchestration should be centralized to promote convenient deployment and life-cycle 
management of multiple agents. In other words, an orchestration system should manage 
the software configuration, operational optimization, provisioning, start up and termina-
tion of agents. Figure 13 depicts how the centralized orchestration system should manage 
multiple distributed cloud-based applications. 
 
Figure 13. Orchestration facilitates deployment and maintenance of multiple  
  machine learning agents 
 37 
The orchestration system deploys and manages the operational state of the agents in an 
automated way. If an error occurs in a running agent, orchestration system recovers the 
situation, for instance, by re-launching the agent on another host.  
3.2.3 Reusability Requirements 
Machine learning agents need to support reusability in such a way that the changes or 
adjustments in operations are not required to be reconstructed in the source code. For 
example, if the context of machine learning changes over time, the agent automatically 
starts to perform additional learning and adjusts the learning-related parameters to fit the 
new circumstance. A developer should be able to inform the changes of desired function-
ality to the agent by using external configuration files. 
External configuration files act as a customize template to the functionality of the soft-
ware. A developer can serve the configuration files to the orchestration system which is 
responsible of ensuring that the states and instances of the software matches with the 
requirements that are defined in the configuration files. 
3.2.4 Performance Requirements 
A machine learning agent has to be able to extract hidden information from data in such 
a way that decisions are based on the extracted information and they improve the overall 
performance of the system. Hidden information consists of knowledge of the data which 
is not described in the initial data set. In addition, the hidden information can be 
knowledge that has not been described by predefined performance evaluation algorithms. 
Occasionally developers may find performance evaluation algorithms difficult to define 
especially if the pattern of the data is complicated. The agent needs to conduct learning 
from the collected data even if the agent has no knowledge of predetermined algorithms 
or models. With this intention, the agent evaluates the fitness of the learning results in 
such a way that the evaluated results facilitate the decision-making processes and thus 
improve the overall performance. 
 38 
As explained earlier, autonomous ships may have multiple states where the objective of 
the ship varies. The agent has to utilize learning for state-specific optimization which 
increases the coverage of learning. Further, state-specific learning enhances the effective-
ness of state-specific optimization which correspondingly improves the overall perfor-
mance of the autonomous ship. 
The agent should support decentralized functionality where resource demanding machine 
learning operations can be distributed among multiple agents. Decentralization enables 
parallel processing which further accelerates learning and also enables learning in a con-
strained environment. 
3.3 Data Preparation Module Requirements 
In efficient machine learning, raw (unprocessed) data needs to be prepared before ma-
chine learning agents can utilize the data as an input because raw data may include noise, 
redundancy and missing values which may cause counterproductive effects. Proper data 
preparation is a preliminary requirement for machine learning related performance im-
provements. 
Data preparation module needs to be interoperable in different cloud-based environments, 
so that the preparation can be executed in the any cloud that has available computational 
resources. For instance, if the data preparation is handled in the edge cloud, it would save 
data storage from other clouds and also save bandwidth by reducing the data volumes. 
3.4 Summary 
In this chapter, we described the requirements for the machine learning agent and the data 
preparation module that are necessary for performing decentralized machine learning in 
a distributed cloud environment. We presented an use-case scenario which introduced use 
case specific requirements for the machine learning agent. 
 39 
The machine learning agent should be able to adapt to different cloud-based environ-
ments, to optimize the engine performance of an autonomous ship, to adjust its operations 
to manage multiple state-specific optimizations and to support decentralization. Data 
preparation module, on the other hand, improves the efficiency of machine learning agent 
by preparing the input data. The preparation consists of redundancy reduction, noise re-
moval and handling of missing data. 
 40 
4 DESIGN PROCEDURES 
In this chapter, we describe the design of the architecture and the functionality of the 
machine learning agent and the data preparation module. We review the mathematical 
methods behind the designed functionality in detail since we later implement the compo-
nents without utilizing existing machine learning frameworks. 
4.1 Architectural Design of the Machine Learning Agent 
Architectural design describes the high-level structure of the machine learning agent. The 
structure consists of software components and their relationships. Architectural design 
clarifies the functional and the deployment design which are explained in their separate 
sections in this thesis. 
4.1.1 Functional Design 
The functionality of the machine learning agent supports supervised machine learning. 
The utilized data is labelled where the inputs are mapped to the corresponding outputs. 
Labelled data is ideal for supervised learning since it enables the supervisor to construct 
a training set of examples. 
Figure 14 depicts the functional architecture of the agent where the layers have are either 
visible or hidden. Visible layers consist of values that are exposed to the execution envi-
ronment and hidden layer handles the computational processing of the agent. 
 41 
 
Figure 14. Overview of functional architecture of a machine learning agent 
Input layer of the agent composes the real inputs of the system where one or multiple 
inputs are determined to be optimized. The agent receives the inputs from the data prep-
aration module which provides non-redundant, noise-free and complete data. 
Hidden layer of the agent utilizes regression based machine learning. Hidden layer in-
cludes the learning procedure of the regression model, evaluation of the learning results 
and logic for conducting deductions. 
In model training, the agent relies on a supervisor to train the regression model of the 
system according to the given input values. After training, the agent evaluates the best 
fitting model and derives the parameters and the degree of the model. The best fitting 
model, in turn, improves the agent’s ability to perform accurate deductions. 
4.1.2 Deployment Design 
We design the machine learning agents as relatively small and independent software com-
ponents that utilize the Hypertext Transfer Protocol (HTTP) in communication. We con-
sider microservice-oriented architecture in the design of the agent. 
 42 
We deploy the agents in a cloud-based environment where the amount of available re-
sources varies between the clouds and, thus, the agent needs to adapt to the situation. 
Microservice-based design of the agents supports easier integration, better runtime per-
formance and more reliable fault tolerance than monolithic design. 
Figure 15 depicts the deployment design of the agents with distributed cloud environ-
ment. In this thesis, the distributed cloud environment consists of three connected clouds: 
edge cloud, regional cloud and central cloud. 
 
Figure 15. Overview of the deployment design of machine learning agents  
In deployment design, we enhance the efficiency in portability of software by packing the 
software components and the required resources of the agent into Docker images.  Docker 
platform supports the deployment of software applications as containers, and the contain-
ers can run on any machine that is running Docker software. Docker, in other words, 
enables convenient software portability for multiple working nodes. Moreover, we em-
ploy Kubernetes to orchestrate deployment, maintenance and running of containerized 
applications. 
Kubernetes manages the containerized agents according to the deployment configura-
tions. The configurations include information, e.g, about the image to be deployed, 
 43 
number of desired replicas and the visibility of the services. Kubernetes guarantees that 
the containerized agents are running as they have been configured to run while recovering 
malfunctions by restarting terminated agent containers. Kubernetes also supports the net-
work communication between the agents and other deployed applications. 
4.2 Machine Learning Design 
We design machine learning operations to fulfil our requirements. The distributed cloud 
environment and the use case introduce challenges and opportunities for machine learn-
ing. Requirements for machine learning emphasizes interoperability, reusability and im-
proved overall performance. 
4.2.1 Supervised Learning Design 
A machine learning agent receives labeled data, as input, which is ideal to be used by 
supervised learning techniques. Supervised learning utilizes a supervisor which con-
structs a training set of examples. Training set is composed of collected data that consists 
of numeric values where the states of the machine are included. Since the collected data 
represents numeric values, we design the agent to takes advantage of polynomial regres-
sion (Figure 16). 
 44 
 
Figure 16. Example of regressions with four different degrees 
The agent strives to learn the best fitting regression model that maps the inputs of the 
system to the outputs as accurately as possible. Regression model provides information 
about the pattern in relation between the values. The degree of the best fitting regression 
model depends on the relation pattern that varies between different inputs and outputs. 
The desirable result of learning is to find out the degree and the parameters of the best 
fitting regression model. 
4.2.2 Conditional Learning Design 
In this thesis, conditional learning refers to a concept where the machine learning opera-
tions depend on both their execution environments and states of the machine. With the 
environment, we mean here that the machine learning agent should adapt and reduce the 
complexity of the operations if the agent detects that the execution environment is re-
stricted (Figure 17). 
 45 
 
Figure 17. Flowchart of conditional learning where an agent can reduce the  
  complexity of the machine learning according to the available resources 
A device, that is optimized by machine learning, may have multiple states so we design 
conditional learning to support state-specific features. States may have different priorities 
where conditional learning aims to optimize the crucial features of a certain state. Condi-
tional learning benefits the overall system performance as well because optimization of 
certain states may require reduced amount of computational resources, and therefore 
learning is possible to be conducted in a more constrained environment. 
4.2.3 Decentralized Learning Design 
Machine learning agent may detect that the available amount of resources are not suffi-
cient enough to conduct machine learning even if the agent has reduced the complexity 
of learning. Consequently, we design the agents to be able to request assistance for learn-
ing from other agents that are on a feasible proximity. For instance, a machine learning 
agent in constrained edge cloud could request assistance on-demand from agents in re-
gional and central clouds. Figure 18 depicts the design of the decentralized machine learn-
ing. 
 46 
Figure 18. Flowchart of decentralized learning where an agent can request or 
  provide assistance from other agents 
The requesting machine learning agent sends a request for other agents where the request-
ing agent describes a machine learning objective that is desired to be processed decen-
tralized. Respectively, the agents that can provide assistance inform the requesting agent 
that they can begin parallel and decentralized learning. After all the parts of learning are 
finished, the requesting machine learning agent aggregates the results of learning. 
4.2.4 Fitness Evaluation Design 
A machine learning operation may end up producing multiple feasible regression models 
so a machine learning agent needs to evaluate the feasible solutions and select the best 
fitting model. As our solution, we employ least squares estimation, where we compare 
the values of the regression model to the corresponding real values. The agent chooses 
the model that results the minimum sum of the deviations, i.e, the best fitting model and 
utilizes that model in deduction as explained in the next section. 
 
 
 47 
4.2.5 Deduction Design 
Deduction logic of the machine learning agent evaluates the best fitting model, which the 
agent learns from the training data, and pursues to find the most optimal solution that 
fulfills the initial learning objectives. Learning objectives refer to the prioritized objec-
tives of the state-specific optimization. 
As an use case related example, let’s assume that the agent should find the optimal “load” 
of an autonomous ship in the sailing state. Here, optimal load means the optimal relation 
between the velocity and the energy consumption in such a way that the ship travels the 
furthest distance by consuming the minimum amount of energy as possible. The outcome 
of the deduction procedure would be parameters or information that improves the perfor-
mance of the control algorithm of the ship. 
In this thesis, we improve the deduction logic by conducting root analysis of a derived 
function where the function represents the best fitting regression model. Root analysis of 
the derived function provides information about maxima and minima values of the origi-
nal function, and depending on whether the original function is monotonic or not. 
4.3 Data Preparation Module Design 
The data preparation module performs redundancy removal, missing data handling and 
noise removal of the collected data. The data preparation module guarantees that the uti-
lization of the prepared data leads to significantly better results in machine learning. 
We design data preparation module to support containerization where the designed com-
ponents can be bundled into a container. Containerization facilitates running of the data 
preparation module in any cloud-based environment which improves the reusability of 
the module. In addition to reusability, containerization enables efficient deployment and 
maintenance of the data preparation module by utilizing Kubernetes. 
 48 
4.3.1 Architecture 
The data preparation module receives raw collected data as an input and produces cleaned 
(prepared) output data that enables more precise monitoring, analytics and machine learn-
ing. Figure 19 depicts the architecture of the data preparation module. 
 
 
Figure 19. Overview of the architecture of the data preparation module 
The functional parts are redundancy removal, missing data handling and noise removal 
units that are responsible for improving the quality of the output data. We regard the 
output of the data preparation module as prepared or clean data. 
In order to reduce unnecessary computational processing in the data preparation module, 
the operations occur in the following sequence: 1) redundancy removal 2) missing data 
handling 3) noise removal.  
 
 49 
4.3.2 Redundancy Removal Design 
Collected data may include redundancy such as duplicates that should be discarded to 
avoid superfluous processing. Duplicates consist of multiple data samples that have iden-
tical content and the redundancy is handled by deleting all duplicates until only one exists. 
The elimination of redundant samples reduces the computation time in data-intensive op-
erations such as machine learning, monitoring and analytics. 
4.3.3 Missing Data Handling Design 
Missing values in the data may produce false bias which affects to the accuracy and ef-
fectiveness of the data related operations. We design the data preparation module to han-
dle missing values by utilizing listwise deletion. 
Listwise deletion is an exclusion method which removes the data sample if it includes 
any missing values. There are two main advantages in listwise deletion: it can be used 
with any kind of data regardless of the data pattern and it does not introduce additional 
computational overhead. Respectively, listwise deletion has a remarkable disadvantage 
by removing significant fraction of the data if the occurrence of missing values is fre-
quent. (Allison 2001: 6) 
In the context of this thesis, the missing values in the collected data occur infrequently 
which means that the main disadvantage of listwise deletion is irrelevant. Avoiding extra 
processing is important in our scenario since it involves resource constrained environ-
ments. It is also worth noting that Allison (2001: 5) has compared different missing data 
handling methods and stated that many of the other methods are inferior to listwise dele-
tion. 
4.3.4 Noise Removal Design 
We design the noise removal to remove anomalies from the collected data by utilizing 
moving average filter which calculates the average value of the data points within a pre-
determined interval. The interval for the filter is a configurable parameter and it represents 
 50 
the order of the filter. A moving average filter smoothens the intervals of the data by 
filtering local unwanted anomalies. (Gonzalez & Catania 2018: 1542) 
The collected data includes noise whose occurrence is stochastic. Our initial assumption 
is that the collected data is mostly unnoisy, so the moving average is a feasible solution 
since it is able to filter the local noise. 
We define the order of the moving average filter in such a way that it removes the noise 
but also preserves the original data pattern. In the implementation phase, the proper order 
of the moving average needs to be tested and validated since the formal method for order 
predetermination is not defined in the theory in digital signal processing (Gonzalez & 
Catania 2018: 1542). 
4.4 Selection of Mathematical Methods 
In this section, we select and introduce mathematical methods to be utilized in machine 
learning. Selected mathematical methods realize the funcitonality, evaluation and deduc-
tion of the learning occurring in the agent. The machine learning agent does not utilize 
any existing  machine learning frameworks, so mathematical methods need to be intro-
duced and scrutinized properly. 
4.4.1 Regression 
The term regression refers to a mathematical procedure that estimates the relations be-
tween variables of interest by constructing a model. Construction of the model is based 
on the development of mathematical expressions that represent the pattern in the relation 
between dependent and independent variables. The constructed model includes parame-
ters that are initially unknown, constant coefficients that determine the behavior of the 
model. In addition to the parameters, a model both has variety in its degree and its math-
ematical complexity which both are dependent on the modeled mathematical operation. 
(Rawlings et al. 1988:1-2) 
 51 
Regression, in machine learning, has no predefined information about the degree and the 
parameters of the model in the context of this thesis. The main objective of machine 
learning agent is to learn the parameters and the degree of the best fitting model. We 
define the supported formulas of the regression models to be linear, quadratic and poly-
nomial in their parameters for the use of the machine learning. 
The simplest regression model is a linear model which represents change of a dependent 
variable at constant rate as the independent value increases or decreases. Rawlings et al. 
(1988: 2) have introduced following formula for the linear model: 
𝑌𝑖 = 𝐵0 + 𝐵1𝑋𝑖 +  ε𝑖 , 
where 𝑌 represents the dependent variable, 𝑋 represents the independent variable, coef-
ficients 𝐵0 and 𝐵1 are the parameters of the model, ε represents the added random error 
and subscript 𝑖 represents the observation unit where 𝑖 = 1, 2, 3 ... n. In this formula, 𝐵0 is 
the intercept and 𝐵1 is the slope of the line 
Linear regression model is accurate if the relation between dependent and independent 
variables has a constant rate of change. If the rate of change is not constant, the linear 
model would be inaccurate, and it is reasonable to fit a nonlinear model instead. 
Quadratic and polynomial regression models are nonlinear which means that the degree 
of the model is second-order or higher. Rawlings et al. (1988: 236) have introduced the 
following formula for quadratic model which is the simplest extension of the straight-line 
model: 
𝑌𝑖 = 𝐵0 + 𝐵1𝑋𝑖 +  𝐵2𝑋𝑖2
 +  ε𝑖 , 
where second-order term, 𝑋2 is involved in addition to 𝑋. 
The formula for regression model of higher-order polynomials has been defined by Rawl-
ings et al. (1988: 236) in following form: 
 52 
𝑌𝑖 = 𝐵0 + 𝐵1𝑋𝑖 +  𝐵2𝑋𝑖2
 + 𝐵3𝑋𝑖3 + ⋯ +  𝐵𝑝𝑋𝑖
𝑝 + ε𝑖 , 
where the number of terms is directly proportional to the 𝑝th degree of the polynomial 
model. 
Rawling et al.(1988: 2)  have stated that a model usually falls into the class of models 
that are linear in the parameters in preliminary studies of the operation but the more real-
istic models are often nonlinear in the parameters. In this thesis, the optimal performance 
of the machine learning operations relies on regression models which are as realistic as 
possible. Linear models have reduced complexity compared to polynomial models, yet 
the machine should utilize the model with the most accurate fit. 
4.4.2 Least Squares Estimation 
Fitness of the regression model needs to be evaluated to find the best model. The least 
squares estimation is a method which we use to evaluate and select the model with the 
best fit. Potential models are evaluated by comparing sums of squared deviations between 
the real measurements and the estimations of the models. In least squares estimation, the 
best fit is the model that results the smallest sum of squared deviations. (Rawlings et al. 
1988:3) 
Machine learning operations validate and select the best fitting regression models by uti-
lizing the principles of the least squares estimation. Rawlings et al. (1988:3) have pre-
sented the following formula for least squares estimation by calculating the sum of the 
squares of the residuals, 𝑆𝑆(𝑅𝑒𝑠): 
𝑆𝑆(𝑅𝑒𝑠) =  ∑ (𝑌𝑟𝑖 − 𝑌𝑒𝑖)
2 𝑛
𝑖=1  , 
where the 𝑌𝑟𝑖 is the real observation, the 𝑌𝑒𝑖 is the estimation of the model and 𝑌𝑟𝑖 −
𝑌𝑒𝑖 is the residual. 
 
 53 
4.4.3 Root Analysis of the Derived Function 
Root analysis of the derived function refers to the observation of a derived function at its 
roots. We utilize roots of the derived function to find the local minima or maxima of a 
function and to find out if the function is monotonic or not. Root or roots of the function 
are values that can be found when the function of x equals zero. (Press et al. 2007: 442) 
have defined root finding generically in the following way: 
𝑓(𝑥) = 0 , 
where all terms are traditionally moved to left- or right-hand side, leaving zero to the 
other side. 
Finding of the root is more challenging as the dimension of the equation grows. In one 
dimensional root finding the root is known to be found but in higher dimensions the ex-
istence of the root or roots are unknown until they are found. Possible number of roots 
that satisfies the equation simultaneously varies on the amount of independent variables 
which means that equation may have more than one solution. In addition to having mul-
tiple satisfying solutions, the equations may have no real roots at all. (Press et al. 2007: 
443) 
Root finding can be implemented by using different numerical methods such as Newton’s 
method, bisection method or secant method. These methods can be divided into enclosure 
and fixed-point methods. Enclosure methods pursue to find the root by shrinking an in-
terval that consists of at least one root, and fixed-point methods strive to approximate the 
root by taking advantage of the information about of the function. The efficiency of dif-
ferent root finding methods is evaluated by observing the required amount of computa-
tional iterations that produces a convergence with a satisfying accuracy. 
Derivative describes the immediate rate of change of a function. The functional principle 
of the derivative is based on the comparison between the values of the function 𝑓 with 
the values of 𝑥 and 𝑥 + ℎ where ℎ is a small quantity. The comparison is used to define 
 54 
the change, ∆ 𝑓 by subtracting the value of 𝑓 (𝑥) from the value 𝑓 (𝑥 + ℎ). The rate of 
change, derivative at point 𝑥, is calculated by dividing the change by the interval ∆ 𝑥 
which is the change from 𝑥 to 𝑥 + ℎ  (Anthony 2011: 41–42) : 
∆ 𝑓(𝑥)
∆ 𝑥
=
𝑓(𝑥 + ℎ) − 𝑓(𝑥)
(𝑥 + ℎ) − 𝑥
=  
𝑓(𝑥 + ℎ) − 𝑓(𝑥)
ℎ
 
Strang (1991: 44) has introduced the formal definition of derivative as follows: 
𝑓′(𝑥) = lim
ℎ→0
 
𝑓(𝑥 + ℎ) − 𝑓(𝑥)
ℎ
 
where the 𝑓′ is the derivative of function 𝑓, 𝑥 is a variable and lim is the limit. Derivative 
of the function 𝑓 can be defined only if the limit exists. Otherwise the derivative does not 
exist. (Strang 1991: 45; Anthony 2011: 42) 
4.5 Summary 
We have described the architectural and functional design of the machine learning agent 
and the data preparation module. We have designed the system to support interoperability, 
reusability and ability to enhance the system performance. Both the machine learning 
agent and data preparation module utilize cloud-based environments, where the amount 
of computational resources varies, so the software has been designed to adjust their oper-
ations according to their deployment environment. 
Moreover, we designed the machine learning agent to be able to optimize the overall 
performance of the system by taking advantage of supervised, conditional and decentral-
ized learning. Conditional learning enables the agent to reduce the complexity of learning 
task and to conduct state-specific learning, whereas decentralized learning enables paral-
lel machine learning to be employed among multiple agents operating in different clouds. 
 55 
Finally, we introduced the mathematical methods that we utilize in the implementation 
phase. The mathematical methods act as the “backbone” of machine learning in this the-
sis. 
 56 
5 IMPLEMENTATION PROCESS 
In this chapter, we implement the designed data preparation module and the machine 
learning agent in order to fulfill the functional and deployment requirements. We imple-
ment the data preparation module first since the machine learning agent is dependent on 
prepared data for reliable results. We develop it using Python programming language and 
two libraries called NumPy and pandas. The mentioned libraries support scientific com-
putation, data analysis and the usage of different data structures (Numpy and Scipy Doc-
umentation 2018; pandas: powerful Python data analysis toolkit 2018). However, we do 
not use any existing machine learning frameworks in this implementation but rather de-
velop machine learning and data preparation software based on the selected mathematical 
methods. 
5.1 Implementation of the Data Preparation Module 
The data preparation module is an independent software application that receives col-
lected raw data as input and produces prepared data as an output. We implement the data 
preparation module to be configurable in such a way that the attributes to be prepared can 
be selected. Moreover, the data preparation module supports state-specific data in order 
to enable state specific machine learning processing. It is worth noting that reconfigura-
bility can spare computation resources because processing of unnecessary attributes  can 
be omitted. 
Data preparation module takes advantage of so called data frames in processing. A data 
frame is a data structure where the data is stored as list of vectors of equal length. Then, 
prepared data is stored into a database where the data is accessible for other operations 
such as machine learning and analytics. 
 
 
 57 
5.1.1 Duplicate Removal 
We implement redundancy reduction by removing duplicates from the given data set. 
Duplicates do not provide any additional information for data reliant tasks such as in 
machine learning but instead they just increase the computational load.  In this thesis, we 
regard duplicate removal to be an effective operation to be implemented since our data 
includes unique time stamps of the measurements. If we would not include unique key 
values such as time stamps, the possibility of having multiple duplicate measurements, 
which may or may not be redundant, exists. 
The data preparation module removes duplicates from the data frames by detecting and 
deleting the redundant rows. However, duplicate removal preserves one instance to rep-
resent the identical set of measurements. After the duplicates are removed, the data frame 
is re-indexed so that the duplicate removal does not violate the structure of the data frame. 
Re-indexing refers to a procedure where the indexes of a constructed list are ordered. 
5.1.2 Listwise Deletion 
We implement missing data handling, in the data preparation module, by utilizing listwise 
deletion. Listwise deletion is an exclusion method where the observation is disregarded 
if it includes missing data. The volume of missing value occurrences is relatively low in 
the collected data so the negative impact of listwise deletion is negligible. 
Listwise deletion utilizes data frames by detecting missing values and removes the rows 
that contain missing values. After listwise deletion, the data frame is re-indexed again so 
that the listwise deletion does not violate the structure of the data frame. 
5.1.3 Moving Average Filter 
We implement noise removal by developing a filter which calculates the moving average 
of the data. Moving average computes the average of data points within a predefined 
interval. In fact, we specify the interval for this filter to have a length of 12 data points. 
 58 
An interval of this length has relatively accurate efficiency in data pattern preservation 
and in the removal of local noise within the scope of our use case. 
Moving average filters the data values that have been defined in the configuration. Con-
figuration consists of the quantities to be filtered and the states in the case state-specific 
filtering is required. The moving average computes the result from non-redundant and 
complete data frames by utilizing rolling function from the pandas library (Figure 20). 
After the data is filtered, it is stored into a database where it can be accessed by machine 
learning operations. 
for state in states: 
  # State-specific filtering 
  state_df = complete_df.loc[complete_df['state'] == state] 
  # Filter attributes of interest 
    for interest in attributes: 
      df.append(state_df.loc[:,interest].rolling(12).mean()) 
 
Figure 20. Noise removal by utilizing rolling mean 
5.2 Implementation of Machine Learning 
We compose machine learning of regression, conditional and decentralized learning. 
First, regression learning is an ideal technique to be utilized since the collected data is 
labeled and the data consists of numerical values. Second, conditionality enables a ma-
chine learning agent to perform state-specific learning and reduce the complexity of on-
demand learning. Third, decentralization facilitates parallel machine learning and, thus, 
supports learning even in constrained environments. 
We describe the implementation of machine learning and deduction logic in two sections. 
In this section, we implement only describe related functionality whereas in the next sec-
tion we implement deduction logic that relies on the results of learning. 
 
 59 
5.2.1 Supervised Learning: Regression 
Collected data consists of location and measured sensor data from an autonomous ship. 
Sensors measure and gather values of physical quantities and enable the data to be labeled. 
Labeled data supports the deployment of supervised learning where a supervisor can es-
tablish a training set of examples. We implement machine learning by utilizing regression 
which is one of the supervised learning techniques. 
We implement regression by utilizing polynomial regression where the result of the learn-
ing is the degree and the parameters of the model. The learning operation relies on a set 
of training data which is provided by a supervisor. The supervisor constructs the training 
data set from the real measurements from the sensors and labels the measurements as 
inputs. The supervisor provides the training data as a data frame and informs the machine 
learning agent about the modeled variables of interest. 
We implement regression based learning with a function that trains regression models 
with different degrees of polynomials. The function receives a training set that composes 
variables of interest as parameters and fits the models with configurable amount of de-
grees. We achieve polynomial regression by utilizing polyfit function from the NumPy 
library (Figure 21). 
def TrainModels (x, y, degree): 
  # Loop through degrees 
  for z in range (1, degree): 
    # Fit models 
    model = np.polyfit(x, y, z) 
 
Figure 21. Simplified regression model training function 
The default configuration consists of degrees from one to seven which restrict higher 
degree models that are computationally more resource demanding. Generally, the accu-
racy of the regression increases as the degree increases but the computational complexity 
increases as well. 
 
 60 
5.2.2 Conditional Learning 
We implement machine learning in such a way that learning supports optimization of the 
state-specific objectives of an autonomous ship. In addition to state-specific optimization, 
machine learning agent is implemented as cloud-native service. Distributed clouds have 
variety in their capacity of resources which may restrict computationally heavy machine 
learning. For these reasons, machine learning is conditional. 
Conditional machine learning operates differently depending on the state of the autono-
mous ship. We implement conditional machine learning by determining machine learning 
objectives related to the state of the ship.  In our use case, conditional learning consist of 
two states: sailing and docking.  
Conditional machine learning adjusts the operational complexity of the learning algo-
rithm based on the deployment environment. The more constrained the environment is, 
the more simple the learning procedure will be. The agent receives conditional learning 
related information about the environment from Kubernetes when it deploys the agent. 
The deployment, in turn, can take place in a distributed cloud environment which consists 
of: edge, regional and central clouds. In order words, the agent may conditionally adjust 
the complexity of the regression algorithm  as depicted in Figure 22. 
# Reduce complexity of local learning 
if(environment == ""edge""): 
    TrainModels (x, y, 3) 
 
elif(environment == ""regional""): 
    TrainModels (x, y, 5) 
 
elif(environment == ""central""): 
    TrainModels (x, y, 8) 
 
Figure 22. Machine learning agent adjusts the complexity of local learning based on 
  the deployment environment 
In this thesis, the conditionality for machine learning supports as a service principles 
where an end-user can configure state-specific objectives for machine learning and set in 
 61 
the environment related boundaries. The end-user can also configure the numerical 
boundaries of machine learning in such a way that the weight of learning is restricted in 
the decision logic. 
5.2.3 Decentralized Learning 
We implement machine learning to support decentralization which enables parallel com-
putation. Parallelism accelerates the overall learning speed and it enables learning in an 
environment where the available amount of resources are constrained. Thus, we imple-
ment decentralization in such a way that machine learning can be distributed among mul-
tiple machine learning agents for different cloud environments. 
Decentralized learning enables machine learning agents to request or provide assistance 
for learning. A machine learning agent that does not have sufficient amount of available 
resources can request assistance from other agents that are on a feasible range. Requesting 
machine learning agent sends a request that informs other agents about the objective of 
learning via HTTP. After assisting machine learning agents have finished computation 
and sent their results, the requesting agent aggregates the results of decentralized learning. 
The following program snippet depicts the functionality of a requesting agent. In the Fig-
ure 23, the requested parameters for the learning algorithm are the optimum of the best 
fitting model in a certain interval and a list of the sums of squared deviations (errors). 
 
 
 
 
 
 
 62 
# IP addresses refer to the agents in feasible range 
request= 'http://192.168.2.131:8080/requestAssistance' 
results = 'http://192.168.2.130:8080/fetchErrors' 
polling = True 
# Request for decentralized learning 
r = requests.put(request, data={'request': True, 'x': x, 'y': y,
    'degree': requestDegrees}) 
# Poll for available agents 
while(polling): 
r = requests.get(results).content 
     # Fetch results of decentralized machine learning 
if(r['done'] == True): 
       decentralOpt = r['optimum'] 
       decentralErrors = r['errorList'] 
         polling = False 
                 
# Aggregate the results 
if(min(decentralErrors) < min(localErrors)): 
aggregatedOpt = decentralOpt 
else: 
aggregatedOpt = localOpt 
 
Figure 23. Machine learning agent can request assistance for decentralized learning 
A requesting machine learning agent can poll other assisting agents in a feasible range to 
distribute processing for them. An assisting agent receives information about the objec-
tive of learning from the requesting agent and then the assisting agent begins to perform 
learning based on the received information. After the learning task is finished, the assist-
ing agent sends the desired parameters to the requesting agent via HTTP. Figure 24 de-
picts the functionality of the assisting agent. 
 
 
 
 
 
 63 
# The IP addresses refer to the agents in feasible range 
assist = 'http://192.168.2.131:8080/requestAssistance' 
results = 'http://192.168.2.130:8080/results' 
polling = True 
# Poll for available agents 
while(polling): 
  r = requests.get(assist).content 
  # Train models that are requested 
  if(r['request'] == True): 
    parameters = TrainModel(r['x'],r['y'],r['degree']) 
    # Send the results of learning 
    r = requests.put(results, data={'decentralOpt':   
  parameters[0], 'decentralErrors': parameters[1]}) 
    polling = False 
 
Figure 24. In our implementation, the role of agent (requesting vs assisting) is 
   configurable 
5.3 Implementation of Deduction Logic 
Without the ability to perform justifiable deductions, a machine learning agent could end 
up producing inconsistent results. In this section, we describe the implemented deduction 
logic that enables the agent to conduct statistical inferences. Deduction logic relies on the 
findings of the learning procedure and they aim to improve machine learning based opti-
mization. The agent utilizes fitness evaluation, model selection, global optimum and ef-
ficiency aggregation in deduction. 
5.3.1 Fitness Evaluation and Model Selection 
The supervisor, in a machine learning agent, trains regression models with different de-
grees in order to find the models that produce a satisfying result. Despite multiple feasible 
solutions would be available, the goal of machine learning is to find and select the most 
accurate model with the best fit. We implement continuous fitness evaluation during the 
learning phase which enables the machine learning agent to select the best fitting model 
during early stages. 
 64 
In this thesis, we evaluate potential regression models by utilizing least squares estimation 
where we calculate the sums of the squared deviations of the models. The models that 
results the smallest sums of squared deviations are regarded as the best fitting models, 
and the agent utilizes these models to support deductions in global optimization (Figure 
25). 
def TrainModels (x, y, degree): 
  # Train models 
  for z in range (1, degree): 
    model = np.polyfit(x, y, z) 
    # Select the measured values 
    for i in set(x): 
      subset = (x == I) 
      # Loop through values 
      for j in range(0, len(subset)): 
        if(subset[j] == True): 
          values.append(y[k]) 
          # Least squares estimation 
          error = error+(np.mean(values)-model(subset[k]))**2 
      # Append errors 
      errorlist.append(error) 
  # Select the model with the smallest sum of squared deviations 
bestFittingModel = 1 + errorlists.index(min(errorlist)) 
 
Figure 25. Extended regression model training function that enables evaluation of 
  learning by taking advantage of least squares estimation 
If the optimization objective is multidimensional, it would be necessary to aggregate the 
information of the selected models that describe the objective. As an example, optimiza-
tion of engine power usage could be regarded as a multidimensional task where the trav-
eled distance, with a certain load, should be maximized and the energy consumption 
should be minimized. In this thesis, we implement multidimensional optimization by tak-
ing advantage in efficiency aggregation and selection as explained in the next section. 
5.3.2 Global Optimum and Efficiency Aggregation 
We derive global optimum by conducting a root analysis of the derivative of the model 
function. The root analysis describes the machine learning agent whether the model 
 65 
function is monotonic or not, and what are the local minima or maxima. In addition to the 
root analysis, the machine learning agent discovers if the feasible function values are in 
a closed or an open interval. Discovery of the intervals is crucial because these boundaries 
may also be the global optimum solution. The machine learning agent obtains the global 
optimum from the results of root analysis and by testing boundaries (Figure 26). 
def RootAnalysis(model, interval): 
  # Find roots of the derivative 
  modelDerivative = model.deriv(1) 
  roots = modelDerivative.roots 
  # Find global maxima 
  if(max(interval) > max(roots): 
    globalMaxima = max(interval) 
  else:  
    globalMaxima = max(roots) 
  # Find global minima 
  if(min(interval) < min(roots): 
    globalMinima = min(interval) 
  else:  
    globalMinima = min(roots) 
 
Figure 26. Root analysis function that derives global minima and maxima 
In this thesis, we utilize functions from the NumPy library to calculate the derivative of 
the model function and the roots of the derivative. The root finding algorithm, in NumPy, 
relies on computing the eigenvalues of the companion matrix, and the algorithm is one of 
the fixed point methods (Numpy v1.13 Manual 2018). 
Efficiency aggregation refers to the utilization of use of the selected regression model. 
We utilize the efficiency aggregation in multi-objective optimization to find a solution 
that satisfies various learning objectives simultaneously. We aggregate efficiencies by 
calculating the ratio between multiple model functions of global optimum (Figure 27). 
 
 
 
 66 
def GlobalOptimum(x1, x2, y, interval): 
  # Train models 
  model1 = TrainModels (df.loc[:, y], df.loc[:, x1]) 
  model2 = TrainModels (df.loc[:, y], df.loc[:, x2]) 
  # Find global optima 
  globalopt1 = RootAnalysis(model1, interval) 
  globalopt2= RootAnalysis(model2, interval) 
  # Calculate efficencies 
  efficiency1 = model1(globalopt1) / model2(globalopt1) 
  efficiency2 = model1(globalopt2) / model2(globalopt2) 
  # Select the best efficiency    
  if(efficiency1> efficiency2): 
    print('Choose value: ', globalopt1) 
  else: 
    print(' Choose value: ', globalopt2) 
 
Figure 27. Deduction logic for multi-objective optimization that calculates  
  and aggregates efficiencies between trained models 
The machine learning agent selects the global optimum that provides the highest aggre-
gated efficiency to foster the efficiency of multi-objective optimization. This way, deci-
sions of the machine learning agent strive to optimize the performance of the system by 
extracting the hidden information from the collected data. 
5.4 Virtual Implementation 
In this section, we containerize the implemented machine learning agent and data prepa-
ration module by utilizing Docker container platform. Containerized versions of the im-
plemented applications are a convenient way of running applications in a distributed 
cloud environment since the containers package the applications together with their 
runtime environments. After containerization, we deploy the containerized applications 
in a cloud-based environment by employing Kubernetes. 
 
 
 67 
5.4.1 Container Implementation 
We bundle the data preparation module and the machine learning agent into Docker con-
tainers. The containers include software components, required libraries and execution 
runtime environments. Containerization facilitates the portability of applications in 
Docker-capable environments. 
Docker daemon builds containers from the configured image. Respectively, docker builds 
images automatically by following the instructions from a Dockerfile which describe the 
additional software libraries and other dependencies to be installed on top of the base 
image (Figure 28).  
# Dockerfile of the machine learning agent / preparation module 
FROM python:2.7-slim 
WORKDIR /base 
ADD . /base 
RUN pip install --trusted-host pypi.python.org -r require-
ments.txt 
 
CMD [""python"", ""Main.py""] 
 
Figure 28. Simplified Dockerfile that defines the working directory, required  
  libraries and the dependencies of an image 
Moreover, we build and store the resulting images into a Docker registry from where the 
Docker daemon can pull a desired image. Figure 29 illustrates how an image of a machine 
learning agent is built from previously shown Dockerfile. 
Docker build -t edgeagent:v1 . 
 
Figure 29. Docker build command builds an image that is based on the instructions
  from a Dockerfile  
Docker has its own container orchestration system (called Docker Swarm) but instead we 
utilize Kubernetes from Google to deploy containers in this thesis. 
 
 68 
5.4.2 Deployment with Kubernetes 
Management of containerized applications can be laborious without using a proper or-
chestration system, especially if  the  usage of the application suddenly peaks out  and 
then service is expected to scale out rapidly. For this reason, we employ Kubernetes to 
facilitate the orchestration of containerized applications in a cloud-based environment. 
We can manually run images, as containers, on a Kubernetes cluster by utilizing kubectl 
run command. Kubectl run command builds a container based on a certain image and 
creates a so called “Deployment” to manage the constructed container. The created De-
ployment, on the Kubernetes cluster, ensures that the desired functionality of the con-
tainer and also recovers the container from malfunctions based on the defined policies. 
With kubectl run command, we can define multiple parameters which include, e.g., net-
working, restart and image pull policies. Figure 30 shows how we can implement a de-
ployment consisting of two containerized machine learning agents from the command 
line. 
kubectl run edgeagent ––image=edgeagent:v1 --port=80 
--replicas=2 ––restart= Always   
 –image-pull-policy = IfNot-Present 
 
Figure 30. Kubectl run command builds a container in the Kubernetes cluster 
In the previous example, the command becomes more the manually typed command be-
comes complex and more difficult to remember if we employ more specific parameters. 
To facilitate easier deployment, we utilize YAML (YAML Ain’t Markup Language) files. 
YAML is a data serialization standard supported by all programming languages (The Of-
ficial YAML Web Site 2018). The complex parameters can be described in the YAML 
file in order to make command line usage easier. In Figure 31, we define an YAML file 
that implements similar functionality as listed in the previous example. 
 
 
 69 
apiVersion: apps/v1 
kind: Deployment 
metadata: 
    name: edgeagent 
spec: 
    replicas: 2 
spec: 
    containers: 
    - image: ""edgeagent:v1"" 
      imagePullPolicy: IfNotPresent 
      name: edgeagent 
      ports: 
         - containerPort: 80 
      restartPolicy: Always 
 
Figure 31. Parameters can be stored in a YAML file to facilitate the deployment 
We manage the deployment of containerized applications by employing kubectl create 
command that constructs a resource based on the defined YAML file. Figure 32 depicts 
a deployment of two machine learning agents in the edge cloud using the previous YAML 
file. 
kubectl create -f edgeagentdeployment.yaml 
 
Figure 32. Utilization of YAML files reduces the complexity of  deployment in 
  a command line 
Usage of the YAML file format enables a flexible development, deployment and man-
agement of containerized applications with Kubernetes. We utilize Kubernetes in orches-
tration to facilitate the deployment of containerized applications in a cloud-based envi-
ronment. Moreover, Kubernetes guarantees the desired functional performance of the ap-
plications. 
5.5 Testbed 
We test the performance of the machine learning agents and a data preparation module in 
a testbed which emulates the key features, such as a distributed cloud environment and 
 70 
deployment of a miniature version of an autonomous ship. The testbed includes a minia-
ture version of the autonomous ship which sails in a water pool. The ship collects sensor 
and location data into a database where the data preparation module processes the col-
lected data. In the testbed, we utilize MongoDB as a database solution which is an open-
source Non-Structured Query Language (NoSQL) database program. 
5.5.1 Autonomous Ship Prototype 
We build a prototype of an autonomous ship from an extruded polystyrene (XPS) foam 
material. The ship carries a Raspberry Pi (RPI) and a portable battery. RPI is a single-
chip computer which is responsible for controlling the ship and collecting the engine, 
sensor and location data. In this testbed, we emulate an edge cloud with the on-board RPI. 
 
Figure 33. Autonomous ship carries a Raspberry Pi that emulates edge cloud 
We use a spherical robot called Sphero as the engine of the prototyped ship. Sphero has 
an application programming interface (API) for JavaScript which enables programmable 
control (Sphero Docs 2018). Control unit, in the RPI, utilizes the API to adjust the per-
formance of the Sphero according to the results of the machine learning. In addition to 
Sphero, we facilitate the control of the autonomous ship by installing a servomotor that 
controls a rudder attached to the end of the ship. The RPI in the ship has a control 
 71 
application (“control unit”) that utilizes the rudder to turn the ship and to stabilize ship’s 
movement. 
5.5.2 Functionality of the Prototype 
We program the prototyped ship to travel between two harbors in a pool. The prototype 
recognizes harbor areas in advance so that the prototype has time to correct and adjust its 
movement using the Sphero and the rudder. Outside of the harbor area, the prototype sails 
towards the next harbor in an energy-efficient manner. 
The ship prototype has two functional states: sailing and docking. In the sailing state, the 
prototype travels longer periods with optimized engine performance. Respectively, in 
docking state, the desired actions of the prototype are careful and accurate so that the ship 
docks safely to the harbor and tries to avoid collisions. These states enable the testing of 
the state-specific learning of the machine learning agents. It is worth noting that the states 
are expandable to cover multiple other scenarios such as emergency and low energy 
states. 
5.5.3 Distributed Cloud Environment 
In this testbed, we simulate the distributed cloud environment by utilizing a Raspberry Pi 
(RPI) and a Dell’s Latitude e7470 Ultrabook laptop. The RPI emulates the edge cloud. 
The laptop, on the other hand, is capable of providing larger amount of computational 
resources than the RPI so the laptop emulates a central cloud. A machine learning agent 
is able to utilize both of the emulated cloud environments in decentralized machine learn-
ing if the demand for additional computational resources increases. 
5.6 Summary 
In this chapter, we implemented the machine learning agent, the data preparation module 
and the testbed. We programmed the components in Python by utilizing two main 
 72 
libraries: NumPy and pandas. These libraries provided us the tools for scientific compu-
tation which facilitated the implementation of machine learning and deduction logic. The 
functionality of the implemented applications is based on the mathematical methods that 
were introduced in section 4.4. 
We also virtualized the machine learning agent and the data preparation module by com-
posing the programmed software components, required libraries and execution runtime 
environments in Docker containers. Containerization facilitates the portability of the soft-
ware and efficient deployment via Kubernetes. 
In the next chapter, we evaluate the implemented machine learning agent and data prep-
aration module by utilizing the testbed. The testbed is a test environment which emulates 
the main features of a production environment. The testbed also enables comprehensive 
evaluation of the implemented components and their functionality. 
 73 
6 ANALYSIS AND EVALUATION 
In this chapter, we test and evaluate the functionality, optimization and interoperability 
of the machine learning agent and the data preparation module. We utilize the imple-
mented testbed to evaluate sailing and docking of a single autonomous ship in a water 
pool. The ship monitors and collects the measured sensor, engine and location data which 
we use to evaluate the machine learning agent and the data preparation module. In the 
collected sensor data, the power level of the ship engine alternates periodically which 
increases the coverage of the response events in data. 
We evaluate the features of the machine learning agent and the data preparation module 
by comparing the real outputs to the predictions that are based on learning. In addition to 
the comparison, we also evaluate the fitness of operational performance. 
6.1 Evaluation of Functionality 
We evaluate the functionality of the data preparation module by comparing the prepared 
data to the corresponding raw data. We regard the functionality to be successful if the 
data preparation module fulfils the criteria of providing clean and complete data. 
Evaluation of machine learning, on the other hand, demonstrates the veracity of the de-
duction results. We evaluate that the machine learning agent is able to utilize and validate 
regression based learning as intended. In validation of regression, the agent automatically 
calculates the least squares estimation of the learned regression model and selects the 
model that produces the smallest sum of squared deviations. In this section, we support 
and visualize the evaluation by utilizing graphs. 
6.1.1 Evaluation of Data Preparation Module 
We test and evaluate the functional performance of the data preparation module by uti-
lizing raw and unprocessed data from the testbed. We specify the following evaluation 
 74 
criteria to evaluate the correctness of the data preparation module: 1) The prepared data 
should have reduced amount of redundant information 2) Prepared data should be not 
corrupted 3) The amount of noise should be decreased in prepared data 4) The pattern of 
the data should be preserved and enhanced. 
We evaluate the data preparation module by using value 24 as the interval (order) of 
moving average filter because the autonomous ship collects data three times per second, 
and the data preparation module calculates the average value of data within 8 seconds 
intervals. Figure 34 depicts the voltage and velocity data before and after being processed 
by the data preparation module. 
 
Figure 34. The upper graphs represent raw system data and the lower graphs 
  represent the results of data preparation 
Figure 34 depicts that the graphs with prepared data have no gaps and simultaneous oc-
currences of data points do not exist at the same moment in time. In other words, the 
prepared data has only unique values and the prepared data does not include any missing 
 75 
values. In the prepared data, the noise filtering has also been successful due to reduction 
of unwanted anomalies. Especially, in the Figure 34, the velocity in the raw data includes 
multiple unrealistic values that do not exist in the velocity of the prepared data. 
The pattern of the raw data is not clearly recognizable, especially in the case velocity data, 
which hinders the reliability of machine learning and analytics. However, the data prep-
aration module has preserved but actually enhanced the pattern of prepared data which is 
now significantly recognizable than in raw data. The velocity graph of the prepared data 
includes clear and repeating downward spikes that indicate when the power level of the 
ship has changed (Figure 34). The spikes show the effect of gliding where the ship had 
brief moments of free motion especially after acceleration when the engine was turned 
off. 
6.1.2 Evaluation of Machine Learning 
We evaluate the correctness of machine learning by comparing the selected model of the 
machine learning agent to multiple pre-defined regressions models. The machine learning 
agent strives to find the best fitting model by utilizing least squares estimation where the 
agent selects the model which results the smallest sum of squared deviations. The agent 
utilizes prepared data, that is provided by the data preparation module, in regressions. 
We evaluate machine learning by training the agent with collected data that consists of 
information about the achieved velocity and the power usage of the tested ship. We vali-
date that the agent selects the best fitting model by calculating sums of squared deviations 
of the regression models that have degrees from one to seven. We chose to evaluate de-
grees only from this range but it is a configurable setting in the agent. Figure 35 depicts 
graphs of regressions and the sums of squared deviations. 
 76 
 
Figure 35. By comparing multiple regression models, the machine learning agent 
  selects the best fitting regression model with a degree of six 
The agent selects correctly the sixth degree regression model which produces the smallest 
sum of squared deviations (Figure 35). One may also observe that the selected regression 
model has a slightly exceptional behavior between the known data points which can be 
fixed by expanding the training set with additional power values. 
6.1.3 Evaluation of Decentralized Learning 
We evaluate the correctness of the functionality of the decentralized learning by compar-
ing the results of decentralized learning to the results of a single machine learning agent. 
In this comparison, we provide identical data sets for decentralized and local machine 
 77 
learning systems and we expect exactly the same results. Both machine learning systems 
have the same learning objective where they strive to find the optimum value for power 
usage of the engine. Figure 36 depicts the learning procedure of a single machine learning 
agent that does not utilize decentralization. 
#-- Machine Learning Agent – 
Best fit found at 7th degree 
Found optimal power level value: 205.76511509951138 
Figure 36. Results of a single machine learning agent 
In this evaluation, we distribute learning to be deployed in two machine learning agents 
that are located in edge and central clouds. The agent in the edge cloud requests assistance 
from the agent in the central cloud. Figure 37 depicts how the agent in central cloud de-
tects a requesting agent in a feasible range. 
#-- Machine Learning Agent of a Central Cloud – 
Detecting agents in feasible range.. 
Request found! 
Starting decentralized learning of degreed 3 to 7 
Best fit found at 7th degree 
Found optimal power level value: 205.76511509951138 
Sending parameters to requesting agent.. 
Parameters sent successfully 
Figure 37. Results of a machine learning agent that is deployed in the central cloud 
The requesting agent sends information about the learning objective to the assisting agent. 
In addition, the requesting agent is responsible for dividing the learning task among the 
agents that contribute in decentralized learning. Finally, the requesting agent receives and 
aggregates the results of the learning task which is depicted in Figure 38. 
 
 
 
 
 78 
#-- Machine Learning Agent of an Edge Cloud – 
Requesting assistance from other agents.. 
Found assisting agent in address: http://192.168.2.131:8080/re-
questAssistance 
 
Allocating local learning of degrees from 1 to 2 
Allocating decentralized learning of degrees from 3 to 7 
Best fit found at 1st degree 
Found optimal power level value: 255 
 
Waiting for other agents to finish learning.. 
Aggregating learning results.. 
Best fit found at 7th degree 
Result of decentralized learning for optimal power usage: 
205.76511509951138 
Figure 38. Results of a requesting machine learning agent that is deployed in the
  edge cloud 
As we observed previously, a single machine learning agent found the best fitting model 
in 7th degree and the optimal value of 205.765 by utilizing local learning. These results 
match with the results of decentralized learning so we can conclude that the distribution 
and aggregation of learning tasks works as expected. 
6.2 Evaluation of Optimized Performance 
In this section, we evaluate the effect of optimization of the machine learning agent. We 
utilize data collected from the testbed where an autonomous ship was sailing for four 
hours. The autonomous ship sailed between two harbors and the ship took advantage of 
two different states: sailing and docking. In both states, we alternated the power of the 
engine in every five minutes to broaden the coverage of the response events in the col-
lected data. 
The machine learning agent learns the optimum degree and parameters of the best fitting 
regression model from the training sets of examples. We evaluate the optimized perfor-
mance by comparing learning results to measured values. From the findings of the com-
parison, we can evaluate if machine learning truly improves the performance of the ship. 
 79 
6.2.1 Evaluation of State-Specific Machine Learning: Sailing 
In sailing state, a machine learning agent strives to optimize the power usage of a ship 
engine in such a way that the energy consumption is minimized and the velocity is as high 
as possible. This multi-objective optimization (Pareto optimization) prolongs the sailing 
distance and supports overall energy efficiency. 
The machine learning agent learns the best fitting regression models that describe the 
structural patterns of changing distance and voltage. In learning, the agent utilizes a train-
ing set of examples which consists of sailing with different power levels in fixed periods 
of time. The electric engine, of an autonomous ship prototype, utilizes unsigned 8-bit 
values to represent the magnitude of the power usage where 0 is the minimum power 
usage and 255 is the maximum. Figure 39 depicts the best fitting regression models that 
the agent derives from the training set, and also two additional iterations of learning. 
 80 
 
Figure 39. The global optimum convergences during iterations of machine learning 
The agent finds best fitting regression models for the distance transform, which is the 
depended variable, when the degree has the value of six in every iteration. The agent finds 
the root derivative in order to discover the global optimum with maximum distance trans-
form. The agent finds the global optimum value of 254 from the regression calculated 
 81 
from the training set but the global optimum value converges to 241 after couple of ma-
chine learning iterations (Figure 39). 
The agent finds the best fitting regression models respectively for voltage transform when 
the degree has the value of one. In this optimization, the objective is to find the global 
optimum where the voltage transform is minimal. As a result, the agent finds the global 
optimum value that converges to zero (Figure 39). 
After the best fitting regression models are found, the agent aggregates the different dis-
tance-power efficiency ratios to find the most optimal solution in this multi-objective op-
timization. Generally, the agent aggregates the efficiency ratios by calculating and com-
paring the distance-power efficiency ratios of multiple feasible regression model func-
tions of global optimum, but, in this evaluation, we calculate ratios for each tested power 
level (Figure 40). 
Efficiency ratios of trained power levels: 
 
Efficiency ratio with power level 20: 0.5087251892035706 
Efficiency ratio with power level 50: 1.8381502685268921 
Efficiency ratio with power level 80: 5.0414485985267292 
Efficiency ratio with power level 110: 6.939491120090184 
Efficiency ratio with power level 140: 8.234854280434233 
Efficiency ratio with power level 170: 10.10325595371303 
Efficiency ratio with power level 200: 12.53758467667819 
Efficiency ratio with power level 230: 14.09660897191777 
Efficiency ratio with power level 255: 13.84067482366743 
 
Efficiency ratios of learned power levels: 
 
Efficiency ratio with power level 254: 13.89075916124045 
Efficiency ratio with power level 241: 14.27218972427181 
Figure 40. Comparison of distance-power efficiency ratios with different power 
  levels 
Efficiency ratio is the highest when the best fitting regression models receive a value that 
converges to 241. The power value of 241 is the most optimal if we want to maximize the 
distance transform with the minimized voltage transform in the tested ship. If the shape 
or weight of the ship changes, additional machine learning would be necessary to perform 
 82 
so that the most optimal power value for a certain ship (possibly with different amount of 
cargo) can be found.  
6.2.2 Evaluation of State-Specific Machine Learning: Docking 
In docking state, an autonomous ship detects a nearby harbor and the ship adjusts its 
movement to support smooth docking. A machine learning agent pursues to optimize the 
power usage of the ship engine in two ways: docking time is minimized and the ship does 
not sail past the harbor. If the ship collides or sails past the harbor, the ship receives a 
penalty which the agent adds in the measured docking time. 
Similarly as in the previous section, the machine learning agent learns the best fitting 
regression model that depicts the relation between different power levels and the corre-
sponding docking times. In this evaluation, we collected data using different power levels 
where we ran four docking procedures per selected power level. Figure 41 illustrates the 
best fitting regression models that the agent discovered by learning. 
 
Figure 41. The degree of the best fitting regression model stabilizes when iterations
  in learning increases 
 83 
The agent learns during training the best fitting regression model when the degree has 
value of five, but, after couple of iterations of learning, the agent finds the best regression 
model when the degree has value of two. Hence, we see that the regression model stabi-
lizes when the global optimum converges. In this evaluation, the agent finds the initial 
global optimum value of 87, but, after the regression model stabilizes, the global optimum 
value converges to 74 (Figure 41).  
Figure 42 depicts coefficients that represents the combination of minimized docking time 
and risk of sailing past harbor. The smaller the coefficient is, the more selected power 
level fulfils the learning objective. 
Coefficients of trained power levels: 
 
Coefficient with power level 20: 0.29131543243480684 
Coefficient with power level 50: 0.22514691115577656 
Coefficient with power level 80: 0.21074554392906975 
Coefficient with power level 110: 0.2481113307546865 
Coefficient with power level 140: 0.33724427163262677 
Coefficient with power level 170: 0.47814436656289044 
Coefficient with power level 200: 0.6708116155454777 
Coefficient with power level 230: 0.9152460185803886 
Coefficient with power level 255: 1.1584857082327833 
 
Coefficients of learned power levels: 
 
Coefficient with power level 87: 0.21483394318703364 
Coefficient with power level 74: 0.20948444505022523 
Figure 42. Comparison of coefficient values with different power levels 
Coefficient has the lowest value when the selected regression model receives the value of 
74. Consequently, we can state that the power value of 74 is the most optimal value when 
we desire to reduce the docking time and avoid the risk of sailing past the harbor. 
 
 
 84 
6.3 Evaluation of Virtualized Agent 
We evaluate that the virtualized machine learning agent and virtualized data preparation 
meet their design criteria in this section. To evaluate virtualization, we run containerized 
machine learning agents, and observe if the agents are able to interoperate in different 
environments and conduct decentralized machine learning by utilizing prepared data. 
Containerized applications run in parallel where they utilize HTTP -based communication 
so that, in addition to functional interoperability, we evaluate the agent’s ability to per-
form inter-process communication (IPC). 
We also evaluate the performance of the selected orchestration system. We implemented 
the deployment of containerized applications by employing Kubernetes which we evalu-
ate also in this section. Furthermore, we evaluate Kubernetes’ capability to restore the 
desired status of the containers upon their failure. 
6.3.1 Evaluation of Container Interoperability 
Container interoperability refers to operational reliability of containerized applications in 
different deployment environments. We evaluate the functionality of the containerized 
applications and that building of docker images works as expected. Figure 43 illustrates 
that the images are built successfully and published in the Docker repository and they can 
be pulled by worker nodes. 
REPOSITORY      TAG        IMAGE ID       CREATED    SIZE 
centralagent    v1         754435a5e51d   11 min      259MB 
edgeagent       v1         b4651e59b841   12 min      259MB 
datamodule      v1         e03ad24323eb   13 min      258MB 
Figure 43. Built docker images of the machine learning agents and the data  
  preparation module are published in the Docker repository 
We evaluate the performance of the decentralized machine learning by running contain-
erized machine learning agents in distributed cloud environment. We regard the machine 
learning results to be successful if decentralized learning (in containers) produces the 
same optimum target value of 241 for power usage in sailing similarly as described 
 85 
previously in section 6.2.1. If the produced value differs from this target, we would notice 
interoperability problems either in decentralized learning or in the data preparation mod-
ule. Figure 44, depicts the functionality of the containerized machine learning agent lo-
cated in an edge cloud. 
#-- Containerized Agent in an Edge Cloud 
Requesting assistance from other agents.. 
Found assisting agent in address: http://192.168.2.131:8080/re-
questAssistance 
 
Allocating local learning of degrees from 1 to 2 
Allocating decentralized learning of degrees from 3 to 7 
Best fit found at 1st degree 
Found optimal power level value: 255 
Waiting for other agents to finish learning.. 
Aggregating learning results.. 
Best fit found at 6th degree 
Result of decentralized learning for optimal power usage: 
241.00216610416206 
Figure 44. Results of a containerized machine learning agent that is deployed in the
  edge cloud 
Figure 45 describes, respectively, the functionality of the containerized machine learning 
agent located in a central cloud. 
#-- Containerized Agent in a Central Cloud – 
Detecting agents in feasible range.. 
Request found! 
Starting decentralized learning of degreed 3 to 7 
Best fit found at 6th degree 
Found optimal power level value: 241.00216610416206 
Sending parameters to requesting agent.. 
Parameters sent successfully 
Figure 45. Results of a containerized machine learning agent that is deployed in the
  central cloud 
Containerized machine learning agents perform in a similar way independently of 
whether they are running at the edge or the central cloud. From these results we can see 
that the result of decentralized learning matches with the target value and we can conclude 
that the behavior of the machine learning agents is identical in the containerized and non-
containerized versions. 
 86 
6.3.2 Evaluation of Orchestration 
We evaluate container deployment and tolerance against failures in Kubernetes based 
distributed cloud environment. In order to evaluate the operational success, Kubernetes 
should be able to deploy the containers that we have defined in YAML files and ensure 
their proper operational functionality. Moreover, Kubernetes should be able to recover 
containerized applications after malfunctions. Figure 46 describes how we are able to 
deploy containers from YAML files by utilizing Kubernetes. 
NAME         DESIRED  CURRENT  UP-TO-DATE AVAILABLE 
centralagent 1        1        1          1   
edgeagent    1        1        1          1  
datamodule   1        1        1          1   
Figure 46. Overview of containers that are deployed and orchestrated by Kubernetes 
Next we observe the status of Kubernetes pods and how the Kubernetes responses when 
unwanted events occur. Figure 47 depicts the information of Kubernetes pod immediately 
after the deployment. 
NAME         READY  STATUS     RESTARTS  AGE 
centralagent 1/1    Running    0         23 min 
edgeagent    1/1    Running    0         24 min  
datamodule   1/1    Running    0         27 min  
Figure 47. Overview of the deployed containers in the Kubernetes pod 
We cause connectivity and environmental failures, on purpose, to test how Kubernetes 
manages orchestration when malfunctions occur. In Figure 48, Kubernetes handles a mal-
function when the containers cannot be run. 
NAME         READY  STATUS        RESTARTS  AGE 
centralagent 0/1    ErrImagePull  2         33 min 
edgeagent    0/1    Error         2         34 min  
datamodule   0/1    Error         2         37 min  
Figure 48. Overview of the runtime failures in containers 
After the connectivity and the environmental states are in a stable, Kubernetes manages 
to restart pods and recover from the malfunction as illustrated in Figure 49. 
 87 
NAME         READY  STATUS     RESTARTS  AGE 
centralagent 1/1    Running    3         37 min 
edgeagent    1/1    Running    3         38 min  
datamodule   1/1    Running    3         41 min  
Figure 49. Overview of the recovered containers that are running again 
Kubernetes is autonomously able to deploy, maintain and recover containerized machine 
learning agents and data preparation module successfully. Under these circumstances, we 
can evaluate that the selected orchestration system supports the deployment of container-
ized applications in a distributed cloud environment. 
6.4 Summary 
In this chapter, we evaluated the functionality, optimization capability and virtual imple-
mentation of the machine learning agent and the data preparation module. We evaluated 
basic functionality of the implemented applications by comparing the results of the dif-
ferent agent configurations to the base line scenario. According to our tests, the machine 
learning software components work as expected. 
After we had evaluated the functionality, we evaluated the optimized performance of an 
autonomous ship by utilizing a machine learning agent. The agent was assigned to handle 
two state-specific machine learning tasks where we evaluated that the machine learning 
agent was able to find the most optimal solution. We compared the results of machine 
learning to the measurement results. Based on comparison, we evaluated that machine 
learning improved the state-specific performance and thus machine learning optimizes 
the overall system performance. 
Finally, we evaluated the virtualized implementation of machine learning agent and data 
preparation module. In evaluation of the virtualized implementation, we evaluated the 
functional interoperability, inter-process communication and orchestration of the contain-
erized applications in a distributed cloud environment. The results of evaluation of the 
virtualized implementation indicate that the agent and the data preparation module 
 88 
operate and communicate as expected in containers and they can be deployed by utilizing 
Kubernetes. In addition to deployment, Kubernetes is able to automatically restore the 
containerized applications after malfunctions and maintain the desired states of the con-
tainers. 
 89 
7 CONCLUSION AND DISCUSSION 
In this chapter, we discuss about the advantages and disadvantages of the decentralization 
of machine learning in a distributed cloud environment based on the findings in this the-
sis. We review the efficiency of machine learning based optimization, the potential in 
decentralized learning and improved portability and fault tolerance introduced by con-
tainerized deployment. 
In this thesis, we extracted hidden information from the data by utilizing machine learn-
ing. The extracted hidden information provided more knowledge about the system per-
formance than we could deduct from the data sets with heuristic approximations. In ad-
dition to the extraction of the hidden information, machine learning can optimize the per-
formance of the autonomous ship in both two states better than the best performance that 
the training set could introduce. However, a drawback is that learning is relatively re-
source demanding which reduces the amount of available resources for other operations. 
We have run decentralized machine learning in parallel in two environments, one of 
which was a constrained environment, which shows that machine learning can also be 
scaled down. Decentralization allowed us to balance the load between two environments 
and spread the load according to the computational capabilities of the environments. Even 
though decentralization facilitates learning in multiple environments, it can suffer from 
increased overhead due to inter-process communication. 
Containerized deployment of the machine learning agents and data preparation module 
fostered portability of the applications in distributed cloud environment. In addition to 
portability, containerization enabled us to use an orchestration system to automate the 
deployment, maintenance and fault tolerance of the running applications. Kubernetes, as 
an orchestration system, served the purpose of Docker container orchestration which en-
abled the deployment of our containerized applications in an automated way. 
In this thesis, we proposed an alternative solution for decentralized and parallel machine 
learning by utilizing a distributed cloud environment. We defined requirements for 
 90 
decentralized machine learning that fosters microservice and “As a Service” -oriented 
development. In addition to development of the related requirements, we introduced a 
real life use case where decentralized machine learning was required to be interoperable 
in a multi-cloud environment and to provide optimized performance. Since decentralized 
machine learning was deployed in the distributed cloud environment, we developed de-
centralized machine learning software to support containerization which facilitates the 
portability and supports Kubernetes based orchestration. 
We designed and implemented a machine learning agent and a data preparation module 
which are independent software applications that can be run in containers. Containeriza-
tion of applications facilitates the portability in the distributed cloud environment and 
enables the orchestration via Kubernetes. The machine learning agent is responsible for 
the performance of decentralized machine learning operations and maintaining inter-pro-
cess communication among other agents. Data preparation module, on the other hand, 
ensures that the data for machine learning is filtered, nonredundant and complete. 
We evaluated success in functional and virtual performance of the implemented machine 
learning agent and data preparation module. In addition to functional and virtual perfor-
mance, the machine learning agent was also able to improve overall system performance 
of an autonomous ship by taking advantage of machine learning. The ship had two pos-
sible functional state where the agent could distinguish different states and conduct state-
specific machine learning. State-specific learning enables the agent to perform more ad-
vantageous deductions since the agent can separate the state-specific learning objectives. 
In this thesis, we managed to improve the overall system performance by utilizing regres-
sion which is one of the supervised learning techniques. 
For further development, we would propose following improvements to be considered: 
support for additional machine learning techniques, support for additional data filtering 
techniques, enhanced ability to monitor cloud metrics, alternative solution with MapRe-
duce framework and support for unikernel deployment. Support for additional technolo-
gies would increase the coverage of use cases for machine learning based optimization. 
Enhanced ability to monitor cloud metrics would benefit machine learning agent to detect 
 91 
the capability of deployment environment more precisely. Finally, machine learning 
agents could be developed to support unikernel deployment which fosters light-weight 
execution of the applications. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 92 
 
REFERENCES 
Allison Paul D. (2001) Missing Data 
Alpaydin Ethem (2010) Introduction to Machine Learning. 
Andreas Hammar (2014) Analysis and Design of High Performance Inter-core Process 
Communication for Linux. 
Anthony Martin (2011) Undergraduate study in Economics, Management, Finance and 
the Social Sciences. [online]. [Referred on 4.6.2018]. Available at : 
http://www.met.edu/Institutes/ICS/NCNHIT/papers/39.pdf 
Bass Len, Clements Paul & Kazman Rick (2003) Software Architecture in Practice. 
Bijuraj L. V. (2013) Clustering and its Applications. 
Brink Henrik, Richards Joseph W., Fetherolf Mark (2017) Real-World Machine Learn-
ing. 
Docker Guide Documentation (2018). About images, containers, and storage drivers. 
[online]. [Referred on 22.5.2018]. Available at: https://docs.docker.com/v17.09/en-
gine/userguide/storagedriver/imagesandcontainers/ 
Docker Product Manuals (2018). Docker Enterprice Edition Platform. [online]. [Referred 
on 21.5.2018]. Available at: https://docs.docker.com/ee/ 
Ericsson (2018) The Ericsson Mobility Report. [online]. [Referred on 24.7.2018]. Avail-
able at: https://www.ericsson.com/en/mobility-report/mobility-visual-
izer?f=1&ft=1&r=2,3,4,5,6,7,8,9&t=8&s=1,2,3&u=1&y=2017,2023&c=1 
Gilbert Strang (1991) Calculus. 
 93 
Google Cloud Platform Documentation (2018) Kubernetes Engine: Container Cluster 
Architecture. [online]. [Referred on 26.5.2018]. Available at: 
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture 
Hwang Kai & Chen Min (2017) Big-Data Analytics for Cloud, IoT and Cognitive Com-
puting. 
Juniper Networks (2018) What is a Docker container. [online]. [Referred on 4.6.2018]. 
Available at: https://www.juniper.net/us/en/products-services/what-is/docker-con-
tainer/ 
Katyal Mayanka & Mishra Atul (2015) Orchestration of Cloud Computing Virtual Re-
sources. [online]. [Referred on 7.6.2018]. Available at: https://ieeex-
plore.ieee.org/document/7019756/ 
Kena Alexander, Choonhwa Lee, Eunsam Kim & Sumi Helal (2017) Enabling End-to-
End Orchestration of Multi-Cloud Applications. [online]. [Referred on 29.5.2018]. 
Available at: https://ieeexplore.ieee.org/document/8008766/ 
Kubernetes Documentation (2018) Concepts: Overview. [online]. [Referred on 
26.5.2018]. Available at: https://kubernetes.io/docs/concepts/overview/ 
Lucky R.W. (1968) Adaptive redundancy removal in data transmission. 
Lukša Marko (2018) Kubernetes in Action. 
Marinescu Dan C. (2013) Cloud Computing: Theory and Practice. 
NumPy v1.13 Manual (2018) Numpy and Scipy Documentation. [online]. [Referred on 
27.7.2018]. Available at: https://docs.scipy.org/doc/ 
 94 
Pandas 0.23.2 Documentation (2018) pandas: powerful Python data analysis toolkit. 
[online]. [Referred on 27.7.2018]. Available at: http://pandas.pydata.org/pandas-
docs/stable/ 
Press William H., Teukoisky Saul A., Vetterling William T. & Flannery Brian P. (2007) 
Numerical Recipes: The Art of Scientific Computing. 
Rawlings John O., Pantula Sastry G. & Dickey David A. (1988) Applied Regression Anal-
ysis: A Research Tool. 
Rodger Richard (2018) The Tao of Microservices 
Rolls-Royce (2016) Autonomous Ships: The Next Step. [online]. [Referred on 2.6.2018]. 
Available at: https://www.rolls-royce.com/~/media/Files/R/Rolls-Royce/docu-
ments/customers/marine/ship-intel/rr-ship-intel-aawa-8pg.pdf 
Scott Charlie, Wolfe Paul & Erwin Mike (1999) Virtual private networks. 
Singh Jatinder, Bacon Jean, Crowcroft Jon, Madhavapeeddy Anil, Pasquier Thomas, Hon 
W. Kuan & Millard Christofer (2014): Technical Report of University of Cambridge: 
Regional clouds: technical considerations. [online]. [Referred on 7.6.2018]. Availa-
ble at: https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-863.pdf 
Smola Alex & Bishwanathan S.V.N (2008) Introduction to Machine Learning. 
Sosinsky Barrie (2011) Cloud Computing Bible. 
Sphero Documentation (2018) Javascript API. [online]. [Referred on 3.7.2018]. Availa-
ble at: https://sdk.sphero.com/community-apis/javascript-sdk/ 
Stefan Poslad & Patricia Charlton (2001) Standardizing Agent Interoperability: The FIPA 
Approach. [online]. [Referred on 11.6.2018]. Available at: 
 95 
http://www.eecs.qmul.ac.uk/~stefan/publications/2001-LNCS-standardising-agent-in-
teroperability.pdf 
Stuart J. Russel & Peter Norvig (1995) Artificial Intelligence A Modern Approach. 
Sutton Richard S. & Barto Andrew G. (2017) Reinforcement Learning: An Introduction. 
Tan Li & Jiang Jean (2013) Digital signal processing: fundamentals and applications. 
The Official YAML Web Site (2018) [online]. [Referred on 12.7.2018]. Available at: 
http://yaml.org/ 
Wang Can, Zhang Sheng, Zhang Huyin, Qian Zhuzhong & Lu Sanglu (2017) Edge Cloud 
Capacity Allocation for Low Delay Computing on Mobile Devices. [online]. [Referred 
on 29.7.2018]. Available at: https://ieeexplore.ieee.org/document/8367279/ 
Zhang Xiujun, Liu Keqin, Liu Zhi-Ping, Duval Béatrice, Richer Jean-Michel, Zhao Xing-
Ming, Hao Jin-Kao Hao & Chen Konan (2013) Bioinformatics. 
Zhu Hong, Bayley Ian (2018) If Docker Is The Answer, What Is The Question. [online]. 
[Referred on 29.5.2018]. Available at: https://ieeexplore.ieee.org/document/8359160/ 
 96 
8 APPENDIX 
A. Alternative Solution with TensorFlow 
# Joel Reijonen (joel.reijonen@ericsson.com) 
 
# Import required libraries  
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import tensorflow as tf  
import time 
 
# -- Initalization of input data -- 
 
# initialization of 100 observations 
n_observations = 100 
 
# xs -> linspace returns evenly spaced numbers over a specified 
# interval 
xs = np.linspace(0, 3, n_observations) 
 
# ys -> sin wave with random uniform that draws samples  #from a 
uniform distribution 
ys = np.sin(xs) + np.random.uniform(0, 0.5, n_observations) 
 
#plot data  
plt.plot(xs,ys) 
 97 
 
# Let's define a function how to train our best fitting  
# regression model  
def TrainModel(x, y, interval): 
  # This set extracts unique values from the data set 
  distinctX = set(x) 
  # And let’s initalize errolist for evaluation 
  errorlist = [] 
  # Let’s make a simple for loop which goes through degrees 
#that we want to observe 
  for degrees in range (1, 7): 
    # initialize error and the index of this degree to be zero 
    error=0 
    index = 0 
    # fit a regression by using np.polyfit 
    z = np.polyfit(x, y, degrees) 
    # convert the model as polynomial 
    p = np.poly1d(z) 
   # Let’s make another loop to go through measurement points 
   for measurement in x: 
     # Least square errors: 
     # Sum the squared errors  
     # errors are calculated from 
     error= error + (y[index]-p(measurement))**2 
     #increment index -> to go through all output values 
     index+=1 
     #--- endloop 
 98 
   # add received error to the list 
   errorlist.append(error) 
  #--- endloop 
  # Get the degree of best fitting model that produces min  
 #error     
  degree = errorlist.index(min(errorlist)) + 1 
  #Receive the model 
  z = np.polyfit(x, y, degree)  
  model = np.poly1d(z)  
  #return polynomial and errorlist 
  return model, errorlist; 
#--- endFunction 
 
#Lets train the model and start the timer 
start= time.time() model, errorlist = TrainModel(xs,ys, [1,8]) 
end=time.time() 
 
time1 = end-start 
#Lets define a linspace set data to be used in plot-ting temp-
data = np.linspace(0,3,10) 
 
#plot the original data 
plt.plot(xs, ys) 
 
#plot the regression over the original data  
plt.plot(tempdata ,model(tempdata)) 
 
 99 
# Plot initial data 
plt.plot(xs,ys) 
# Start timer 
start= time.time() 
# tf.placeholders for the input and output of the network.      
# Placeholders are variables which we need to fill in when we   
# are ready to compute the graph 
X = tf.placeholder(tf.float32) 
Y = tf.placeholder(tf.float32) 
# Instead of a single factor and a bias, we'll create a  
# polynomial function of different polynomial degrees  
# We will then learn the influence that each degree of the  
#input (X^0, X^1, X^2, ...) has on the final output (Y). 
 
# Initialization of the variables of the graph (net-work) 
# Let's initialize 4 weights 
# In neural networks we initialize weights stochastically 
 
# Let’s utilize tf.variables: 
Y_pred = tf.Variable(tf.random_normal([1],     
       mean=0.5, stddev=0.1), name='bias')  
# Utilize tf.variables through loop: 
for pow_i in range(1, 4): 
W = tf.Variable(tf.random_normal([1],    
     mean=0.5, stddev=0.1), 
name='weight_%d' % pow_i) 
  # Initialize predicted output values 
  Y_pred = tf.add(tf.multiply(tf.pow(X, pow_i), W), Y_pred)  
  #--- endLoop 
 
# Cost function will measure the distance between our 
# observations and predictions and average over them. 
 
# This is somehow similar cost function as we programmed  
# previously in our 1st version  
cost = tf.reduce_sum(tf.pow(Y_pred - Y, 2))    
    / (n_observations - 1) 
# Use gradient descent to optimize W,b 
# Performs a single step in the negative gradient 
learning_rate = 0.05 
 
#Lets define a gradient descent optimizer via tf.train 
# Here we minimize the sum of cost 
optimizer = tf.train.GradientDescentOptimizer(      
                     learning_rate).minimize(cost) 
 
 100 
# Let’s define epochs, that define size of loop in  
# training 
n_epochs = 10000 
 
# We create a session to use the graph 
with tf.Session() as sess: 
  # Here we tell tensorflow that we want to initialize 
  # all the variables in the graph so we can use them 
  sess.run(tf.global_variables_initializer()) 
     
  # Fit all training data 
  # Initialize previous training cost 
  prev_training_cost = 0.0 
     
  # Loop through epochs 
  for epoch_i in range(n_epochs): 
    # Use zip to loop through both sets of our initial data  
    for (x, y) in zip(xs, ys): 
      # Utilize run from initialized session 
      # Feed our set of data together with optimizer  
      sess.run(optimizer, feed_dict={X: x, Y: y}) 
      #--- endLoop 
    # Update training cost by run and using cost, now feed  
    # all data  
    training_cost = sess.run(cost, feed_dict={X: xs, Y: ys}) 
    # Print updated training cost (cost should descent!)  
    print('Current training cost: ', training_cost) 
    # Plot in every 100 epochs 
    if epoch_i % 100 == 0: 
      plt.plot(xs, Y_pred.eval(feed_dict={X:   
      xs}, session=sess), 'k', alpha=0.6) 
    # Allow the training to quit when we have reached a  
    #reasonable accuracy 
    if np.abs(prev_training_cost - training_cost) < 0.001: 
       break 
    # Update cost 
    prev_training_cost = training_cost 
    #--- endSession 
# Stop timer 
end=time.time() time2= end-start 
# Plot both of the graphs into same figure 
plt.plot(tempdata, model(tempdata)) 
# Print ratio: Network training time divided by training  
# time of our solution  
print(""RATIO: "", time2/time1) 
('Current training cost: ', 0.03191368) 
 101 
('Current training cost: ', 0.031566862) 
('RATIO: ', 5.1100263354363396) 
# In following graph, gray line is the regression that is  
# obtained from this neural network (TensorFlow) and orange     
# line is obtained from the solution that was used in this  
# thesis 
  
 
# Conclusions: 
# Despite the fact that the neural network approach is slower   
# less accurate and needs to be informed about the degree of    
# regression model in advance, the neural network approach      
# supports more general use cases where we do not necessarily   
# know what kind of algorithm would provide feasible results. 
# 
# In addition, the decentralization of neural network based     
# model training would be challenging to implement since        
# training is dependent on the result of previous training      
# iteration 
 
 
",233002133,"{'doi': None, 'oai': 'oai:osuva.uwasa.fi:10024/9493'}",Decentralized Machine Learning for Autonomous Ships in Distributed Cloud Environment,,2018-01-01T00:00:00+00:00,,[],['https://osuva.uwasa.fi/bitstream/handle/10024/9493/osuva_8438.pdf?sequence=1&isAllowed=y'],,2018,"[{'type': 'download', 'url': 'https://core.ac.uk/download/233002133.pdf'}, {'type': 'reader', 'url': 'https://core.ac.uk/reader/233002133'}, {'type': 'thumbnail_m', 'url': 'https://core.ac.uk/image/233002133/medium'}, {'type': 'thumbnail_l', 'url': 'https://core.ac.uk/image/233002133/large'}, {'type': 'display', 'url': 'https://core.ac.uk/outputs/233002133'}, {'type': 'similar', 'url': 'https://core.ac.uk/display/233002133?source=1&algorithmId=15&similarToDoc=&similarToDocKey=URL&recSetID=83760304-a4c6-45d2-a791-acd857471d0f&position=2&recommendation_type=same_repo&otherRecs=188221025,233002133,156876069,481573328,345087530,267982567,186300827,186274913,322434405,229562277'}]","Machine learning is a concept where a computing machine is capable to improve its own performance through experience or training. Machine learning has been adopted as an optimization solution in broad field of information technology (IT) industry. In addition, the availability of data has become more and more easier since the effective data storage and telecommunication technologies such as new generation cloud computing are developing. Cloud computing refers to a network-centric paradigm which provides additional computational resources and a scalable data storage. Even though the utilization of cloud computing enables improved performance of machine learning, cloud computing increases the overall complexity of the system as well.

In this thesis, we develop a machine learning agent which is an independent software application that is responsible for the implementation and integration of decentralized machine learning in a distributed cloud environment. Decentralization of machine learning enables parallel machine learning between multiple machine learning agents that are deployed in multiple clouds. In addition to the development of machine learning agent, we develop a data preparation module which ensures that the data is clean and complete.

We develop the machine learning agent and the data preparation module to support container implementation by taking advantage in Docker container platform. Containerization of the applications facilitates portability in multi-cloud deployments and enables efficient orchestration by utilizing Kubernetes. In this thesis, we do not utilize existing machine learning frameworks but rather we implement machine learning by applying known mathematical methods.

We have divided the development of the software applications in three phases: requirement specification, design and implementation. In requirement specification, we describe the essential features that are required to be included. Based on the requirements, we design the applications to fulfill expectations and respectively we utilize the design to guide the implementation. In the final chapter of this thesis, we evaluate functionality, ability to emhance performance and virtualized implementation of the applications.fi=Opinnäytetyö kokotekstinä PDF-muodossa.|en=Thesis fulltext in PDF format.|sv=Lärdomsprov tillgängligt som fulltext i PDF-format","[""fi=Diplomityö|en=Master's thesis (M.Sc. (Tech.))|sv=Diplomarbete|"", 'Decentralized machine learning', 'distributed cloud computing', 'data preparation', 'containerization', 'orchestration', 'fi=Tietotekniikan koulutusohjelma (DI)|', 'fi=Automaatiotekniikka|en=Automation Technology|']",disabled
